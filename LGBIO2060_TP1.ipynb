{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "LGBIO2060_TP1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2021/blob/main/LGBIO2060_TP1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53044446"
      },
      "source": [
        "# LGBIO2060 Exercice session 1\n",
        "__Authors:__ Simon Vandergooten and Clémence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n",
        "\n",
        "Statistical reminder :\n",
        "In this first exercise session we will refresh your memory about the basis of statistics needed for the rest of the sessions.\n",
        "After this session you should be able to:\n",
        "* Sample data from a specific probability distribution.\n",
        "* Use basic probability rules to infer parameters.\n",
        "* Understand and use the maximum likelihood principle.\n",
        "* Understand the Bayes' rule and the impact of a prior in the inference."
      ],
      "id": "53044446"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647e2160"
      },
      "source": [
        "## Imports and helper functions\n",
        "**Please execute the cell(s) below to initialize the notebook environment.**"
      ],
      "id": "647e2160"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69d47f08"
      },
      "source": [
        "#Import the libraries \n",
        "import numpy as np #for the math stuff\n",
        "import matplotlib.pyplot as plt #for the plot handling\n",
        "import scipy as sp\n",
        "from scipy.stats import norm\n",
        "from numpy.random import default_rng   # a default random number generator"
      ],
      "id": "69d47f08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c399337b"
      },
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets  # interactive display\n",
        "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label, interact_manual\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "# use NMA plot style\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "my_layout = widgets.Layout()"
      ],
      "id": "c399337b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ba945b"
      },
      "source": [
        "#@title Plotting Functions\n",
        "\n",
        "def plot_random_sample(x, y, figtitle = None):\n",
        "    \"\"\" Plot the random sample between 0 and 1 for both the x and y axes.\n",
        "\n",
        "    Args:\n",
        "        x (ndarray): array of x coordinate values across the random sample\n",
        "        y (ndarray): array of y coordinate values across the random sample\n",
        "        figtitle (str): title of histogram plot (default is no title)\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    plt.xlim([-0.25, 1.25]) # set x and y axis range to be a bit less than 0 and greater than 1\n",
        "    plt.ylim([-0.25, 1.25])\n",
        "    plt.scatter(dataX, dataY)\n",
        "    if figtitle is not None:\n",
        "        fig.suptitle(figtitle, size=16)\n",
        "    plt.show()\n",
        "\n",
        "def plot_random_walk(x, y, figtitle = None):\n",
        "    \"\"\" Plots the random walk within the range 0 to 1 for both the x and y axes.\n",
        "\n",
        "    Args:\n",
        "        x (ndarray): array of steps in x direction\n",
        "        y (ndarray): array of steps in y direction\n",
        "        figtitle (str): title of histogram plot (default is no title)\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(x,y,'b-o', alpha = 0.5)\n",
        "    plt.xlim(-0.1,1.1)\n",
        "    plt.ylim(-0.1,1.1)\n",
        "    ax.set_xlabel('x location')\n",
        "    ax.set_ylabel('y location')\n",
        "    plt.plot(x[0], y[0], 'go')\n",
        "    plt.plot(x[-1], y[-1], 'ro')\n",
        "\n",
        "    if figtitle is not None:\n",
        "        fig.suptitle(figtitle, size=16)\n",
        "    plt.show()\n",
        "\n",
        "def plot_hist(data, xlabel, figtitle = None, num_bins = None):\n",
        "    \"\"\" Plot the given data as a histogram.\n",
        "\n",
        "    Args:\n",
        "        data (ndarray): array with data to plot as histogram\n",
        "        xlabel (str): label of x-axis\n",
        "        figtitle (str): title of histogram plot (default is no title)\n",
        "        num_bins (int): number of bins for histogram (default is 10)\n",
        "\n",
        "    Returns:\n",
        "        count (ndarray): number of samples in each histogram bin\n",
        "        bins (ndarray): center of each histogram bin\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Count')\n",
        "    if num_bins is not None:\n",
        "        count, bins, _ = plt.hist(data, bins = num_bins)\n",
        "    else:\n",
        "        count, bins, _ = plt.hist(data, bins = np.arange(np.min(data)-.5, np.max(data)+.6)) # 10 bins default\n",
        "    if figtitle is not None:\n",
        "        fig.suptitle(figtitle, size=16)\n",
        "    plt.show()\n",
        "    return count, bins\n",
        "\n",
        "def my_plot_single(x, px):\n",
        "    \"\"\"\n",
        "    Plots normalized Gaussian distribution\n",
        "\n",
        "    Args:\n",
        "        x (numpy array of floats):     points at which the likelihood has been evaluated\n",
        "        px (numpy array of floats):    normalized probabilities for prior evaluated at each `x`\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "  \"\"\"\n",
        "    if px is None:\n",
        "        px = np.zeros_like(x)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, px, '-', color='C2', LineWidth=2, label='Prior')\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Orientation (Degrees)')\n",
        "\n",
        "def plot_gaussian_samples_true(samples, xspace, mu, sigma, xlabel, ylabel):\n",
        "    \"\"\" Plot a histogram of the data samples on the same plot as the gaussian\n",
        "    distribution specified by the give mu and sigma values.\n",
        "\n",
        "    Args:\n",
        "        samples (ndarray): data samples for gaussian distribution\n",
        "        xspace (ndarray): x values to sample from normal distribution\n",
        "        mu (scalar): mean parameter of normal distribution\n",
        "        sigma (scalar): variance parameter of normal distribution\n",
        "        xlabel (str): the label of the x-axis of the histogram\n",
        "        ylabel (str): the label of the y-axis of the histogram\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    # num_samples = samples.shape[0]\n",
        "\n",
        "    count, bins, _ = plt.hist(samples, density=True)\n",
        "    plt.plot(xspace, norm.pdf(xspace, mu, sigma),'r-')\n",
        "    plt.show()\n",
        "    \n",
        "def plot_likelihoods(likelihoods, mean_vals, variance_vals):\n",
        "    \"\"\" Plot the likelihood values on a heatmap plot where the x and y axes match\n",
        "    the mean and variance parameter values the likelihoods were computed for.\n",
        "\n",
        "    Args:\n",
        "        likelihoods (ndarray): array of computed likelihood values\n",
        "        mean_vals (ndarray): array of mean parameter values for which the\n",
        "                            likelihood was computed\n",
        "        variance_vals (ndarray): array of variance parameter values for which the\n",
        "                            likelihood was computed\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(likelihoods)\n",
        "\n",
        "    cbar = ax.figure.colorbar(im, ax=ax)\n",
        "    cbar.ax.set_ylabel('log likelihood', rotation=-90, va=\"bottom\")\n",
        "\n",
        "    ax.set_xticks(np.arange(len(mean_vals)))\n",
        "    ax.set_yticks(np.arange(len(variance_vals)))\n",
        "    ax.set_xticklabels(mean_vals)\n",
        "    ax.set_yticklabels(variance_vals)\n",
        "    ax.set_xlabel('Mean')\n",
        "    ax.set_ylabel('Variance')\n",
        "\n",
        "def posterior_plot(x, likelihood=None, prior=None, posterior_pointwise=None, ax=None):\n",
        "    \"\"\"\n",
        "    Plots normalized Gaussian distributions and posterior.\n",
        "\n",
        "    Args:\n",
        "        x (numpy array of floats):         points at which the likelihood has been evaluated\n",
        "        auditory (numpy array of floats):  normalized probabilities for auditory likelihood evaluated at each `x`\n",
        "        visual (numpy array of floats):    normalized probabilities for visual likelihood evaluated at each `x`\n",
        "        posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
        "        ax: Axis in which to plot. If None, create new axis.\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    if likelihood is None:\n",
        "        likelihood = np.zeros_like(x)\n",
        "\n",
        "    if prior is None:\n",
        "        prior = np.zeros_like(x)\n",
        "\n",
        "    if posterior_pointwise is None:\n",
        "        posterior_pointwise = np.zeros_like(x)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "    ax.plot(x, likelihood, '-C1', LineWidth=2, label='Auditory')\n",
        "    ax.plot(x, prior, '-C0', LineWidth=2, label='Visual')\n",
        "    ax.plot(x, posterior_pointwise, '-C2', LineWidth=2, label='Posterior')\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Orientation (Degrees)')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "def plot_classical_vs_bayesian_normal(num_points, mu_classic, var_classic,\n",
        "                                      mu_bayes, var_bayes):\n",
        "    \"\"\" Helper function to plot optimal normal distribution parameters for varying\n",
        "    observed sample sizes using both classic and Bayesian inference methods.\n",
        "\n",
        "    Args:\n",
        "        num_points (int): max observed sample size to perform inference with\n",
        "        mu_classic (ndarray): estimated mean parameter for each observed sample size\n",
        "                                using classic inference method\n",
        "        var_classic (ndarray): estimated variance parameter for each observed sample size\n",
        "                                using classic inference method\n",
        "        mu_bayes (ndarray): estimated mean parameter for each observed sample size\n",
        "                                using Bayesian inference method\n",
        "        var_bayes (ndarray): estimated variance parameter for each observed sample size\n",
        "                                using Bayesian inference method\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    xspace = np.linspace(0, num_points, num_points)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel('n data points')\n",
        "    ax.set_ylabel('mu')\n",
        "    plt.plot(xspace, mu_classic,'r-', label = \"Classical\")\n",
        "    plt.plot(xspace, mu_bayes,'b-', label = \"Bayes\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel('n data points')\n",
        "    ax.set_ylabel('sigma^2')\n",
        "    plt.plot(xspace, var_classic,'r-', label = \"Classical\")\n",
        "    plt.plot(xspace, var_bayes,'b-', label = \"Bayes\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "id": "35ba945b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc5db31"
      },
      "source": [
        "## Section 1: Probability distribution\n",
        "\n",
        "This section will be devoted to the exploration of some probability distribution.\n",
        "\n",
        "The goal is to give you the intuition about what is a probability distribution and why it can be useful for modeling."
      ],
      "id": "cfc5db31"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1761d1f9"
      },
      "source": [
        "### Coding Exercise 1.1: Create randomness\n",
        "\n",
        "Numpy has many functions and capabilities related to randomness.  We can draw random numbers from various probability distributions. For example, to draw 5 uniform numbers between 0 and 100, you would use `np.random.uniform(0, 100, size = (5,))`. \n",
        "\n",
        " We will use `np.random.seed` to set a specific seed for the random number generator. For example, `np.random.seed(0)` sets the seed as 0. By including this, we are actually making the random numbers reproducible, which may seem odd at first. Basically if we do the below code without that 0, we would get different random numbers every time we run it. By setting the seed to 0, we ensure we will get the same random numbers. There are lots of reasons we may want randomness to be reproducible.\n",
        "\n",
        "```\n",
        "np.random.seed(0)\n",
        "random_nums = np.random.uniform(0, 100, size = (5,))\n",
        "```\n",
        "\n",
        "Below, you will complete a function `generate_random_sample` that randomly generates `num_points` $x$ and $y$ coordinate values drawn from a uniform distribution, all within the range 0 to 1. You will then generate 10 points and visualize."
      ],
      "id": "1761d1f9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0b3b718"
      },
      "source": [
        "def generate_random_sample(num_points):\n",
        "    \"\"\" Generate a random sample containing a desired number of points (num_points)\n",
        "    in the range [0, 1] using a random number generator object.\n",
        "\n",
        "     Args:\n",
        "       num_points (int): number of points desired in random sample\n",
        "\n",
        "     Returns:\n",
        "       dataX, dataY (ndarray, ndarray): arrays of size (num_points,) containing x\n",
        "       and y coordinates of sampled points\n",
        "\n",
        "     \"\"\"\n",
        "    ###################################################################\n",
        "    ## TODO for students: Draw the uniform numbers\n",
        "    ###################################################################\n",
        "    dataX = ...\n",
        "    dataY = ...\n",
        "    \n",
        "    return dataX, dataY\n",
        "    ###################################################################\n",
        "\n",
        "# Set a seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set number of points to draw\n",
        "num_points = 10\n",
        "\n",
        "# Draw random points\n",
        "dataX, dataY = generate_random_sample(num_points)\n",
        "    \n",
        "# Visualize\n",
        "with plt.xkcd():\n",
        "    plot_random_sample(dataX, dataY, \"Random sample of 10 points\")"
      ],
      "id": "a0b3b718",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc027602"
      },
      "source": [
        "### Coding exercice 1.2: Gaussian Distribution\n",
        "\n",
        "The most widely used continuous distribution is probably the Gaussian distribution. It is extremely common across all kinds of statistical analyses. Because of the central limit theorem, many quantities are Gaussian distributed. Gaussians also have some nice mathematical properties that permit simple closed-form solutions to several important problems. \n",
        "\n",
        "As a working example, imagine that a human participant is asked to point in the direction where they perceived a sound coming from. As an approximation, we can assume that the direction they point towards is Gaussian distributed, with a mean on target and variance depending on the error range. \n",
        "\n",
        "In this exercise, you will implement a Gaussian by filling in the missing portions of code for the function `my_gaussian` below. Gaussians have two parameters. The **mean** $\\mu$, which sets the location of its center, and its \"scale\" or spread is controlled by its **standard deviation** $\\sigma$, or **variance** $\\sigma^2$ (i.e. the square of standard deviation). **Be careful not to use one when the other is required.**\n",
        "\n",
        "The equation for a Gaussian probability density function is:\n",
        "$$\n",
        "f(x;\\mu,\\sigma^2)=\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "In Python $\\pi$ and $e$ can be written as `np.pi` and `np.exp` respectively.\n",
        "\n",
        "As a probability distribution this has an integral of one when integrated from $-\\infty$ to $\\infty$. However, in the following, your numerical Gaussian will only be computed over a finite number of points. You therefore need to explicitly normalize it to sum to one yourself based on the `step_size` used. \n",
        "\n",
        "You can test your function with $\\mu= 1$ and $\\sigma = 1.5$"
      ],
      "id": "dc027602"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0e05afc"
      },
      "source": [
        "def my_gaussian(x_points, mu, sigma):\n",
        "    \"\"\" Returns normalized Gaussian estimated at points `x_points`, with\n",
        "    parameters: mean `mu` and standard deviation `sigma`\n",
        "\n",
        "    Args:\n",
        "      x_points (ndarray of floats): points at which the gaussian is evaluated\n",
        "      mu (scalar): mean of the Gaussian\n",
        "      sigma (scalar): standard deviation of the gaussian\n",
        "\n",
        "    Returns:\n",
        "      (numpy array of floats) : normalized Gaussian evaluated at `x`\n",
        "    \"\"\"\n",
        "    ###################################################################\n",
        "    ## TODO for students: Implement the gaussian equation\n",
        "    ###################################################################\n",
        "    px = ...\n",
        "    #Do not forget to normalize\n",
        "    \n",
        "    return px\n",
        "#Choose some parameters\n",
        "mu = ...\n",
        "sigma = ...\n",
        "   \n",
        "###################################################################\n",
        "#Create data  \n",
        "step_size = 0.1\n",
        "x_points = np.arange(-5, 5, step_size)\n",
        "\n",
        "#Generate the Gaussian\n",
        "Gaussian = my_gaussian(x_points, mu, sigma)\n",
        "\n",
        "#Visualise\n",
        "my_plot_single(x_points, Gaussian)"
      ],
      "id": "a0e05afc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coCXopw26tnj"
      },
      "source": [
        "*Example output:*\n",
        "\n",
        "\n",
        "<img alt='Solution hint' align='left' width=413 height=300 src=https://raw.githubusercontent.com/svandergoote/LGBIO2060-2021/master/Solutions/TP1_ex1.PNG>"
      ],
      "id": "coCXopw26tnj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fc51412"
      },
      "source": [
        "Questions:\n",
        "* What changes do you observe by varying the mean ?\n",
        "* What changes do you observe by varying the variance ?"
      ],
      "id": "6fc51412"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1743733d"
      },
      "source": [
        "## Section 2: Statistical inference\n",
        "The goal of this section is to explain how it is possible to do inference by inverting the generative process.\n",
        "\n",
        "By completing the exercises in this tutorial, you should:\n",
        "* understand what the likelihood function is, and have some intuition of why it is important\n",
        "* know how to summarise the Gaussian distribution using mean and variance \n",
        "* know how to maximise a likelihood function\n",
        "* be able to do simple inference in both classical and Bayesian ways"
      ],
      "id": "1743733d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b543ff2"
      },
      "source": [
        "### Section 2.1: Basic rules of probability\n",
        "\n",
        "Definitions: \n",
        "* $P(A) \\in [ \\,0,1 ]\\,$ is the probability that event A is true.\n",
        "* $P(\\neg\\, A) = 1 - P(A)$ is the **complementary probability** of the event A.\n",
        "* $P(A \\cap B) = P(A | B)  P(B) = P(B | A)P(A) = P(A,B)$ is the **joint probability** of events A and B. Meaning both events are true. If the two events are independant, $P(A \\cap B) = P(A)  P(B)$.\n",
        "* $P(A | B) = P(A \\cap B)/P(B)$ is the **conditional probability** of the event A being true given the event B is true.\n",
        "* $P(A) = \\sum_{B} P(A,B) = \\sum_B P(A|B)P(B)$ is the **marginal probability** of the event A."
      ],
      "id": "6b543ff2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb122a2a"
      },
      "source": [
        "### Math exercice 2.1: Example\n",
        "In order to check if those formula are clear to you, let's take a small example to illustrate them.\n",
        "\n",
        "Imagine that for the past 10 years, hundreds of students' opinions on the LGBIO2060 course have been gathered aswell as if they have pass the exam. It turns out that on average 60% of the students actually enjoyed the course and 30% succeed at the exam (Those numbers are fictional).\n",
        "\n",
        "We will use the following notations:\n",
        "\n",
        "$P(E)=0.6$ is the probability that students enjoyed the course.\n",
        "\n",
        "$P(S)=0.3$ is the probability that students succeed at the exam."
      ],
      "id": "cb122a2a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf947068"
      },
      "source": [
        "#### A) Product\n",
        "Assuming that enjoying the course and passing the exam are two indepedent events, what is the probability that a randomly chosen student enjoyed the course and passed the exam ? i.e $P(E \\cap S)$"
      ],
      "id": "cf947068"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b083e83f"
      },
      "source": [
        "#Enter your answer here\n",
        "p_joint = ..."
      ],
      "id": "b083e83f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "c055097b"
      },
      "source": [
        "# @title Test your answer\n",
        "if p_joint == 0.18:\n",
        "    print('Your answer is correct, congratulations !!!')\n",
        "else:\n",
        "    print('Ho :( Your answer is not the expected one')"
      ],
      "id": "c055097b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c65dafb"
      },
      "source": [
        "#### B) Joint probability\n",
        "Unfortunately for you, Clémence tells you that the two variables are not independent. She gives you the following useful information: of those student that enjoyed the course, only 20 percent also succeeded in the exam, i.e. the probability of a student succeeding in the exam given that he enjoyed the course is $P(S|E)=0.2$.\n",
        "\n",
        "Given this new information, what is the probability that a randomly chosen student succeeds in the exam and enjoys the course ?"
      ],
      "id": "1c65dafb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b3494a0"
      },
      "source": [
        "#Enter your answer here\n",
        "p_joint_not_independent = ..."
      ],
      "id": "4b3494a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "93da44cb"
      },
      "source": [
        "# @title Test your answer \n",
        "if p_joint_not_independent == 0.12:\n",
        "    print('Your answer is correct, congratulations !!!')\n",
        "else:\n",
        "    print('Ho :( Your answer is not the expected one')"
      ],
      "id": "93da44cb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e381ab42"
      },
      "source": [
        "#### C) Conditional probability\n",
        "You ask a random student that followed this course a few years ago and you find out that he obtained the grade of 17. What is the probability that he enjoyed the course $P(E|S)$?"
      ],
      "id": "e381ab42"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ed7f980"
      },
      "source": [
        "#Enter your answer here \n",
        "p_cond = ..."
      ],
      "id": "1ed7f980",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "0dfcffbe"
      },
      "source": [
        "# @title Test your answer \n",
        "if p_cond == 0.4:\n",
        "    print('Your answer is correct, congratulations !!!')\n",
        "else:\n",
        "    print('Ho :( Your answer is not the expected one')"
      ],
      "id": "0dfcffbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cc7d3af"
      },
      "source": [
        "#### D) Marginal probability\n",
        "Let's check that everything has been done correctly. Based on our knowledge about the conditional probabilities, we should be able to use marginalisation to recover the marginal probability of a random student to succeed the course. We know that it should be $P(E)=0.6$.\n",
        "\n",
        "Check that you obtain the same result based on conditional probabilities for $P(E|S)$ and $P(E|\\neg S)$ using the marginalization formula. You can use $P(E|\\neg S)=0.685$ but it is a good exercice to check if you can compute it by yourself."
      ],
      "id": "0cc7d3af"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "539a9105"
      },
      "source": [
        "#Enter your code here\n",
        "p_cond_succeed = ...\n",
        "p_cond_notSucceed = ...\n",
        "\n",
        "p_enjoy = ... * ... + ... * ...\n",
        "#Visualise the result\n",
        "print(p_enjoy)"
      ],
      "id": "539a9105",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "d3b9a9ea"
      },
      "source": [
        "# @title Test your answer\n",
        "p_enjoy_true = 0.6\n",
        "if (np.abs(p_enjoy - p_enjoy_true)<0.015):\n",
        "    print('Your answer is correct, congratulations !!!')\n",
        "else:\n",
        "    print('Ho :( Your answer is not the expected one')"
      ],
      "id": "d3b9a9ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1b54b8f"
      },
      "source": [
        "### Section 2.2 Likelihoods\n",
        "Most of the times when you are trying to model something you have two things:\n",
        "\n",
        "* Observations/data **x**.\n",
        "* A probabilistic model $P(x|\\theta)$\n",
        "\n",
        "And your goal is to estimate the hidden properties $\\theta$ that gave rise to the data **x**.\n",
        "\n",
        "For example, if your probabilistic model is a Gaussian distribution $\\mathcal{N}(x_i,\\mu,\\sigma)$ your goal is to find the parameters $\\theta=\\{\\mu,\\sigma\\}$ that maximise the probability that your data **x** were obtained given those parameters.\n",
        "\n",
        "A classical method to achieve such result is the **maximum likelihood**, which consists to maximise the probability of model with regard to $\\theta$.\n",
        "\n",
        "$$\\hat\\theta = argmax \\, P(x|\\theta)$$\n",
        "\n",
        "This equation translates the fact the that you want to find the parameters $\\theta$ that maximize the probability that your data **x** were indeed obtained by your probabilistic model.\n",
        "\n",
        "In other words, if we do not know the parameters $\\mu$, $\\sigma$ that generated the data, we can try to **infer** which parameter values (given our model) gives the best (highest) likelihood. This is what we call statistical inference: trying to infer what parameters make our observed data the most likely or probable?"
      ],
      "id": "a1b54b8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04d4f4e"
      },
      "source": [
        "### Coding Exercise 2.2: Computing likelihood\n",
        "\n",
        "Let's start with computing the likelihood of some set of data points being drawn from a Gaussian distribution with a mean and variance we choose. \n",
        "\n",
        "\n",
        "\n",
        "As multiplying small probabilities together can lead to very small numbers, it is often convenient to report the *logarithm* of the likelihood. This is just a convenient transformation and as logarithm is a monotonically increasing function this does not change what parameters maximise the function.\n",
        "\n",
        "Here you have to implement the log-likelihood in `compute_likelihood_normal`:\n",
        "$$Log \\, Likelihood = \\sum_{i=1}^n log\\left(P(x_i)\\right)$$\n"
      ],
      "id": "f04d4f4e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de42220a"
      },
      "source": [
        "def compute_likelihood_normal(x, mean_val, standard_dev_val):\n",
        "  \"\"\" Computes the log-likelihood values given a observed data sample x, and\n",
        "  potential mean and variance values for a normal distribution\n",
        "\n",
        "    Args:\n",
        "      x (ndarray): 1-D array with all the observed data\n",
        "      mean_val (scalar): value of mean for which to compute likelihood\n",
        "      standard_dev_val (scalar): value of variance for which to compute likelihood\n",
        "\n",
        "    Returns:\n",
        "      likelihood (scalar): value of likelihood for this combination of means/variances\n",
        "  \"\"\"\n",
        "\n",
        "  ###################################################################\n",
        "  ## TODO for student\n",
        "  ###################################################################\n",
        "\n",
        "  # Get probability of each data point \n",
        "  p_data = ...\n",
        "\n",
        "  # Compute likelihood\n",
        "  likelihood = ...\n",
        "\n",
        "  return likelihood\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "# Compute likelihood for a guessed mean/standard dev\n",
        "guess_mean = 4\n",
        "guess_standard_dev = .1\n",
        "likelihood = compute_likelihood_normal(x, guess_mean, guess_standard_dev)\n",
        "print(likelihood)"
      ],
      "id": "de42220a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78e3d1e5"
      },
      "source": [
        "You should get a likelihood of -92904.81. This is somewhat meaningless to us! For it to be useful, we need to compare it to the likelihoods computed using other guesses of the mean or standard deviation. The visualization below shows us the likelihood for various values of the mean and the standard deviation. Essentially, we are performing a rough grid-search over means and standard deviations.  What would you guess as the true mean and standard deviation based on this visualization? "
      ],
      "id": "78e3d1e5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "a1e3e75d"
      },
      "source": [
        "# @title Execute to visualize likelihoods\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "\n",
        "# Compute likelihood for different mean/variance values\n",
        "mean_vals = np.linspace(1, 10, 10) # potential mean values to ry\n",
        "standard_dev_vals = np.array([0.7, 0.8, 0.9, 1, 1.2, 1.5, 2, 3, 4, 5]) # potential variance values to try\n",
        "\n",
        "# Initialise likelihood collection array\n",
        "likelihood = np.zeros((mean_vals.shape[0], standard_dev_vals.shape[0]))\n",
        "\n",
        "# Compute the likelihood for observing the gvien data x assuming\n",
        "# each combination of mean and variance values\n",
        "for idxMean in range(mean_vals.shape[0]):\n",
        "  for idxVar in range(standard_dev_vals .shape[0]):\n",
        "    likelihood[idxVar,idxMean]= sum(np.log(norm.pdf(x, mean_vals[idxMean],\n",
        "                                              standard_dev_vals[idxVar])))\n",
        "\n",
        "# Uncomment once you've generated the samples and compute likelihoods\n",
        "xspace = np.linspace(0, 10, 100)\n",
        "plot_likelihoods(likelihood, mean_vals, standard_dev_vals)"
      ],
      "id": "a1e3e75d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0769a1c8"
      },
      "source": [
        "### Section 2.2.1: Searching for best parameters\n",
        "\n",
        "We want to do inference on this data set, i.e. we want to infer the parameters that most likely gave rise to the data given our model. Intuitively that means that we want as good as possible a fit between the observed data and the probability distribution function with the best inferred parameters. We can search for the best parameters manually by trying out a bunch of possible values of the parameters, computing the likelihoods, and picking the parameters that resulted in the highest likelihood. "
      ],
      "id": "0769a1c8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37db783c"
      },
      "source": [
        "#### Interactive Demo 2.2: Maximum likelihood inference\n",
        "\n",
        "Try to see how well you can fit the probability distribution to the data by using the demo sliders to control the mean and standard deviation parameters of the distribution. We will visualize the histogram of data points (in blue) and the Gaussian density curve with that mean and standard deviation (in red). Below, we print the log-likelihood.\n",
        "\n",
        "- What (approximate) values of mu and sigma result in the best fit?\n",
        "- How does the value below the plot (the log-likelihood) change with the quality of fit?"
      ],
      "id": "37db783c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "41b51288"
      },
      "source": [
        "# @title Make sure you execute this cell to enable the widget and fit by hand!\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "vals = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "def plotFnc(mu,sigma):\n",
        "  loglikelihood= sum(np.log(norm.pdf(vals,mu,sigma)))\n",
        "  #calculate histogram\n",
        "\n",
        "  #prepare to plot\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('probability')\n",
        "\n",
        "  #plot histogram\n",
        "  count, bins, ignored = plt.hist(vals,density=True)\n",
        "  x = np.linspace(0,10,100)\n",
        "\n",
        "  #plot pdf\n",
        "  plt.plot(x, norm.pdf(x,mu,sigma),'r-')\n",
        "  plt.show()\n",
        "  print(\"The log-likelihood for the selected parameters is: \" + str(loglikelihood))\n",
        "\n",
        "#interact(plotFnc, mu=5.0, sigma=2.1);\n",
        "#interact(plotFnc, mu=widgets.IntSlider(min=0.0, max=10.0, step=1, value=4.0),sigma=widgets.IntSlider(min=0.1, max=10.0, step=1, value=4.0));\n",
        "interact(plotFnc, mu=(0.0, 15.0, 0.1),sigma=(0.1, 5.0, 0.1));"
      ],
      "id": "41b51288",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUOfjg2mComQ"
      },
      "source": [
        "### Section 2.2.2: Analytical solution\n",
        "\n",
        "Sometimes, things work out well and we can come up with formulas for the maximum likelihood estimates of parameters. We won't get into this further but basically we could set the derivative of the likelihood to 0 (to find a maximum) and solve for the parameters. This won't always work but for the Gaussian distribution, it does.\n",
        "\n",
        "Specifically , the special thing about the Gaussian is that mean and standard deviation of the random sample can effectively approximate the two parameters of a Gaussian, $\\mu, \\sigma$.\n",
        "\n",
        "\n",
        "Hence using the  mean, $\\bar{x}=\\frac{1}{n}\\sum_i x_i$, and variance, $\\bar{\\sigma}^2=\\frac{1}{n} \\sum_i (x_i-\\bar{x})^2 $ of the sample should give us the best/maximum likelihood.\n",
        "\n",
        "Complete the missing parts in the following piece of code to observe what are the estimate you find with the analytical method."
      ],
      "id": "NUOfjg2mComQ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpsQMdjkC0bE"
      },
      "source": [
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "###################################################################\n",
        "  ## TODO for student\n",
        "###################################################################\n",
        "# Compute and print sample means and standard deviations\n",
        "sample_mean = ...\n",
        "sample_std = ...\n",
        "\n",
        "print(\"This is the sample mean as estimated by analytical method: \" + str(sample_mean))\n",
        "print(\"This is the sample standard deviation as estimated by analytical method: \" + str(sample_std))"
      ],
      "id": "KpsQMdjkC0bE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "132241fd"
      },
      "source": [
        "### Section 2.2.3: Numerical optimization to find parameters\n",
        "Sometimes, it is not possible to find the optimal parameters analitycally and numerical methods must be used.\n",
        "\n",
        "\n",
        "Let's again assume that we have a data set, $\\mathbf{x}$, assumed to be generated by a normal distribution.\n",
        "We want to maximise the likelihood of the parameters $\\mu$ and $\\sigma^2$. We can do so using a couple of tricks:\n",
        "\n",
        "*   Using a log transform will not change the maximum of the function, but will allow us to work with very small numbers that could lead to problems with machine precision.\n",
        "*   Maximising a function is the same as minimising the negative of a function, allowing us to use the minimize optimisation provided by scipy.\n",
        "\n",
        "The optimisation could be done using `sp.optimize.minimize`, which does a version of gradient descent (there are hundreds of ways to do numerical optimisation, we will not cover these here!)."
      ],
      "id": "132241fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c08a4a7"
      },
      "source": [
        "---\n",
        "# Section 3: Bayesian Inference"
      ],
      "id": "9c08a4a7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b107fd5"
      },
      "source": [
        "For Bayesian inference we do not focus on the likelihood function $L(y)=P(x|y)$, but instead focus on the posterior distribution: \n",
        "\n",
        "$$P(y|x)=\\frac{P(x|y)P(y)}{P(x)}$$\n",
        "\n",
        "which is composed of the **likelihood** function $P(x|y)$, the **prior** $P(y)$ and a normalising term $P(x)$\n",
        "\n",
        "hile there are other advantages to using Bayesian inference (such as the ability to derive Bayesian Nets), we will start by focusing on the role of the prior in inference. Does including prior information allow us to infer parameters in a better way?"
      ],
      "id": "7b107fd5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "235a314f"
      },
      "source": [
        "### Think! 3.1: Bayesian inference with Gaussian distribution\n",
        "\n",
        "In the above sections we performed inference using maximum likelihood, i.e. finding the parameters that maximised the likelihood of a set of parameters, given the model and data.\n",
        "\n",
        "We will now repeat the inference process, but with an added Bayesian prior, and compare it to the \"classical\" inference (maximum likelihood) process we did before (Section 2).\n",
        "\n",
        "For the prior we start by guessing a mean of 5 (mean of previously observed data points 4 and 6) and variance of 1 (variance of 4 and 6). We use a trick (not detailed here) that is a simplified way of applying a prior, that allows us to just add these 2 values (pseudo-data) to the real data.\n",
        "\n",
        "See the visualization below that shows the mean and standard deviation inferred by our classical maximum likelihood approach and the Bayesian approach for different numbers of data points.\n",
        "\n",
        "Remembering that our true values are $\\mu = 5$, and $\\sigma^2 = 1$, how do the Bayesian inference and classical inference compare?"
      ],
      "id": "235a314f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "da423900"
      },
      "source": [
        "# @title Execute to visualize inference\n",
        "\n",
        "def classic_vs_bayesian_normal(mu, sigma, num_points, prior):\n",
        "  \"\"\" Compute both classical and Bayesian inference processes over the range of\n",
        "  data sample sizes (num_points) for a normal distribution with parameters\n",
        "  mu,sigma for comparison.\n",
        "\n",
        "  Args:\n",
        "    mu (scalar): the mean parameter of the normal distribution\n",
        "    sigma (scalar): the standard deviation parameter of the normal distribution\n",
        "    num_points (int): max number of points to use for inference\n",
        "    prior (ndarray): prior data points for Bayesian inference\n",
        "\n",
        "  Returns:\n",
        "    mean_classic (ndarray): estimate mean parameter via classic inference\n",
        "    var_classic (ndarray): estimate variance parameter via classic inference\n",
        "    mean_bayes (ndarray): estimate mean parameter via Bayesian inference\n",
        "    var_bayes (ndarray): estimate variance parameter via Bayesian inference\n",
        "  \"\"\"\n",
        "\n",
        "  # Initialize the classical and Bayesian inference arrays that will estimate\n",
        "  # the normal parameters given a certain number of randomly sampled data points\n",
        "  mean_classic = np.zeros(num_points)\n",
        "  var_classic = np.zeros(num_points)\n",
        "\n",
        "  mean_bayes = np.zeros(num_points)\n",
        "  var_bayes = np.zeros(num_points)\n",
        "\n",
        "  for nData in range(num_points):\n",
        "\n",
        "    random_num_generator = default_rng(0)\n",
        "    x = random_num_generator.normal(mu, sigma, nData + 1)\n",
        "\n",
        "    # Compute the mean of those points and set the corresponding array entry to this value\n",
        "    mean_classic[nData] = np.mean(x)\n",
        "\n",
        "    # Compute the variance of those points and set the corresponding array entry to this value\n",
        "    var_classic[nData] = np.var(x)\n",
        "\n",
        "    # Bayesian inference with the given prior is performed below for you\n",
        "    xsupp = np.hstack((x, prior))\n",
        "    mean_bayes[nData] = np.mean(xsupp)\n",
        "    var_bayes[nData] = np.var(xsupp)\n",
        "\n",
        "  return mean_classic, var_classic, mean_bayes, var_bayes\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set normal distribution parameters, mu and sigma\n",
        "mu = 5\n",
        "sigma = 1\n",
        "\n",
        "# Set the prior to be two new data points, 4 and 6, and print the mean and variance\n",
        "prior = np.array((4, 6))\n",
        "print(\"The mean of the data comprising the prior is: \" + str(np.mean(prior)))\n",
        "print(\"The variance of the data comprising the prior is: \" + str(np.var(prior)))\n",
        "\n",
        "mean_classic, var_classic, mean_bayes, var_bayes = classic_vs_bayesian_normal(mu, sigma, 60, prior)\n",
        "plot_classical_vs_bayesian_normal(60, mean_classic, var_classic, mean_bayes, var_bayes)"
      ],
      "id": "da423900",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRU6Ayoam8I3"
      },
      "source": [
        "#### Discuss the impact of the prior on the on the inference of the mean and the variance."
      ],
      "id": "lRU6Ayoam8I3"
    }
  ]
}