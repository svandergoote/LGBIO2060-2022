{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2022/blob/main/LGBIO2060_TP6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "02woErAA9Dso"
      },
      "source": [
        "## LGBIO2060 Exercise session 6\n",
        "\n",
        "# Optimal control of a discrete variable\n",
        "\n",
        "__Authors:__ Simon Vandergooten, Cl√©mence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hZmkZMa49Dsq"
      },
      "source": [
        "---\n",
        "# Tutorial Objectives\n",
        "\n",
        "In this tutorial, we will implement a **binary control** task: a Partially Observable Markov Decision Process (POMDP) that describes fishing. The agent (you) seeks reward from two fishing sites without directly observing where the school of fish is. This makes the world a Hidden Markov Model (HMM). Based on when and where you catch fish, you keep updating your belief about the fish location, i.e., the posterior of the fish given past observations. You should control your position to get the most fish while minimizing the cost of switching sides.\n",
        "\n",
        "You've already learned about stochastic dynamics, latent states, and measurements. Now we introduce **actions**, based on the new concepts of **control, utility, and policy**. This general structure provides a foundational model for the brain's computations because it includes a perception-action loop where the animal can gather information, draw inferences about its environment, and select actions with the greatest benefit.\n",
        "\n",
        "In this tutorial, you will:\n",
        "* Use the Hidden Markov Models you learned about previously to model the world state.\n",
        "* Use the observations (fish caught) to build beliefs (posterior distributions) about the fish location.\n",
        "* Evaluate the quality of different control policies for choosing actions.\n",
        "* Discover the policy that maximizes utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "hV0ze2Jl9Dsr"
      },
      "source": [
        "---\n",
        "## Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "tags": [],
        "id": "i8L3vUh39Dss"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "from math import isclose\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Plotting functions\n",
        "def plot_states(S):\n",
        "  # 0 represents left and 1 right\n",
        "  n = np.size(S)\n",
        "  X = np.arange(n)\n",
        "  label = [\"left\", \"right\"]\n",
        "\n",
        "  ax = plt.subplot(1, 1, 1)\n",
        "  ax.plot(X,S)\n",
        "  ax.set_title('Evolution of the fish location')\n",
        "  ax.set_xlabel('Time')\n",
        "  ax.set_ylabel('Location')\n",
        "  ax.set_yticks(np.array([0, 1]))\n",
        "  ax.set_yticklabels(label)\n",
        "\n",
        "def plot_fishingSituation(S, my_side, M):\n",
        "  # We will add stars if we caught a fish\n",
        "  n = np.size(S)\n",
        "  X = np.arange(n)\n",
        "\n",
        "  ax = plt.subplot(1, 1, 1)\n",
        "  ax.plot(X,S)\n",
        "  ax.set_title('Evolution of the fish location')\n",
        "  ax.set_xlabel('Time')\n",
        "  ax.set_ylabel('Location')\n",
        "  ax.set_yticks(np.array([0, 1]))\n",
        "  ax.set_yticklabels([\"left\", \"right\"])\n",
        "\n",
        "  # add symbols representing the side the fisherman is\n",
        "  ax.plot(X, my_side,'o', label='my side',markersize=2)\n",
        "  ax.legend(loc=\"upper right\")\n",
        "\n",
        "  # add stars representing fish catch\n",
        "  idx = np.where(M == 1)\n",
        "  ax.plot(X[idx], my_side[idx], 'r*',label='catch')\n",
        "  ax.legend(loc=\"upper right\")\n",
        "\n",
        "\n",
        "def plot_Policy(S,belief, my_side, M, threshold,actions):\n",
        "  # First subplot will be the evolution of the belief\n",
        "  n = np.size(S)\n",
        "  X = np.arange(n)\n",
        "  n_show = 50\n",
        "  ax = plt.subplot(2, 1, 1)\n",
        "  ax.plot(X[:n_show],belief[0,:n_show])\n",
        "  ax.set_title('Evolution of the belief about the right side')\n",
        "  ax.set_xlabel('Time')\n",
        "  ax.set_ylabel('Belief')\n",
        "\n",
        "  ax.plot(X[:n_show],np.repeat(threshold,n_show),label='Threshold')\n",
        "  # Stars for actions\n",
        "  idx = np.where(np.array(actions[:n_show]) == \"switch\")\n",
        "  # we keep only the switch associated to right to left\n",
        "  idx_right = np.where(np.array(my_side[:n_show]) == 1)\n",
        "  idx = np.intersect1d(idx, idx_right)\n",
        "  X_restricted = X[:n_show]\n",
        "  ax.plot(X_restricted[idx],np.repeat(threshold,np.size(idx)), 'r*',label='switch')\n",
        "  ax.legend(loc='lower right')\n",
        "\n",
        "  # Are we on the same side as the fishes ?\n",
        "  ax = plt.subplot(2, 1, 2)\n",
        "  ax.plot(X[:n_show], S[:n_show], label='fish')\n",
        "  ax.plot(X[:n_show], my_side[:n_show],'o', label='my side',markersize=2)\n",
        "  ax.legend(loc=\"center right\")\n",
        "  ax.set_xlabel('Time')\n",
        "  ax.set_ylabel('Location')\n",
        "  ax.set_yticks(np.array([0, 1]))\n",
        "  ax.set_yticklabels([\"left\", \"right\"])"
      ],
      "metadata": {
        "id": "eMo6tcehDS_g",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "WBP1Aym59Dsu"
      },
      "source": [
        "---\n",
        "# Section 1: Analyzing the Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Jc5OOm7n9Dsu"
      },
      "source": [
        "**Problem Setting**\n",
        "\n",
        "*1. State dynamics:* There are two possible locations for the fish: Left and Right. Secretly, at each time step, the fish may switch sides with a certain probability $p_{\\rm sw} = 1 - p_{\\rm stay}$. This is the binary switching model. The fish location, $s^{\\rm fish}$, is latent; you get measurements about it when you try to catch fish. This gives you a *belief* or posterior probability of the current location given your history of measurements.\n",
        "\n",
        "*2. Actions:* Unlike past practical sessions, you can now **act** on the process! You may stay on your current location (Left or Right), or switch to the other side.\n",
        "\n",
        "*3. Rewards and Costs:* You get rewarded for each fish you catch (one fish is worth 1 \"point\"). If you're on the same side as the fish, you'll catch more, with probability $q_{\\rm high}$ per discrete time step. Otherwise, you may still catch some fish with probability $q_{\\rm low}$. However, you pay a price of $C$ points for switching to the other side. So you better decide wisely!\n",
        "\n",
        "<br>\n",
        "\n",
        "**Maximizing Utility**\n",
        "\n",
        "To decide \"wisely\" and maximize your total utility (total points), you will follow a **policy** that prescribes what to do in any situation. Here the situation is determined by your location and your **belief** $b_t$ (posterior) about the fish location (remember that the fish location is a latent variable).\n",
        "\n",
        "In optimal control theory, the belief is the posterior probability over the latent variable given all the past measurements. It can be shown that maximizing the expected utility with respect to this posterior is optimal.\n",
        "\n",
        "In our problem, the belief can be represented by a single number because the fish are either on the left or the right side. So we write:\n",
        "\n",
        "\\begin{equation}\n",
        "b_t = p(s^{\\rm fish}_t = {\\rm Right}\\  |\\  m_{0:t}, a_{0:t-1})\n",
        "\\end{equation}\n",
        "\n",
        "where $m_{0:t}$ are the measurements and $a_{0:t-1}$ are the actions (stay or switch).\n",
        "\n",
        "Finally, we will parameterize the policy by a simple threshold on beliefs: when your belief that fish are on your current side falls below a threshold $\\theta$, you switch to the other side.\n",
        "\n",
        "You will discover that if you pick the right threshold, this simple policy happens to be optimal!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## State dynamics\n",
        "\n",
        "Let's start by defining the HMM that describes the fish dynamics. \\\\\n",
        "When $s = 0$, the school of fish is on the **left** side; when $s=1$, the school of fish is on the **right** side.  "
      ],
      "metadata": {
        "id": "eG-n1MFAag5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_state(switch_proba, start_proba, n_states):\n",
        "  '''\n",
        "  Create an HMM binary state variable.\n",
        "  Args:\n",
        "    switch_proba (array): the probabilities to switch from states. [rightToLeft, LeftToRight]\n",
        "    start_proba (array): the initial probabilities of being on each side. [p_right, p_left]\n",
        "    n_states (int): the number of time steps\n",
        "\n",
        "  Returns:\n",
        "    S (array): the vector of state for each time step.\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "  #Initialize S\n",
        "  S = np.zeros(n_states)\n",
        "\n",
        "  #Step 1: Initial state (Hint: np.random.choice)\n",
        "  S[0] = ...\n",
        "\n",
        "  #Step 2: Transition matrix\n",
        "  T = ...\n",
        "\n",
        "  #Step 3: Iterate on each time step to find the new state s[t] based on S[t-1]\n",
        "\n",
        "\n",
        "\n",
        "  return S\n",
        "\n",
        "#Set random seed\n",
        "np.random.seed(54)\n",
        "\n",
        "#Set parameters of HMM\n",
        "switch_proba = np.array([0.2, 0.2])\n",
        "start_proba = np.array([0, 1]) #The initial state is left (0)\n",
        "n_states = 50\n",
        "\n",
        "#Generate the hidden states vector\n",
        "S = generate_state(switch_proba, start_proba, n_states)\n",
        "\n",
        "#Plot state evolution\n",
        "plot_states(S)"
      ],
      "metadata": {
        "id": "sPp11t_iafjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "YfUO4TlE9Dsv"
      },
      "source": [
        "---\n",
        "# Section 2: Catching fish"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have created the hidden states, i.e., the true position of the fish, you will create the binary measurements vector *M* from it.\n",
        "\n",
        "Recall that in reality we don't have access to the hidden states but only to noisy measurements that give us information about those hidden states we want to infer.\n",
        "\n",
        "You will implement the function `sample` that generates one sample $m_t$ based on the hidden state $s_t$ and the side you are fishing $side_t$.\n",
        "- $m_t=0$ if no catch and 1 if catch\n",
        "- If you're on the same side as the fish you have a *high* probability of reward  to catch one fish, and a *low* probability of reward otherwise. Note that therefore, these two probabilities must not necesserality sum to 1, why ?\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OpHeB7trYDo8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(S, rewards_prob, my_side):\n",
        "  '''\n",
        "  Create a binary measurement from HMM states\n",
        "\n",
        "    Args:\n",
        "      S (int): The hidden state, i.e. the side of the school of fishes (0 if left, 1 if right)\n",
        "      rewards_prob (array): [high low], with high (low) being the probability of catching a fish when you are on the correct (wrong) side.\n",
        "      my_side (int): Your side (0 if left, 1 if right)\n",
        "\n",
        "    Returns:\n",
        "      M (int): The binary measurements (1 if you catch a fish; 0 if not).\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "\n",
        "\n",
        "  return M\n",
        "\n",
        "\n",
        "np.random.seed(56)\n",
        "rewards_prob = np.array([0.8, 0.1])\n",
        "# Lazy strategy : we stay always on the left side\n",
        "my_side = np.zeros_like(S)\n",
        "\n",
        "#Generate the measurements vector M\n",
        "M = np.zeros_like(S)\n",
        "for t in range(len(S)):\n",
        "  M[t] = sample(S[t], rewards_prob, my_side[t])\n",
        "\n",
        "\n",
        "plot_fishingSituation(S, my_side, M)"
      ],
      "metadata": {
        "id": "lPKvRV91a6B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "KSo19Pgs9Dsw"
      },
      "source": [
        "---\n",
        "# Section 3: Belief dynamics and belief distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now compute the belief about the fish location thanks to our measurement, i.e. did we catch a fish or not, and our prior knowledge of the HMM dynamics.\n",
        "In this exercice, you will implement the function *belief_update* that updates the posterior probability about the fish's location given our measurements $p(s_t|m_{0:t})$.\n",
        "\n",
        "To compute the likelihood, remember that it corresponds to the probability of getting a specific measurement given a latent state. To compute the posterior probability at the instant *t* of the fishes being left (resp. right), you need compute the probability of getting $m_t$ given that the school of fish is on the left (right) side.  "
      ],
      "metadata": {
        "id": "8mWik9jFvN2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def belief_update(belief_past, my_side, M, switch_proba, rewards_prob):\n",
        "    \"\"\"\n",
        "    using PAST belief on the LEFT box, CURRENT location and\n",
        "        and measurement to update belief\n",
        "\n",
        "    Args:\n",
        "      belief_past (array): p(s_{t-1}|m_{1:t-1}), [right left].\n",
        "      my_side : 0 if left, 1 if right.\n",
        "      M : 1 if a fish was catched.\n",
        "      switch_proba (array): the probabilities to switch from states. [rightToLeft, LeftToRight]\n",
        "      rewards_prob (array): [high, low]\n",
        "    Returns:\n",
        "      belief (array): p(s_{t}|m_{1:t}), [right left].\n",
        "\n",
        "    \"\"\"\n",
        "    #Transition matrix\n",
        "    T = ...\n",
        "\n",
        "    # Update prior\n",
        "    prediction = ...\n",
        "\n",
        "    # Get the likelihood\n",
        "\n",
        "\n",
        "    # Compute posterior\n",
        "    belief = ...\n",
        "\n",
        "    #Normalize\n",
        "\n",
        "    return belief"
      ],
      "metadata": {
        "id": "lIIiaMY7wR5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "pMYuw0-h9Dsw"
      },
      "source": [
        "---\n",
        "# Section 4: Implementing a threshold policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "9JFPSS-O9Dsw"
      },
      "source": [
        "Now we'll switch the policy from the 'lazy' policy (always staying on the same side) used above to a threshold policy. You'll change your location whenever your belief is low enough that you're on the correct side. You'll update the function `policy_threshold(threshold, belief, loc)`. This policy takes three inputs and will return the optimal action:\n",
        "\n",
        "1. The `belief` about the fish state. For convenience, we will represent the belief at time¬†*t* using a 2-dimensional vector. The first element is the belief that the fish are on the right, and the second element is the belief the fish are on the left. At every time step, these elements sum to 1.\n",
        "\n",
        "2. Your location `my_side`, represented as \"Left\" = 0 and \"Right\" = 1.\n",
        "\n",
        "3. A belief `threshold` that determines when to switch. When your belief that you are on the same side as the fish drops below this threshold, you should move to the other location, and otherwise stay.\n",
        "\n",
        "Your function should return an action for each time *t*, which takes the value of \"stay\" or \"switch\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "execution": {},
        "tags": [],
        "id": "0-dFk6N29Dsw"
      },
      "outputs": [],
      "source": [
        "def policy_threshold(threshold, belief, my_side):\n",
        "  \"\"\"\n",
        "  chooses whether to switch side based on whether the belief\n",
        "      on the current site drops below the threshold\n",
        "\n",
        "  Args:\n",
        "    threshold (float): the threshold of belief on the current site,\n",
        "                        when the belief is lower than the threshold, switch side\n",
        "    belief (numpy array of float, 2-dimensional): the belief on the\n",
        "                                                  two sites at a certain time\n",
        "    my_side (int) : the location of the agent at a certain time\n",
        "                0 for left side, 1 for right side\n",
        "\n",
        "  Returns:\n",
        "    act (string): \"stay\" or \"switch\"\n",
        "  \"\"\"\n",
        "\n",
        "  ## Hint: use my_side value to determine which row of belief you need to use\n",
        "\n",
        "  # Write the if statement\n",
        "\n",
        "  return act"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "i7W05tvK9Ds1"
      },
      "source": [
        "---\n",
        "# Section 5: Implementing a value function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "mztgbrvp9Ds2"
      },
      "source": [
        "Let's find out how good our threshold is. For that, we will calculate a **value function** that quantifies our utility (total points). We will use this value to compare different thresholds; remember, our goal is to maximize the amount of fish we catch while minimizing the effort involved in changing locations.\n",
        "\n",
        "The value is the total expected utility per unit time.\n",
        "\n",
        "\\begin{equation}\n",
        "V(\\theta) = \\frac{1}{T}\\left( \\sum_t R(s_t) - C(a_t) \\right)\n",
        "\\end{equation}\n",
        "\n",
        "where $R(s_t)$ is the instantaneous reward we get at side $s_t$ and $C(a_t)$ is the cost we paid for the chosen action. Remember, we receive one point for fish caught and pay `cost_sw` points for switching to the other side.\n",
        "\n",
        "We could take this average mathematically over the probabilities of rewards and actions. However, we can get the same answer by simply averaging the _actual_ rewards and costs **over a long time**. This is what you are going to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "LK6n-4Ge9Ds2"
      },
      "outputs": [],
      "source": [
        "def get_value(rewards, actions, cost_sw):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    rewards (numpy array of length T): whether a reward is obtained (1) or not (0) at each time step\n",
        "    actions (numpy array of length T): action, \"stay\" or \"switch\", taken at each time step.\n",
        "    cost_sw (float): the cost of switching to the other location\n",
        "\n",
        "  Returns:\n",
        "    value (float): expected utility per unit time\n",
        "  \"\"\"\n",
        "  cost =0\n",
        "\n",
        "\n",
        "  # Calculate the value function\n",
        "  value = ...\n",
        "\n",
        "  return value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 6 : Putting everything together\n",
        "\n",
        "Using the functions you defined earlier, complete the function `run_policy` that returns a total utility for a given threshold.\n"
      ],
      "metadata": {
        "id": "SuJXRM6YTL8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_policy(threshold, switch_proba, start_proba, rewards_prob, my_initial_side, n_steps):\n",
        "  \"\"\"\n",
        "  This function executes the policy (fully parameterized by the threshold) and\n",
        "  returns two arrays:\n",
        "    The sequence of actions taken from time 0 to T\n",
        "    The sequence of rewards obtained from time 0 to T\n",
        "  \"\"\"\n",
        "  # Simulate the latent state\n",
        "\n",
        "\n",
        "  return S, M, actions[1:], belief, my_side\n",
        "\n",
        "threshold =0.6\n",
        "my_initial_side = 0\n",
        "n_steps = 1000\n",
        "cost_switch = 2\n",
        "\n",
        "np.random.seed(56)\n",
        "S, rewards, actions, belief, my_side = run_policy(threshold, switch_proba, start_proba, rewards_prob, my_initial_side, n_steps)\n",
        "# Plot\n",
        "plot_Policy(S,belief, my_side, M, threshold,actions)\n",
        "\n",
        "value = get_value(rewards, actions, cost_switch)\n"
      ],
      "metadata": {
        "id": "-7IlPetYUDAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "u154we6I9Ds3"
      },
      "source": [
        "Now that you have a mechanism to find out how good a threshold is, we will use a brute force approach to **compute the optimal threshold**: we'll just try all thresholds, simulate the value of each, and pick the best one. Complete the function `get_optimal_threshold(p_stay, low_rew_p, high_rew_p, cost_sw)`.\n",
        "\n",
        "**Thinking questions:**\n",
        "\n",
        "* Try to vary some parameters and observe how will evolve the optimal threshold : switching cost, probability that the fishes switch sides, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {},
        "id": "2KSTkkHd9Ds3"
      },
      "outputs": [],
      "source": [
        "def get_optimal_threshold(switch_proba, start_proba, rewards_prob, my_initial_side, n_steps, cost_sw):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    p_stay (float): probability of fish staying in their current location\n",
        "    low_rew_p (float): probability of catching fish when you and the fist are in different locations.\n",
        "    high_rew_p (float): probability of catching fish when you and the fist are in the same location.\n",
        "    cost_sw (float): the cost of switching to the other location\n",
        "\n",
        "  Returns:\n",
        "    value (float): expected utility per unit time\n",
        "  \"\"\"\n",
        "\n",
        "  # Create an array of 20 equally distanced candidate thresholds (min = 0., max=1.):\n",
        "  threshold_array = ...\n",
        "\n",
        "  # Using the function get_value() and run_policy that you coded before and compute the value of your\n",
        "  # candidate thresholds:\n",
        "\n",
        "  # Create an array to store the value of each of your candidates:\n",
        "  value_array = ...\n",
        "\n",
        "\n",
        "  # Find the threshold that maximize the reward\n",
        "  optimal_thresh = ...\n",
        "\n",
        "  return threshold_array, value_array, optimal_thresh\n",
        "\n",
        "\n",
        "# Feel free to change these parameters\n",
        "low_rew_prob = 0.1     # Even if fish are somewhere else, you can catch some fish with probability low_rew_prob\n",
        "high_rew_prob = 0.7    # When you and the fish are in the same place, you can catch fish with probability high_rew_prob\n",
        "rewards_prob = np.array([high_rew_prob, low_rew_prob])\n",
        "cost_sw = 0.1         # When you switch locations, you pay this cost: cost_sw\n",
        "\n",
        "\n",
        "# Visually determine the threshold that obtains the maximum utility.\n",
        "# Remember, policies are parameterized by a threshold on beliefs:\n",
        "# when your belief that the fish are on your current side falls below a threshold ùúÉ, you switch to the other side.\n",
        "threshold_array, value_array, optimal_threshold = get_optimal_threshold(switch_proba, start_proba, rewards_prob, my_initial_side, n_steps, cost_sw)\n",
        "print(optimal_threshold)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernel": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": [
          "# Tutorial 1- Optimal Control for Discrete State\n",
          "\n",
          "Please execute the cell below to initialize the notebook environment.\n",
          "\n",
          "import numpy as np                 # import numpy\n",
          "import scipy               # import scipy\n",
          "import random                      # import basic random number generator functions\n",
          "from scipy.linalg import inv\n",
          "\n",
          "import matplotlib.pyplot as plt    # import matplotlib\n",
          "\n",
          "---\n",
          "\n",
          "## Tutorial objectives\n",
          "\n",
          "In this tutorial, we will implement a binary HMM task.\n",
          "\n",
          "---\n",
          "\n",
          "## Task Description\n",
          "\n",
          "There are two boxes. The box can be in a high-rewarding state ($s=1$), which means that a reward will be delivered with high probabilty $q_{high}$; or the box can be in low-rewarding state ($s=0$), then the reward will be delivered with low probabilty $q_{low}$.\n",
          "\n",
          "The states of the two boxes are latent. At a certain time, only one of the sites can be in high-rewarding state, and the other box will be the opposite. The states of the two boxes switches with a certain probability $p_{sw}$. \n",
          "\n",
          "![alt text](switching.png \"Title\")\n",
          "\n",
          "\n",
          "The agent may stay at one site for sometime. As the agent accumulates evidence about the state of the box on that site, it may choose to stay or switch to the other side with a switching cost $c$. The agent keeps beliefs on the states of the boxes, which is the posterior probability of the state being high-rewarding given all the past observations. Consider the belief on the state of the left box, we have \n",
          "\n",
          "$$b(s_t) = p(s_t = 1 | o_{0:t}, l_{0:t}, a_{0:t-1})$$\n",
          "\n",
          "where $o$ is the observation that whether a reward is obtained, $l$ is the location of the agent, $a$ is the action of staying ($a=0$) or switching($a=1$). \n",
          "\n",
          "Since the two boxes are completely anti-correlated, i.e. only one of the boxes is high-rewarded at a certain time, the the other one is low-rewarded, the belief on the two boxes should sum up to be 1. As a result, we only need to track the belief on one of the boxes. \n",
          "\n",
          "The policy of the agent depends on a threshold on beliefs. When the belief on the box on the other side gets higher than the threshold $\\theta$, the agent will switch to the other side. In other words, the agent will choose to switch when it is confident enough that the other side is high rewarding. \n",
          "\n",
          "The value function can be defined as the reward rate during a single trial.\n",
          "\n",
          "$$v(\\theta) = \\sum_t r_t - c\\cdot 1_{a_t = 1}$$ \n",
          "\n",
          "we would like to see the relation between the threshold and the value function. \n",
          "\n",
          "### Exercise 1: Control for binary HMM\n",
          "In this excercise, we generate the dynamics for the binary HMM task as described above. \n",
          "\n",
          "# This function is the policy based on threshold\n",
          "\n",
          "def policy(threshold, bel, loc):\n",
          "    if loc == 0:\n",
          "        if bel[1]  >= threshold:\n",
          "            act = 1\n",
          "        else:\n",
          "            act = 0\n",
          "    else:  # loc = 1\n",
          "        if bel[0] >= threshold:\n",
          "            act = 1\n",
          "        else:\n",
          "            act = 0\n",
          "\n",
          "    return act\n",
          "\n",
          "# This function generates the dynamics\n",
          "\n",
          "def generateProcess(params):\n",
          "\n",
          "    T, p_sw, q_high, q_low, cost_sw, threshold = params\n",
          "    world_state = np.zeros((2, T), int)  # value :1: good box; 0: bad box\n",
          "    loc = np.zeros(T, int)  # 0: left box               1: right box\n",
          "    obs = np.zeros(T, int)  # 0: did not get food        1: get food\n",
          "    act = np.zeros(T, int)  # 0 : stay                   1: switch and get food from the other side\n",
          "    bel = np.zeros((2, T), float)  # the probability that the left box has food,\n",
          "    # then the probability that the second box has food is 1-b\n",
          "\n",
          "\n",
          "    p = np.array([1 - p_sw, p_sw])  # transition probability to good state\n",
          "    q = np.array([q_low, q_high])\n",
          "    q_mat = np.array([[1 - q_high, q_high], [1 - q_low, q_low]])\n",
          "\n",
          "    for t in range(T):\n",
          "        if t == 0:\n",
          "            world_state[0, t] = 1    # good box\n",
          "            world_state[1, t] = 1 - world_state[0, t]\n",
          "            loc[t] = 0\n",
          "            obs[t] = 0\n",
          "            bel_0 = np.random.random(1)[0]\n",
          "            bel[:, t] = np.array([bel_0, 1-bel_0])\n",
          "\n",
          "            act[t] = policy(threshold, bel[:, t], loc[t])\n",
          "\n",
          "        else:\n",
          "            world_state[0, t] = np.random.binomial(1, p[world_state[0, t - 1]])\n",
          "            world_state[1, t] = 1 - world_state[0, t]\n",
          "\n",
          "            if act[t - 1] == 0:\n",
          "                loc[t] = loc[t - 1]\n",
          "            else:  # after weitching, open the new box, deplete if any; then wait a usualy time\n",
          "                loc[t] = 1 - loc[t - 1]\n",
          "\n",
          "            # new observation\n",
          "            obs[t] = np.random.binomial(1, q[world_state[loc[t], t-1]])\n",
          "\n",
          "            # update belief posterior, p(s[t] | obs(0-t), act(0-t-1))\n",
          "            bel_0 = (bel[0, t-1] * p_sw  + bel[1, t-1] * (1 - p_sw)) * q_mat[loc[t], obs[t]]\n",
          "            bel_1 = (bel[1, t - 1] * p_sw + bel[0, t - 1] * (1 - p_sw)) * q_mat[1-loc[t], obs[t]]\n",
          "\n",
          "            bel[0, t] = bel_0 / (bel_0 + bel_1)\n",
          "            bel[1, t] = bel_1 / (bel_0 + bel_1)\n",
          "\n",
          "            act[t] = policy(threshold, bel[:, t], loc[t])\n",
          "\n",
          "    return bel, obs, act, world_state, loc\n",
          "\n",
          "# value function \n",
          "def value_function(obs, act, cost_sw, discount):\n",
          "    T = len(obs)\n",
          "    discount_time = np.array([discount ** t for t in range(T)])\n",
          "\n",
          "    #value = (np.sum(obs) - np.sum(act) * cost_sw) / T\n",
          "    value = (np.sum(np.multiply(obs, discount_time)) - np.sum(np.multiply(act, discount_time)) * cost_sw) / T\n",
          "\n",
          "    return value\n",
          "\n",
          "def switch_int(obs, act):\n",
          "    sw_t = np.where(act == 1)[0]\n",
          "    sw_int = sw_t[1:] - sw_t[:-1]\n",
          "\n",
          "    return sw_int\n",
          "\n",
          "#Plotting \n",
          "def plot_dynamics(bel, obs, act, world_state, loc):\n",
          "    T = len(obs)\n",
          "\n",
          "    showlen = min(T, 100)\n",
          "    startT = 0\n",
          "\n",
          "    endT = startT + showlen\n",
          "    showT = range(startT, endT)\n",
          "    time_range = np.linspace(0, showlen - 1)\n",
          "\n",
          "    fig_posterior, [ax0, ax1, ax_loc, ax2, ax3] = plt.subplots(5, 1, figsize=(15, 10))\n",
          "\n",
          "    ax0.plot(world_state[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax0.set_ylabel('Left box', rotation=360, fontsize=22)\n",
          "    ax0.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax0.set_xticks(np.arange(0, showlen, 10))\n",
          "    ax0.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax0.set_xlim([0, showlen])\n",
          "\n",
          "\n",
          "    ax3.plot(world_state[1, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax3.set_ylabel('Right box', rotation=360, fontsize=22)\n",
          "    ax3.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax3.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax3.set_xlim([0, showlen])\n",
          "    ax3.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "    ax1.plot(bel[0, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax1.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
          "    ax1.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax1.set_ylabel('Belief on \\n left box', rotation=360, fontsize=22)\n",
          "    ax1.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax1.set_xlim([0, showlen])\n",
          "    ax1.set_ylim([0, 1])\n",
          "    ax1.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "\n",
          "    ax_loc.plot(1 - loc[showT], 'g.-', markersize=12, linewidth=5, label = 'location')\n",
          "    ax_loc.plot((act[showT] - .1) * .8, 'v', markersize=10, label = 'action')\n",
          "    ax_loc.plot(obs[showT] * .5, '*', markersize=5, label = 'reward')\n",
          "    ax_loc.legend(loc=\"upper right\")\n",
          "    ax_loc.set_xlim([0, showlen])\n",
          "    ax_loc.set_ylim([0, 1])\n",
          "    #ax_loc.set_yticks([])\n",
          "    ax_loc.set_xticks([0, showlen])\n",
          "    ax_loc.tick_params(axis='both', which='major', labelsize=18)\n",
          "    labels = [item.get_text() for item in ax_loc.get_yticklabels()]\n",
          "    labels[0] = 'Right'\n",
          "    labels[-1] = 'Left'\n",
          "    ax_loc.set_yticklabels(labels)\n",
          "\n",
          "    ax2.plot(bel[1, showT], color='dodgerblue', markersize=10, linewidth=3.0)\n",
          "    ax2.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
          "    ax2.set_xlabel('time', fontsize=18)\n",
          "    ax2.yaxis.set_label_coords(-0.1, 0.25)\n",
          "    ax2.set_ylabel('Belief on  \\n  right box', rotation=360, fontsize=22)\n",
          "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
          "    ax2.set_xlim([0, showlen])\n",
          "    ax2.set_ylim([0, 1])\n",
          "    ax2.set_xticks(np.arange(0, showlen, 10))\n",
          "\n",
          "    plt.show()\n",
          "\n",
          "def plot_val_thre(threshold_array, value_array):\n",
          "    fig_, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
          "    ax.plot(threshold_array, value_array)\n",
          "    ax.set_ylim([np.min(value_array), np.max(value_array)])\n",
          "    ax.set_title('threshold vs value')\n",
          "    ax.set_xlabel('threshold')\n",
          "    ax.set_ylabel('value')\n",
          "    plt.show()\n",
          "\n",
          "T = 5000\n",
          "p_sw = .95          # state transiton probability\n",
          "q_high = .7\n",
          "q_low = 0 #.2\n",
          "cost_sw = 1 #int(1/(1-p_sw)) - 5\n",
          "threshold = .8    # threshold of belief for switching\n",
          "discount = 1\n",
          "\n",
          "step = 0.1\n",
          "threshold_array = np.arange(0, 1 + step, step)\n",
          "value_array = np.zeros(threshold_array.shape)\n",
          "\n",
          "for i in range(len(threshold_array)):\n",
          "    threshold = threshold_array[i]\n",
          "    params = [T, p_sw, q_high, q_low, cost_sw, threshold]\n",
          "    bel, obs, act, world_state, loc = generateProcess(params)\n",
          "    value_array[i] = value_function(obs, act, cost_sw, discount)\n",
          "    sw_int = switch_int(obs, act)\n",
          "    #print(np.mean(sw_int))\n",
          "\n",
          "    if threshold == 0.8:\n",
          "        plot_dynamics(bel, obs, act, world_state, loc)\n",
          "\n",
          "plot_val_thre(threshold_array, value_array)\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n",
          "\n"
        ]
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}