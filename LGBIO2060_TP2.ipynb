{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBIO2060-TP2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP8FwT8mKEKkTboSaPMdbxz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2021/blob/Tp2/LGBIO2060_TP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53044446"
      },
      "source": [
        "# LGBIO2060 Exercice session 2\n",
        "__Authors:__ Simon Vandergooten and Clemence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n",
        "\n",
        "TO CHANGE\n",
        "Bayesian decision, binary hidden sate:\n",
        "\n",
        "In this first exercise session we will refresh your memory about the basis of statistics needed for the rest of the sessions.\n",
        "After this session you should be able to:\n",
        "* Sample data from a specific probability distribution.\n",
        "* Use basic probability rules to infer parameters.\n",
        "* Understand and use the maximum likelihood principle.\n",
        "* Understand the Bayes' rule and the impact of a prior in the inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ7G2IGx64V_"
      },
      "source": [
        "(Reminder: product rule: if two events are independant, their joint probability is the product of their of each's probability 4)\n",
        "\n",
        "INTRO \n",
        "\n",
        "Bayesian inference is.... Combine prior knowledge adnd a noisy measurement to infer the latent state. => Formule (cfr slide 37 intro). This will help take a decsision on what to do (mini intro sur l'utilité)\n",
        "\n",
        "\n",
        "Donner l'exemple du gone fishng. \n",
        "\n",
        "This notebook will introduce the fundamental building blocks for Bayesian statistics: \n",
        "\n",
        "1. How do we combine the possible loss (or gain) for making a decision with our probabilitic knowledge?\n",
        "2. How do we use probability distributions to represent hidden states?\n",
        "3. How does marginalization work and how can we use it?\n",
        "4. How do we combine new information with our prior knowledge? \n",
        "\n",
        "In this session, you will handle binary state, it means that X can be either A or B. Next week, we will extend this theory to continuous states, i.e. X can take any value between -inf => inf. \n",
        "\n",
        "Note: You will have to apply some stat principle, don't hesitate to have a look on the previous notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TFvUeaon26yS"
      },
      "source": [
        "# Setup  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "EPcqRKsw26yT"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, transforms, gridspec\n",
        "from scipy.optimize import fsolve\n",
        "\n",
        "from collections import namedtuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "sGyEzzm_26yU"
      },
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "from ipywidgets import GridspecLayout, HBox, VBox, FloatSlider, Layout, ToggleButtons\n",
        "from ipywidgets import interactive, interactive_output, Checkbox, Select\n",
        "from IPython.display import clear_output\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "PrkQwXxc26yU"
      },
      "source": [
        "# @title Plotting Functions\n",
        "\n",
        "def plot_joint_probs(P, ):\n",
        "    assert np.all(P >= 0), \"probabilities should be >= 0\"\n",
        "    # normalize if not\n",
        "    P = P / np.sum(P)\n",
        "    marginal_y = np.sum(P,axis=1)\n",
        "    marginal_x = np.sum(P,axis=0)\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.1, 0.65\n",
        "    bottom, height = 0.1, 0.65\n",
        "    spacing = 0.005\n",
        "\n",
        "    # start with a square Figure\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "\n",
        "    joint_prob = [left, bottom, width, height]\n",
        "    rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
        "    rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
        "\n",
        "    rect_x_cmap = plt.cm.Blues\n",
        "    rect_y_cmap = plt.cm.Reds\n",
        "\n",
        "    # Show joint probs and marginals\n",
        "    ax = fig.add_axes(joint_prob)\n",
        "    ax_x = fig.add_axes(rect_histx, sharex=ax)\n",
        "    ax_y = fig.add_axes(rect_histy, sharey=ax)\n",
        "\n",
        "    # Show joint probs and marginals\n",
        "    ax.matshow(P,vmin=0., vmax=1., cmap='Greys')\n",
        "    ax_x.bar(0, marginal_x[0], facecolor=rect_x_cmap(marginal_x[0]))\n",
        "    ax_x.bar(1, marginal_x[1], facecolor=rect_x_cmap(marginal_x[1]))\n",
        "    ax_y.barh(0, marginal_y[0], facecolor=rect_y_cmap(marginal_y[0]))\n",
        "    ax_y.barh(1, marginal_y[1], facecolor=rect_y_cmap(marginal_y[1]))\n",
        "    # set limits\n",
        "    ax_x.set_ylim([0,1])\n",
        "    ax_y.set_xlim([0,1])\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{P[i,j]:.2f}\"\n",
        "        ax.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = marginal_x[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_x.text(i, v +0.1, c, va='center', ha='center', color='black')\n",
        "        v = marginal_y[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_y.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "\n",
        "    # set up labels\n",
        "    ax.xaxis.tick_bottom()\n",
        "    ax.yaxis.tick_left()\n",
        "    ax.set_xticks([0,1])\n",
        "    ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels(['Silver','Gold'])\n",
        "    ax.set_yticklabels(['Small', 'Large'])\n",
        "    ax.set_xlabel('color')\n",
        "    ax.set_ylabel('size')\n",
        "    ax_x.axis('off')\n",
        "    ax_y.axis('off')\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_prior_likelihood_posterior(prior, likelihood, posterior):\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.12\n",
        "    small_width = 0.1\n",
        "    left_space = left + small_width + padding\n",
        "    added_space = padding + width\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom , width, height]\n",
        "    rect_posterior = [left_space +  added_space, bottom , width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood, sharey=ax_prior)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey = ax_prior)\n",
        "\n",
        "    rect_colormap = plt.cm.Blues\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = rect_colormap(prior[0, 0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = rect_colormap(prior[1, 0]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='Greens')\n",
        "\n",
        "\n",
        "    # Probabilities plot details\n",
        "    # ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "    #              ylabel = 'state (s)', title = \"Prior p(s)\")\n",
        "    ax_prior.set(xlim = [1, 0], xticks = [], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\")\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m (left) | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "\n",
        "    ax_posterior.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Posterior p(s | m)')\n",
        "    ax_posterior.xaxis.set_ticks_position('bottom')\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{posterior[i,j]:.2f}\"\n",
        "        ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i, 0]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "\n",
        "\n",
        "def plot_prior_likelihood(ps, p_a_s1, p_a_s0, measurement):\n",
        "    likelihood = np.asarray([[p_a_s1, 1-p_a_s1],[p_a_s0, 1-p_a_s0]])\n",
        "    assert 0.0 <= ps <= 1.0\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "    if measurement == \"Fish\":\n",
        "        posterior = likelihood[:, 0] * prior\n",
        "    else:\n",
        "        posterior = (likelihood[:, 1] * prior).reshape(-1)\n",
        "    posterior /= np.sum(posterior)\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.12\n",
        "    small_width = 0.2\n",
        "    left_space = left + small_width + padding\n",
        "    small_padding = 0.05\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom , width, height]\n",
        "    rect_posterior = [left_space + width + small_padding, bottom , small_width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood, sharey=ax_prior)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey=ax_prior)\n",
        "\n",
        "    prior_colormap = plt.cm.Blues\n",
        "    posterior_colormap = plt.cm.Greens\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = prior_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = prior_colormap(prior[1]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    # ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='')\n",
        "    ax_posterior.barh(0, posterior[0], facecolor = posterior_colormap(posterior[0]))\n",
        "    ax_posterior.barh(1, posterior[1], facecolor = posterior_colormap(posterior[1]))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\", xticks = [])\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "    ax_posterior.set(xlim = [0, 1], xticks = [], yticks = [0, 1],\n",
        "                     yticklabels = ['left', 'right'], title = \"Posterior p(s | m)\")\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    # for i,j in zip(x.flatten(), y.flatten()):\n",
        "    #     c = f\"{posterior[i,j]:.2f}\"\n",
        "    #     ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = posterior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_posterior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    return fig\n",
        "\n",
        "\n",
        "from matplotlib import colors\n",
        "def plot_utility(ps):\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "\n",
        "    utility = np.array([[2, -3], [-2, 1]])\n",
        "\n",
        "    expected = prior @ utility\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.16\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.02\n",
        "    small_width = 0.1\n",
        "    left_space = left + small_width + padding\n",
        "    added_space = padding + width\n",
        "\n",
        "    fig = plt.figure(figsize=(17, 3))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_utility = [left + added_space , bottom , width, height]\n",
        "    rect_expected = [left + 2* added_space, bottom , width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_utility = fig.add_axes(rect_utility, sharey=ax_prior)\n",
        "    ax_expected = fig.add_axes(rect_expected)\n",
        "\n",
        "    rect_colormap = plt.cm.Blues\n",
        "\n",
        "    # Data of plots\n",
        "    ax_prior.barh(0, prior[0], facecolor = rect_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = rect_colormap(prior[1]))\n",
        "    ax_utility.matshow(utility, cmap='cool')\n",
        "    norm = colors.Normalize(vmin=-3, vmax=3)\n",
        "    ax_expected.bar(0, expected[0], facecolor = rect_colormap(norm(expected[0])))\n",
        "    ax_expected.bar(1, expected[1], facecolor = rect_colormap(norm(expected[1])))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], xticks = [], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Probability of state\")\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Utility plot details\n",
        "    ax_utility.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'action (a)',\n",
        "                   title = 'Utility')\n",
        "    ax_utility.xaxis.set_ticks_position('bottom')\n",
        "    ax_utility.spines['left'].set_visible(False)\n",
        "    ax_utility.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Expected utility plot details\n",
        "    ax_expected.set(title = 'Expected utility', ylim = [-3, 3],\n",
        "                    xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                    xlabel = 'action (a)',\n",
        "                    yticks = [])\n",
        "    ax_expected.xaxis.set_ticks_position('bottom')\n",
        "    ax_expected.spines['left'].set_visible(False)\n",
        "    ax_expected.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{utility[i,j]:.2f}\"\n",
        "        ax_utility.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = expected[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_expected.text(i, 2.5, c, va='center', ha='center', color='black')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0, measurement):\n",
        "    assert 0.0 <= ps <= 1.0\n",
        "    assert 0.0 <= p_a_s1 <= 1.0\n",
        "    assert 0.0 <= p_a_s0 <= 1.0\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "    likelihood = np.asarray([[p_a_s1, 1-p_a_s1],[p_a_s0, 1-p_a_s0]])\n",
        "    utility = np.array([[2.0, -3.0], [-2.0, 1.0]])\n",
        "    # expected = np.zeros_like(utility)\n",
        "\n",
        "    if measurement == \"Fish\":\n",
        "        posterior = likelihood[:, 0] * prior\n",
        "    else:\n",
        "        posterior = (likelihood[:, 1] * prior).reshape(-1)\n",
        "    posterior /= np.sum(posterior)\n",
        "    # expected[:, 0] = utility[:, 0] * posterior\n",
        "    # expected[:, 1] = utility[:, 1] * posterior\n",
        "    expected = posterior @ utility\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.3\n",
        "    padding = 0.12\n",
        "    small_width = 0.2\n",
        "    left_space = left + small_width + padding\n",
        "    small_padding = 0.05\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 9))\n",
        "\n",
        "    rect_prior = [left, bottom + height + padding, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom + height + padding , width, height]\n",
        "    rect_posterior = [left_space + width + small_padding, bottom + height + padding , small_width, height]\n",
        "\n",
        "    rect_utility = [padding, bottom, width, height]\n",
        "    rect_expected = [padding + width + padding + left, bottom, width, height]\n",
        "\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood)\n",
        "    ax_prior = fig.add_axes(rect_prior, sharey=ax_likelihood)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey=ax_likelihood)\n",
        "    ax_utility = fig.add_axes(rect_utility)\n",
        "    ax_expected = fig.add_axes(rect_expected)\n",
        "\n",
        "    prior_colormap = plt.cm.Blues\n",
        "    posterior_colormap = plt.cm.Greens\n",
        "    expected_colormap = plt.cm.Wistia\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = prior_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = prior_colormap(prior[1]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    ax_posterior.barh(0, posterior[0], facecolor = posterior_colormap(posterior[0]))\n",
        "    ax_posterior.barh(1, posterior[1], facecolor = posterior_colormap(posterior[1]))\n",
        "    ax_utility.matshow(utility, vmin=0., vmax=1., cmap='cool')\n",
        "    # ax_expected.matshow(expected, vmin=0., vmax=1., cmap='Wistia')\n",
        "    ax_expected.bar(0, expected[0], facecolor = expected_colormap(expected[0]))\n",
        "    ax_expected.bar(1, expected[1], facecolor = expected_colormap(expected[1]))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\", xticks = [])\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "    ax_posterior.set(xlim = [0, 1], xticks = [], yticks = [0, 1],\n",
        "                     yticklabels = ['left', 'right'], title = \"Posterior p(s | m)\")\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Utility plot details\n",
        "    ax_utility.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                   xlabel = 'action (a)', yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   title = 'Utility', ylabel = 'state (s)')\n",
        "    ax_utility.xaxis.set_ticks_position('bottom')\n",
        "    ax_utility.spines['left'].set_visible(False)\n",
        "    ax_utility.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Expected Utility plot details\n",
        "    ax_expected.set(ylim = [-2, 2], xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                 xlabel = 'action (a)', title = 'Expected utility', yticks=[])\n",
        "    # ax_expected.axis('off')\n",
        "    ax_expected.spines['left'].set_visible(False)\n",
        "    # ax_expected.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "    #                 xlabel = 'action (a)',\n",
        "    #                title = 'Expected utility')\n",
        "    # ax_expected.xaxis.set_ticks_position('bottom')\n",
        "    # ax_expected.spines['left'].set_visible(False)\n",
        "    # ax_expected.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i in ind:\n",
        "        v = posterior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_posterior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{utility[i,j]:.2f}\"\n",
        "        ax_utility.text(j,i, c, va='center', ha='center', color='black')\n",
        "    # for i,j in zip(x.flatten(), y.flatten()):\n",
        "    #     c = f\"{expected[i,j]:.2f}\"\n",
        "    #     ax_expected.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = expected[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_expected.text(i, v, c, va='center', ha='center', color='black')\n",
        "\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "TpolPjRh26ya"
      },
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "def compute_marginal(px, py, cor):\n",
        "    \"\"\" Calculate 2x2 joint probabilities given marginals p(x=1), p(y=1) and correlation\n",
        "\n",
        "      Args:\n",
        "        px (scalar): marginal probability of x\n",
        "        py (scalar): marginal probability of y\n",
        "        cor (scalar): correlation value\n",
        "\n",
        "      Returns:\n",
        "        ndarray of size (2, 2): joint probability array of x and y\n",
        "    \"\"\"\n",
        "\n",
        "    p11 = px*py + cor*np.sqrt(px*py*(1-px)*(1-py))\n",
        "    p01 = px - p11\n",
        "    p10 = py - p11\n",
        "    p00 = 1.0 - p11 - p01 - p10\n",
        "\n",
        "    return np.asarray([[p00, p01], [p10, p11]])\n",
        "\n",
        "\n",
        "def compute_cor_range(px,py):\n",
        "    \"\"\" Calculate the allowed range of correlation values given marginals p(x=1)\n",
        "      and p(y=1)\n",
        "\n",
        "    Args:\n",
        "      px (scalar): marginal probability of x\n",
        "      py (scalar): marginal probability of y\n",
        "\n",
        "    Returns:\n",
        "      scalar, scalar: minimum and maximum possible values of correlation\n",
        "    \"\"\"\n",
        "\n",
        "    def p11(corr):\n",
        "        return px*py + corr*np.sqrt(px*py*(1-px)*(1-py))\n",
        "    def p01(corr):\n",
        "        return px - p11(corr)\n",
        "    def p10(corr):\n",
        "        return py - p11(corr)\n",
        "    def p00(corr):\n",
        "        return 1.0 - p11(corr) - p01(corr) - p10(corr)\n",
        "    Cmax = min(fsolve(p01, 0.0), fsolve(p10, 0.0))\n",
        "    Cmin = max(fsolve(p11, 0.0), fsolve(p00, 0.0))\n",
        "    return Cmin, Cmax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68uQc483pPNg"
      },
      "source": [
        "#Part 1: Bayesian inference with one measurement\n",
        "\n",
        "In this first part, you will learn to integrate a measurement to prior knowledge to refine your estimate about the true hidden state. Bla bla bla\n",
        "\n",
        "Formule\n",
        "\n",
        "Let's take each ingredient step by step to get some intuition of how it works behind the math. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0a8j2QE6CyW"
      },
      "source": [
        "\n",
        "##Likelihood\n",
        "\n",
        "When you arrive at the dock, tu observe un pêcheur oecher d'un côté de la berge. Soit il a attrapé un poisson, soit il n'en a pas attrapé. Ceci est la mesure m qui va t'aider à déterminer de quel côté se trouve le banc de poissons.   \n",
        "\n",
        "The likelihood corresponds to the probability to obtain a certain measurement given the state of the system. In this case, suppose the fisher man is at the left, it corresponds for example to the probability of catching a fish, given that the fish are at the left. \n",
        "\n",
        "Ex: Let's say that you have 70% chance of catching a fish if you are on the right side of the dock, but only 20% if your are at the wrong side of the dock. \n",
        "\n",
        "1) Please figure out each of the following (might be easiest to do this separately and then compare notes):\n",
        "\n",
        "probability of catching a fish given that the school of fish is on the left side,  P(m=catch fish|s=left) \n",
        "probability of not catching a fish given that the school of fish is on the left side,  P(m=no fish|s=left) \n",
        "probability of catching a fish given that the school of fish is on the right side,  P(m=catch fish|s=right) \n",
        "probability of not catching a fish given that the school of fish is on the right side,  P(m=no fish|s=right) \n",
        "2) If the fisherperson catches a fish, which side would you guess the school is on? Why?\n",
        "\n",
        "3) If the fisherperson does not catch a fish, which side would you guess the school is on? Why?\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Jxhqf5zs26yr"
      },
      "source": [
        "### Think! 3: Guessing the location of the fish\n",
        "\n",
        "Let's say we go to different dock to fish. Here, there are different probabilities of catching fish given the state of the world. At this dock, if you fish on the side of the dock where the fish are, you have a 70% chance of catching a fish. If you fish on the wrong side, you will catch a fish with only 20% probability. These are the likelihoods of observing someone catching a fish! That is, you are taking a measurement by seeing if someone else catches a fish!\n",
        "\n",
        "You see a fisherperson is fishing on the left side.\n",
        "\n",
        "1) Please figure out each of the following (might be easiest to do this separately and then compare notes):\n",
        "- probability of catching a fish given that the school of fish is on the left side, $P(m = \\textrm{catch fish} | s = \\textrm{left} )$\n",
        "- probability of not catching a fish given that the school of fish is on the left side, $P(m = \\textrm{no fish} | s = \\textrm{left})$\n",
        "- probability of catching a fish given that the school of fish is on the right side, $P(m = \\textrm{catch  fish} | s = \\textrm{right})$\n",
        "- probability of not catching a fish given that the school of fish is on the right side, $P(m = \\textrm{no fish} | s = \\textrm{right})$\n",
        "\n",
        "2) If the fisherperson catches a fish, which side would you guess the school is on? Why?\n",
        "\n",
        "3) If the fisherperson does not catch a fish, which side would you guess the school is on? Why?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "38FA-M9s26yr"
      },
      "source": [
        "In the prior exercise, you tried to guess where the school of fish was based on the measurement you took (watching someone fish). You did this by choosing the state (side where you think the fish are) that maximized the probability of the measurement. In other words, you estimated the state by maximizing the likelihood (the side with the highest probability of measurement given state $P(m|s$)). This is called maximum likelihood estimation (MLE) and you've encountered it before during this course, in the [pre-reqs statistics day](https://compneuro.neuromatch.io/tutorials/W0D5_Statistics/student/W0D5_Tutorial2.html#section-2-2-maximum-likelihood) and on [Model Fitting day](https://compneuro.neuromatch.io/tutorials/W1D3_ModelFitting/student/W1D3_Tutorial2.html)!\n",
        "\n",
        "But, what if you had been going to this dock for years and you knew that the fish were almost always on the left side? This should probably affect how you make your estimate -- you would rely less on the single new measurement and more on your prior knowledge. This is the fundemental idea behind Bayesian inference, as we will see later in this tutorial!\n",
        "\n",
        "\n",
        "Cool d'avoir un bout de code où ils définissent ce likelihood qui pourra rentrer dans la fcontion posterior plus tard "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enYWS8ra_FX6"
      },
      "source": [
        "likelihood = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQVd-xyF6FLu"
      },
      "source": [
        "##Prior\n",
        "\n",
        "But, what if you had been going to this dock for years and you knew that the fish were almost always on the left side? This should probably affect how you make your estimate -- you would rely less on the single new measurement and more on your prior knowledge. This is the fundemental idea behind Bayesian inference, as we will see later in this tutorial!\n",
        "\n",
        "Donner les valeurs + bout de code qui les définit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYMGA1mJ_Aux"
      },
      "source": [
        "prior ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMjGPIMC6HPV"
      },
      "source": [
        "##Normalization (marginal likelihood)\n",
        "\n",
        "Caser la formule de marginalisation (cfr slides)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jVw5Vz8w26yx"
      },
      "source": [
        "### Math Exercise 4.2.2: Computing marginal likelihood\n",
        "\n",
        "When we normalize to find the posterior, we need to determine the marinal likelihood--or evidence--for the measurement we observed. To do this, we need to marginalize as we just did above to find the probabilities of a color or size. Only, in this case, we are marginalizing to remove a conditioning variable! In this case, let's consider the likelihood of fish (if we observed a fisherperson fishing on the **right**).\n",
        "\n",
        "| p(m\\|s)       | m = fish | m = no fish  |\n",
        "| ------------ | ---------- | -------------- |\n",
        "| s = left     | 0.1      | 0.9          |\n",
        "| s = right    | 0.5      | 0.5          |\n",
        "\n",
        "\n",
        "The table above shows us the **likelihoods**, just as we explored earlier.\n",
        "\n",
        "You want to know the total probability of a fish being caught, $P(m = \\textrm{fish})$, by the fisherperson fishing on the right. (You would need this to calcualate the posterior.) To do this, you will need to consider the prior probability, $p(s)$, and marginalize over the hidden states!\n",
        "\n",
        "This is an example of marginalizing, or conditioning away, the variable we are not interested in as well.\n",
        "\n",
        "Please complete the following math problems to further practice thinking through probabilities:\n",
        "\n",
        "1. Calculate the marginal likelihood of the fish being caught, $P(m = \\textrm{fish})$, if the priors are: $p(s = \\textrm{left}) = 0.3$ and $p(s = \\textrm{right}) = 0.7$.\n",
        "2. Calculate the marginal likelihood of the fish being caught,  $P(m = \\textrm{fish})$, if the priors are: $p(s = \\textrm{left}) = 0.6$ and $p(s = \\textrm{right}) = 0.4$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyqbzfLR_Szj"
      },
      "source": [
        "p(m) ="
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZ7yyIy6I6G"
      },
      "source": [
        "##Posterior\n",
        "\n",
        "You are now ready to put everything together ! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "Q6KTKHm626yy"
      },
      "source": [
        "Marginalization is going to be used to combine our prior knowlege, which we call the **prior**, and our new information from a measurement, the **likelihood**. Only in this case, the information we gain about the hidden state we are interested in, where the fish are, is based on the relationship between the probabilities of the measurement and our prior. \n",
        "\n",
        "We can now calculate the full posterior distribution for the hidden state ($s$) using Bayes' Rule. As we've seen, the posterior is proportional the the prior times the likelihood. This means that the posterior probability of the hidden state ($s$) given a measurement ($m$) is proportional to the likelihood of the measurement given the state times the prior probability of that state:\n",
        "\n",
        "$$ P(s | m) \\propto P(m | s) P(s)  $$\n",
        "\n",
        "We say proportional to instead of equal because we need to normalize to produce a full probability distribution:\n",
        "\n",
        "$$ P(s | m) = \\frac{P(m | s) P(s)}{P(m)}  $$\n",
        "\n",
        "Normalizing by this $P(m)$ means that our posterior is a complete probability distribution that sums or integrates to 1 appropriately. We now can use this new, complete probability distribution for any future inference or decisions we like! In fact, as we will see tomorrow, we can use it as a new prior! Finally, we often call this probability distribution our beliefs over the hidden states, to emphasize that it is our subjective knowlege about the hidden state.\n",
        "\n",
        "For many complicated cases, like those we might be using to model behavioral or brain inferences, the normalization term can be intractable or extremely complex to calculate. We can be careful to choose probability distributions were we can analytically calculate the posterior probability or numerical approximation is reliable. Better yet, we sometimes don't need to bother with this normalization! The normalization term, $P(m)$, is the probability of the measurement. This does not depend on state so is essentially a constant we can often ignore. We can compare the unnormalized posterior distribution values for different states because how they relate to each other is unchanged when divided by the same constant. We will see how to do this to compare evidence for different hypotheses tomorrow. (It's also used to compare the likelihood of models fit using maximum likelihood estimation)\n",
        "\n",
        "In this relatively simple example, we can compute the marginal likelihood $P(m)$ easily by using:\n",
        "$$P(m) = \\sum_s P(m | s) P(s)$$\n",
        "We can then normalize so that we deal with the full posterior distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "6KHbU9Ag26yy"
      },
      "source": [
        "## Math Exercise 5: Calculating a posterior probability\n",
        "\n",
        "Our prior is $p(s = \\textrm{left}) = 0.3$ and $p(s = \\textrm{right}) = 0.7$. In the video, we learned that the chance of catching a fish given they fish on the same side as the school was 50%. Otherwise, it was 10%. We observe a person fishing on the left side. Our likelihood is: \n",
        "\n",
        "\n",
        "| Likelihood: p(m \\| s) | m = fish   | m = no fish  |\n",
        "| ----------------- |----------|----------|\n",
        "| s = left          | 0.5          | 0.5         |\n",
        "| s = right         | 0.1        |  0.9       |\n",
        "\n",
        "\n",
        "Calculate the posterior probability (on paper) that:\n",
        "\n",
        "1. The school is on the left if the fisherperson catches a fish: $p(s = \\textrm{left} | m = \\textrm{fish})$ (hint: normalize by computing $p(m = \\textrm{fish})$)\n",
        "2. The school is on the right if the fisherperson does not catch a fish: $p(s = \\textrm{right} | m = \\textrm{no fish})$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "ZB8RVbtE26yz"
      },
      "source": [
        "## Coding Exercise 5: Computing Posteriors\n",
        "\n",
        "Let's implement our above math to be able to compute posteriors for different priors and likelihoods.\n",
        "\n",
        "As before, our prior is $p(s = \\textrm{left}) = 0.3$ and $p(s = \\textrm{right}) = 0.7$. In the video, we learned that the chance of catching a fish given they fish on the same side as the school was 50%. Otherwise, it was 10%. We observe a person fishing on the left side. Our likelihood is: \n",
        "\n",
        "\n",
        "| Likelihood: p(m \\| s) | m = fish   | m = no fish  |\n",
        "| ----------------- |----------|----------|\n",
        "| s = left          | 0.5          | 0.5         |\n",
        "| s = right         | 0.1        |  0.9       |\n",
        "\n",
        "\n",
        "We want our full posterior to take the same 2 by 2 form. Make sure the outputs match your math answers!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "KyaGn0QV26yz"
      },
      "source": [
        "def compute_posterior(likelihood, prior):\n",
        "  \"\"\" Use Bayes' Rule to compute posterior from likelihood and prior\n",
        "\n",
        "  Args:\n",
        "    likelihood (ndarray): i x j array with likelihood probabilities where i is\n",
        "                    number of state options, j is number of measurement options\n",
        "    prior (ndarray): i x 1 array with prior probability of each state\n",
        "\n",
        "  Returns:\n",
        "    ndarray: i x j array with posterior probabilities where i is\n",
        "            number of state options, j is number of measurement options\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TODO for students ##\n",
        "  # Fill out function and remove\n",
        "  raise NotImplementedError(\"Student exercise: implement compute_posterior\")\n",
        "  #################################################\n",
        "\n",
        "  # Compute unnormalized posterior (likelihood times prior)\n",
        "  posterior = ... # first row is s = left, second row is s = right\n",
        "\n",
        "  # Compute p(m)\n",
        "  p_m = np.sum(posterior, axis = 0)\n",
        "\n",
        "  # Normalize posterior (divide elements by p_m)\n",
        "  posterior /= ...\n",
        "\n",
        "  return posterior\n",
        "\n",
        "\n",
        "# Make prior\n",
        "prior = np.array([0.3, 0.7]).reshape((2, 1)) # first row is s = left, second row is s = right\n",
        "\n",
        "# Make likelihood\n",
        "likelihood = np.array([[0.5, 0.5], [0.1, 0.9]]) # first row is s = left, second row is s = right\n",
        "\n",
        "# Compute posterior\n",
        "posterior = compute_posterior(likelihood, prior)\n",
        "\n",
        "# Visualize\n",
        "plot_prior_likelihood_posterior(prior, likelihood, posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CXA6qs9n26yz"
      },
      "source": [
        "[*Click for solution*](https://github.com/NeuromatchAcademy/course-content/tree/master//tutorials/W3D1_BayesianDecisions/solutions/W3D1_Tutorial1_Solution_1a2cc907.py)\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1665.0 height=674.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D1_BayesianDecisions/static/W3D1_Tutorial1_Solution_1a2cc907_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jc1G9VFp26yz"
      },
      "source": [
        "## Interactive Demo 5: What affects the posterior?\n",
        "\n",
        "Now that we can understand the implementation of *Bayes rule*, let's vary the parameters of the prior and likelihood to see how changing the prior and likelihood affect the posterior. \n",
        "\n",
        "In the demo below, you can change the prior by playing with the slider for $p( s = left)$. You can also change the likelihood by changing the probability of catching a fish given that the school is on the left and the probability of catching a fish given that the school is on the right. The fisherperson you are observing is fishing on the left.\n",
        " \n",
        "\n",
        "1.   Keeping the likelihood constant, when does the prior have the strongest influence over the posterior? Meaning, when does the posterior look most like the prior no matter whether a fish was caught or not?\n",
        "2.   What happens if the likelihoods for catching a fish are similar when you fish on the correct or incorrect side?\n",
        "3.  Set the prior probability of the state = left to 0.6 and play with the likelihood. When does the likelihood exert the most influence over the posterior?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "5J2x9xwh26y0"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "# style = {'description_width': 'initial'}\n",
        "ps_widget = widgets.FloatSlider(0.3, description='p(s = left)',\n",
        "                                min=0.01, max=0.99, step=0.01)\n",
        "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish on left | state = left)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "p_a_s0_widget = widgets.FloatSlider(0.1, description='p(fish on left | state = right)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "# observed_widget = widgets.Checkbox(value=False, description='Observed fish (m)',\n",
        "#                                  disabled=False, indent=False,\n",
        "#                                  layout=Layout(display=\"flex\", justify_content=\"center\"))\n",
        "\n",
        "observed_widget = ToggleButtons(options=['Fish', 'No Fish'],\n",
        "    description='Observation (m) on the left:', disabled=False, button_style='',\n",
        "    layout=Layout(width='auto', display=\"flex\"),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "widget_ui = VBox([ps_widget,\n",
        "                  HBox([p_a_s1_widget, p_a_s0_widget]),\n",
        "                  observed_widget])\n",
        "widget_out = interactive_output(plot_prior_likelihood,\n",
        "                                {'ps': ps_widget,\n",
        "                                'p_a_s1': p_a_s1_widget,\n",
        "                                'p_a_s0': p_a_s0_widget,\n",
        "                                'measurement': observed_widget})\n",
        "display(widget_ui, widget_out)\n",
        "\n",
        "# @widgets.interact(\n",
        "#     ps=ps_widget,\n",
        "#     p_a_s1=p_a_s1_widget,\n",
        "#     p_a_s0=p_a_s0_widget,\n",
        "#     m_right=observed_widget\n",
        "# )\n",
        "# def make_prior_likelihood_plot(ps,p_a_s1,p_a_s0,m_right):\n",
        "#     fig = plot_prior_likelihood(ps,p_a_s1,p_a_s0,m_right)\n",
        "#     plt.show(fig)\n",
        "#     plt.close(fig)\n",
        "#     return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9ezsX96KU1"
      },
      "source": [
        "##Utility and decision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "LIgENTQo26yj"
      },
      "source": [
        "This video covers utility and expected utility.\n",
        "\n",
        "<details>\n",
        "<summary> <font color=‘blue’>Click here for text recap of video </font></summary>\n",
        "\n",
        "You need to decide where to fish. It may seem obvious - you could just fish on the side where the probability of the fish being is higher! Unfortunately, decisions and actions are always a little more complicated. Deciding to fish may be influenced by more than just the probability of the school of fish being there as we saw by the potential issues of submarines and sunburn. The consequences of the action you take is based on the true (but hidden) state of the world and the action you choose! In our example, fishing on the wrong side, where there aren't many fish, is likely to lead to you spending your afternoon not catching fish and therefore getting a sunburn. The submarine represents a risk to fishing on the right side that is greater than the left side. If you want to know what to expect from taking the action of fishing on one side or the other, you need to calculate the expected utility.\n",
        "\n",
        "You know the (prior) probability that the school of fish is on the left side of the dock today, $P(s = \\textrm{left})$. So, you also know the probability the school is on the right, $P(s = \\textrm{right})$, because these two probabilities must add up to 1.\n",
        "\n",
        "We quantify gains and losses numerically using a **utility** function $U(s,a)$, which describes the consequences of your actions: how much value you gain (or if negative, lose) given the state of the world ($s$) and the action you take ($a$). In our example, our utility can be summarized as:\n",
        "\n",
        "| Utility: U(s,a)   | a = left   | a = right  |\n",
        "| ----------------- |------------|------------|\n",
        "| s = Left          | +2         | -3         |\n",
        "| s = right         | -2         | +1         |\n",
        "\n",
        "To use possible gains and losses to choose an action, we calculate the **expected utility** of that action by weighing these utilities with the probability of that state occuring. This allows us to choose actions by taking probabilities of events into account: we don't care if the outcome of an action-state pair is a loss if the probability of that state is very low. We can formalize this as:\n",
        "\n",
        "$$ \\text{Expected utility of action a} = \\sum_{s}U(s,a)P(s) $$\n",
        "\n",
        "In other words, the expected utility of an action a is the sum over possible states of the utility of that action and state times the probability of that state.\n",
        "\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "8_WOYTv826yk"
      },
      "source": [
        "## Interactive Demo 2: Exploring the decision\n",
        "\n",
        "Let's start to get a sense of how all this works using the interactive demo below. You can change the probability that the school of fish is on the left side,$p(s = \\textrm{left})$, using the slider. You will see the utility function (a matrix) in the middle and the corresponding expected utility for each action on the right.\n",
        "\n",
        "First, make sure you understand how the expected utility of each action is being computed from the probabilities and the utility values. In the initial state: the probability of the fish being on the left is 0.9 and on the right is 0.1. The expected utility of the action of fishing on the left is then $U(s = \\textrm{left},a = \\textrm{left})p(s = \\textrm{left}) + U(s = \\textrm{right},a = \\textrm{left})p(s = \\textrm{right}) = 2(0.9) + -2(0.1) = 1.6$. Essentially, to get the expected utility of action $a$, you are doing a weighted sum over the relevant column of the utility matrix (corresponding to action $a$) where the weights are the state probabilities.\n",
        "\n",
        "For each of these scenarios, think and discuss first. Then use the demo to try out each and see if your action would have been correct (that is, if the expected value of that action is the highest).\n",
        "\n",
        "\n",
        "1.  You just arrived at the dock for the first time and have no sense of where the fish might be. So you guess that the probability of the school being on the left side is 0.5 (so the probability on the right side is also 0.5). Which side would you choose to fish on given our utility values?\n",
        "2.  You think that the probability of the school being on the left side is very low (0.1) and correspondingly high on the right side (0.9). Which side would you choose to fish on given our utility values?\n",
        "3.  What would you choose if the probability of the school being on the left side is slightly lower than on the right side (0. 4 vs 0.6)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "yr9jf1yQ26yk"
      },
      "source": [
        "# @markdown Execute this cell to use the widget\n",
        "ps_widget = widgets.FloatSlider(0.9, description='p(s = left)', min=0.0, max=1.0, step=0.01)\n",
        "\n",
        "@widgets.interact(\n",
        "    ps = ps_widget,\n",
        ")\n",
        "def make_utility_plot(ps):\n",
        "    fig = plot_utility(ps)\n",
        "    plt.show(fig)\n",
        "    plt.close(fig)\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jTG5t-or26y1"
      },
      "source": [
        "\n",
        "We will explore how to consider the expected utility of an action based on our belief (the posterior distribution) about where we think the fish are. Now we have all the components of a Bayesian decision: our prior information, the likelihood given a measurement, the posterior distribution (belief) and our utility (the gains and losses). This allows us to consider the relationship between the true value of the hidden state, $s$, and what we *expect* to get if we take action, $a$, based on our belief!\n",
        "\n",
        "Let's use the following widget to think about the relationship between these probability distributions and utility function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BZxBj-Uh26y1"
      },
      "source": [
        "## Interactive Demo! 6: What is more important, the probabilities or the utilities?\n",
        "\n",
        "We are now going to put everything we've learned together to gain some intuitions for how each of the elements that goes into a Bayesian decision comes together. Remember, the common assumption in neuroscience, psychology, economics, ecology, etc. is that we (humans and animals) are tying to maximize our expected utility. There is a lot going on in this demo as it brings everything in this tutorial together in one place - please spend time making sure you understand the controls and the plots, especially how everything relates together.\n",
        "\n",
        "1. Can you find a situation where the expected utility is the same for both actions?\n",
        "2. What is more important for determining the expected utility: the prior or a new measurement (the likelihood)?\n",
        "3. Why is this a normative model?\n",
        "4. Can you think of ways in which this model would need to be extended to describe human or animal behavior?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "CWPZBH4u26y1"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "# style = {'description_width': 'initial'}\n",
        "\n",
        "ps_widget = widgets.FloatSlider(0.3, description='p(s = left)',\n",
        "                                min=0.01, max=0.99, step=0.01, layout=Layout(width='300px'))\n",
        "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish on left | state = left)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "p_a_s0_widget = widgets.FloatSlider(0.1, description='p(fish on left | state = right)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "\n",
        "observed_widget = ToggleButtons(options=['Fish', 'No Fish'],\n",
        "    description='Observation (m) on the left:', disabled=False, button_style='',\n",
        "    layout=Layout(width='auto', display=\"flex\"),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "widget_ui = VBox([ps_widget,\n",
        "                  HBox([p_a_s1_widget, p_a_s0_widget]),\n",
        "                  observed_widget])\n",
        "\n",
        "widget_out = interactive_output(plot_prior_likelihood_utility,\n",
        "                                {'ps': ps_widget,\n",
        "                                'p_a_s1': p_a_s1_widget,\n",
        "                                'p_a_s0': p_a_s0_widget,\n",
        "                                'measurement': observed_widget})\n",
        "display(widget_ui, widget_out)\n",
        "\n",
        "# @widgets.interact(\n",
        "#     ps=ps_widget,\n",
        "#     p_a_s1=p_a_s1_widget,\n",
        "#     p_a_s0=p_a_s0_widget,\n",
        "#     m_right=observed_widget\n",
        "# )\n",
        "# def make_prior_likelihood_utility_plot(ps, p_a_s1, p_a_s0,m_right):\n",
        "#     fig = plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0,m_right)\n",
        "#     plt.show(fig)\n",
        "#     plt.close(fig)\n",
        "#     return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrJU5nqpj86"
      },
      "source": [
        "#Part 2: Integration of several measurements (Drift diffusion model)"
      ]
    }
  ]
}