{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBIO2060-TP2.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMCFc4zmmrqK3t416x2sdbm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2021/blob/Tp2/LGBIO2060_TP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53044446"
      },
      "source": [
        "# LGBIO2060 Exercice session 2\n",
        "\n",
        "#Bayesian inference of a binary hidden state \n",
        "\n",
        "__Authors:__ Simon Vandergooten and ClÃ©mence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n",
        "\n",
        "\n",
        "In this first exercise session, we will introduce the basic principle of bayesian inference in order to produce an estimate of a binary hidden state and take decisions based on this estimate and the associated uncertainty.  \n",
        "\n",
        "After this session you should be able to:\n",
        "* use probability distribution to represent hidden states \n",
        "* Combine new information with your prior knowledge\n",
        "* Combine the possible loss (or gain) for making a decision with your probabilistic knowledge\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ7G2IGx64V_"
      },
      "source": [
        "#Introduction and context \n",
        "\n",
        "The world is full of latent variables that you cannot observe directly. Then you need to estimate these variables from the prior knowledge you have about it and some noisy measurements. \n",
        "\n",
        "Bayesian inference is a method based on the Bayes theorem used to  combine prior knowledge and noisy measurements to infer an unknown state. The estimate obtained with the Bayes theorem is associated to some uncertainty (how confident we are that our estimate is true), and helps making a decision. \n",
        "\n",
        "We will first handle binary state. It means that X can be either A or B. Next week, we will extend this theorem to continuous states, i.e. X can take any value between $-\\infty \\to \\infty$. \n",
        "\n",
        "Today, your going to the lake, hoping to catch some fishes. At the dock, you have to decide if you are going to fish on the left side or on the right side. To make this decision, you would want to know which side the school of fish is on to maximise your chances to catch a fish. As you cannot see directly where they are (the fishes's state is the latent variable that you're trying to estimate), you will have to base your decision on your previous knowledge (for example, you know that most of the time, fishes are on the left) and on a measurement (a fisherperson is on the right side of the deck and has caught a fish). Additionaly, by fishing on the right side of the deck, you'll have a higher probability of getting a sunburn, wich is not desirable. \n",
        "\n",
        "IMAGE\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "TFvUeaon26yS"
      },
      "source": [
        "# Setup  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "EPcqRKsw26yT"
      },
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches, transforms, gridspec\n",
        "from scipy.optimize import fsolve\n",
        "from collections import namedtuple\n",
        "from scipy import stats\n",
        "from scipy.special import erf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "sGyEzzm_26yU"
      },
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets       # interactive display\n",
        "from ipywidgets import GridspecLayout, HBox, VBox, FloatSlider, Layout, ToggleButtons\n",
        "from ipywidgets import interactive, interactive_output, Checkbox, Select\n",
        "from IPython.display import clear_output\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#%config InlineBackend.figure_format = 'retina'\n",
        "#plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "PrkQwXxc26yU",
        "cellView": "form"
      },
      "source": [
        "# @title Plotting Functions\n",
        "\n",
        "def plot_joint_probs(P, ):\n",
        "    assert np.all(P >= 0), \"probabilities should be >= 0\"\n",
        "    # normalize if not\n",
        "    P = P / np.sum(P)\n",
        "    marginal_y = np.sum(P,axis=1)\n",
        "    marginal_x = np.sum(P,axis=0)\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.1, 0.65\n",
        "    bottom, height = 0.1, 0.65\n",
        "    spacing = 0.005\n",
        "\n",
        "    # start with a square Figure\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "\n",
        "    joint_prob = [left, bottom, width, height]\n",
        "    rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
        "    rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
        "\n",
        "    rect_x_cmap = plt.cm.Blues\n",
        "    rect_y_cmap = plt.cm.Reds\n",
        "\n",
        "    # Show joint probs and marginals\n",
        "    ax = fig.add_axes(joint_prob)\n",
        "    ax_x = fig.add_axes(rect_histx, sharex=ax)\n",
        "    ax_y = fig.add_axes(rect_histy, sharey=ax)\n",
        "\n",
        "    # Show joint probs and marginals\n",
        "    ax.matshow(P,vmin=0., vmax=1., cmap='Greys')\n",
        "    ax_x.bar(0, marginal_x[0], facecolor=rect_x_cmap(marginal_x[0]))\n",
        "    ax_x.bar(1, marginal_x[1], facecolor=rect_x_cmap(marginal_x[1]))\n",
        "    ax_y.barh(0, marginal_y[0], facecolor=rect_y_cmap(marginal_y[0]))\n",
        "    ax_y.barh(1, marginal_y[1], facecolor=rect_y_cmap(marginal_y[1]))\n",
        "    # set limits\n",
        "    ax_x.set_ylim([0,1])\n",
        "    ax_y.set_xlim([0,1])\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{P[i,j]:.2f}\"\n",
        "        ax.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = marginal_x[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_x.text(i, v +0.1, c, va='center', ha='center', color='black')\n",
        "        v = marginal_y[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_y.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "\n",
        "    # set up labels\n",
        "    ax.xaxis.tick_bottom()\n",
        "    ax.yaxis.tick_left()\n",
        "    ax.set_xticks([0,1])\n",
        "    ax.set_yticks([0,1])\n",
        "    ax.set_xticklabels(['Silver','Gold'])\n",
        "    ax.set_yticklabels(['Small', 'Large'])\n",
        "    ax.set_xlabel('color')\n",
        "    ax.set_ylabel('size')\n",
        "    ax_x.axis('off')\n",
        "    ax_y.axis('off')\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_prior_likelihood_posterior(prior, likelihood, posterior):\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.12\n",
        "    small_width = 0.1\n",
        "    left_space = left + small_width + padding\n",
        "    added_space = padding + width\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom , width, height]\n",
        "    rect_posterior = [left_space +  added_space, bottom , width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood, sharey=ax_prior)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey = ax_prior)\n",
        "\n",
        "    rect_colormap = plt.cm.Blues\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = rect_colormap(prior[0, 0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = rect_colormap(prior[1, 0]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='Greens')\n",
        "\n",
        "\n",
        "    # Probabilities plot details\n",
        "    # ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "    #              ylabel = 'state (s)', title = \"Prior p(s)\")\n",
        "    ax_prior.set(xlim = [1, 0], xticks = [], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\")\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m (left) | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "\n",
        "    ax_posterior.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Posterior p(s | m)')\n",
        "    ax_posterior.xaxis.set_ticks_position('bottom')\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{posterior[i,j]:.2f}\"\n",
        "        ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i, 0]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "\n",
        "\n",
        "def plot_prior_likelihood(ps, p_a_s1, p_a_s0, measurement):\n",
        "    likelihood = np.asarray([[p_a_s1, 1-p_a_s1],[p_a_s0, 1-p_a_s0]])\n",
        "    assert 0.0 <= ps <= 1.0\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "    if measurement == \"Fish\":\n",
        "        posterior = likelihood[:, 0] * prior\n",
        "    else:\n",
        "        posterior = (likelihood[:, 1] * prior).reshape(-1)\n",
        "    posterior /= np.sum(posterior)\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.12\n",
        "    small_width = 0.2\n",
        "    left_space = left + small_width + padding\n",
        "    small_padding = 0.05\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom , width, height]\n",
        "    rect_posterior = [left_space + width + small_padding, bottom , small_width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood, sharey=ax_prior)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey=ax_prior)\n",
        "\n",
        "    prior_colormap = plt.cm.Blues\n",
        "    posterior_colormap = plt.cm.Greens\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = prior_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = prior_colormap(prior[1]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    # ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='')\n",
        "    ax_posterior.barh(0, posterior[0], facecolor = posterior_colormap(posterior[0]))\n",
        "    ax_posterior.barh(1, posterior[1], facecolor = posterior_colormap(posterior[1]))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\", xticks = [])\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "    ax_posterior.set(xlim = [0, 1], xticks = [], yticks = [0, 1],\n",
        "                     yticklabels = ['left', 'right'], title = \"Posterior p(s | m)\")\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    # for i,j in zip(x.flatten(), y.flatten()):\n",
        "    #     c = f\"{posterior[i,j]:.2f}\"\n",
        "    #     ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = posterior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_posterior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    return fig\n",
        "\n",
        "\n",
        "from matplotlib import colors\n",
        "def plot_utility(ps):\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "\n",
        "    utility = np.array([[2, -3], [-2, 1]])\n",
        "\n",
        "    expected = prior @ utility\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.16\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.02\n",
        "    small_width = 0.1\n",
        "    left_space = left + small_width + padding\n",
        "    added_space = padding + width\n",
        "\n",
        "    fig = plt.figure(figsize=(17, 3))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_utility = [left + added_space , bottom , width, height]\n",
        "    rect_expected = [left + 2* added_space, bottom , width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_utility = fig.add_axes(rect_utility, sharey=ax_prior)\n",
        "    ax_expected = fig.add_axes(rect_expected)\n",
        "\n",
        "    rect_colormap = plt.cm.Blues\n",
        "\n",
        "    # Data of plots\n",
        "    ax_prior.barh(0, prior[0], facecolor = rect_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = rect_colormap(prior[1]))\n",
        "    ax_utility.matshow(utility, cmap='cool')\n",
        "    norm = colors.Normalize(vmin=-3, vmax=3)\n",
        "    ax_expected.bar(0, expected[0], facecolor = rect_colormap(norm(expected[0])))\n",
        "    ax_expected.bar(1, expected[1], facecolor = rect_colormap(norm(expected[1])))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], xticks = [], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Probability of state\")\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Utility plot details\n",
        "    ax_utility.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'action (a)',\n",
        "                   title = 'Utility')\n",
        "    ax_utility.xaxis.set_ticks_position('bottom')\n",
        "    ax_utility.spines['left'].set_visible(False)\n",
        "    ax_utility.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Expected utility plot details\n",
        "    ax_expected.set(title = 'Expected utility', ylim = [-3, 3],\n",
        "                    xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                    xlabel = 'action (a)',\n",
        "                    yticks = [])\n",
        "    ax_expected.xaxis.set_ticks_position('bottom')\n",
        "    ax_expected.spines['left'].set_visible(False)\n",
        "    ax_expected.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{utility[i,j]:.2f}\"\n",
        "        ax_utility.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = expected[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_expected.text(i, 2.5, c, va='center', ha='center', color='black')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0, measurement):\n",
        "    assert 0.0 <= ps <= 1.0\n",
        "    assert 0.0 <= p_a_s1 <= 1.0\n",
        "    assert 0.0 <= p_a_s0 <= 1.0\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "    likelihood = np.asarray([[p_a_s1, 1-p_a_s1],[p_a_s0, 1-p_a_s0]])\n",
        "    utility = np.array([[2.0, -3.0], [-2.0, 1.0]])\n",
        "    # expected = np.zeros_like(utility)\n",
        "\n",
        "    if measurement == \"Fish\":\n",
        "        posterior = likelihood[:, 0] * prior\n",
        "    else:\n",
        "        posterior = (likelihood[:, 1] * prior).reshape(-1)\n",
        "    posterior /= np.sum(posterior)\n",
        "    # expected[:, 0] = utility[:, 0] * posterior\n",
        "    # expected[:, 1] = utility[:, 1] * posterior\n",
        "    expected = posterior @ utility\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.3\n",
        "    padding = 0.12\n",
        "    small_width = 0.2\n",
        "    left_space = left + small_width + padding\n",
        "    small_padding = 0.05\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 9))\n",
        "\n",
        "    rect_prior = [left, bottom + height + padding, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom + height + padding , width, height]\n",
        "    rect_posterior = [left_space + width + small_padding, bottom + height + padding , small_width, height]\n",
        "\n",
        "    rect_utility = [padding, bottom, width, height]\n",
        "    rect_expected = [padding + width + padding + left, bottom, width, height]\n",
        "\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood)\n",
        "    ax_prior = fig.add_axes(rect_prior, sharey=ax_likelihood)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey=ax_likelihood)\n",
        "    ax_utility = fig.add_axes(rect_utility)\n",
        "    ax_expected = fig.add_axes(rect_expected)\n",
        "\n",
        "    prior_colormap = plt.cm.Blues\n",
        "    posterior_colormap = plt.cm.Greens\n",
        "    expected_colormap = plt.cm.Wistia\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = prior_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = prior_colormap(prior[1]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    ax_posterior.barh(0, posterior[0], facecolor = posterior_colormap(posterior[0]))\n",
        "    ax_posterior.barh(1, posterior[1], facecolor = posterior_colormap(posterior[1]))\n",
        "    ax_utility.matshow(utility, vmin=0., vmax=1., cmap='cool')\n",
        "    # ax_expected.matshow(expected, vmin=0., vmax=1., cmap='Wistia')\n",
        "    ax_expected.bar(0, expected[0], facecolor = expected_colormap(expected[0]))\n",
        "    ax_expected.bar(1, expected[1], facecolor = expected_colormap(expected[1]))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\", xticks = [])\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "    ax_posterior.set(xlim = [0, 1], xticks = [], yticks = [0, 1],\n",
        "                     yticklabels = ['left', 'right'], title = \"Posterior p(s | m)\")\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Utility plot details\n",
        "    ax_utility.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                   xlabel = 'action (a)', yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   title = 'Utility', ylabel = 'state (s)')\n",
        "    ax_utility.xaxis.set_ticks_position('bottom')\n",
        "    ax_utility.spines['left'].set_visible(False)\n",
        "    ax_utility.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Expected Utility plot details\n",
        "    ax_expected.set(ylim = [-2, 2], xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                 xlabel = 'action (a)', title = 'Expected utility', yticks=[])\n",
        "    # ax_expected.axis('off')\n",
        "    ax_expected.spines['left'].set_visible(False)\n",
        "    # ax_expected.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "    #                 xlabel = 'action (a)',\n",
        "    #                title = 'Expected utility')\n",
        "    # ax_expected.xaxis.set_ticks_position('bottom')\n",
        "    # ax_expected.spines['left'].set_visible(False)\n",
        "    # ax_expected.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i in ind:\n",
        "        v = posterior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_posterior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{utility[i,j]:.2f}\"\n",
        "        ax_utility.text(j,i, c, va='center', ha='center', color='black')\n",
        "    # for i,j in zip(x.flatten(), y.flatten()):\n",
        "    #     c = f\"{expected[i,j]:.2f}\"\n",
        "    #     ax_expected.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = expected[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_expected.text(i, v, c, va='center', ha='center', color='black')\n",
        "\n",
        "    return fig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "TpolPjRh26ya",
        "cellView": "form"
      },
      "source": [
        "# @title Helper Functions\n",
        "\n",
        "def compute_marginal(px, py, cor):\n",
        "    \"\"\" Calculate 2x2 joint probabilities given marginals p(x=1), p(y=1) and correlation\n",
        "\n",
        "      Args:\n",
        "        px (scalar): marginal probability of x\n",
        "        py (scalar): marginal probability of y\n",
        "        cor (scalar): correlation value\n",
        "\n",
        "      Returns:\n",
        "        ndarray of size (2, 2): joint probability array of x and y\n",
        "    \"\"\"\n",
        "\n",
        "    p11 = px*py + cor*np.sqrt(px*py*(1-px)*(1-py))\n",
        "    p01 = px - p11\n",
        "    p10 = py - p11\n",
        "    p00 = 1.0 - p11 - p01 - p10\n",
        "\n",
        "    return np.asarray([[p00, p01], [p10, p11]])\n",
        "\n",
        "\n",
        "def compute_cor_range(px,py):\n",
        "    \"\"\" Calculate the allowed range of correlation values given marginals p(x=1)\n",
        "      and p(y=1)\n",
        "\n",
        "    Args:\n",
        "      px (scalar): marginal probability of x\n",
        "      py (scalar): marginal probability of y\n",
        "\n",
        "    Returns:\n",
        "      scalar, scalar: minimum and maximum possible values of correlation\n",
        "    \"\"\"\n",
        "\n",
        "    def p11(corr):\n",
        "        return px*py + corr*np.sqrt(px*py*(1-px)*(1-py))\n",
        "    def p01(corr):\n",
        "        return px - p11(corr)\n",
        "    def p10(corr):\n",
        "        return py - p11(corr)\n",
        "    def p00(corr):\n",
        "        return 1.0 - p11(corr) - p01(corr) - p10(corr)\n",
        "    Cmax = min(fsolve(p01, 0.0), fsolve(p10, 0.0))\n",
        "    Cmin = max(fsolve(p11, 0.0), fsolve(p00, 0.0))\n",
        "    return Cmin, Cmax\n",
        "\n",
        "\n",
        "# @title Helper Functions\n",
        "\n",
        "def simulate_and_plot_SPRT_fixedtime(mu, sigma, stop_time, num_sample,\n",
        "                                     verbose=True):\n",
        "  \"\"\"Simulate and plot a SPRT for a fixed amount of time given a std.\n",
        "\n",
        "  Args:\n",
        "    mu (float): absolute mean value of the symmetric observation distributions\n",
        "    sigma (float): Standard deviation of the observations.\n",
        "    stop_time (int): Number of steps to run before stopping.\n",
        "    num_sample (int): The number of samples to plot.\n",
        "    \"\"\"\n",
        "\n",
        "  evidence_history_list = []\n",
        "  if verbose:\n",
        "    print(\"#Trial\\tTotal_Evidence\\tDecision\")\n",
        "  for i in range(num_sample):\n",
        "    evidence_history, decision, Mvec = simulate_SPRT_fixedtime(mu, sigma, stop_time)\n",
        "    if verbose:\n",
        "      print(\"{}\\t{:f}\\t{}\".format(i, evidence_history[-1], decision))\n",
        "    evidence_history_list.append(evidence_history)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  maxlen_evidence = np.max(list(map(len,evidence_history_list)))\n",
        "  ax.plot(np.zeros(maxlen_evidence), '--', c='red', alpha=1.0)\n",
        "  for evidences in evidence_history_list:\n",
        "    ax.plot(np.arange(len(evidences)), evidences)\n",
        "    ax.set_xlabel(\"Time\")\n",
        "    ax.set_ylabel(\"Accumulated log likelihood ratio\")\n",
        "    ax.set_title(\"Log likelihood ratio trajectories under the fixed-time \" +\n",
        "                  \"stopping rule\")\n",
        "\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def plot_accuracy_vs_stoptime(mu, sigma, stop_time_list, accuracy_analytical_list, accuracy_list=None):\n",
        "  \"\"\"Simulate and plot a SPRT for a fixed amount of times given a std.\n",
        "\n",
        "  Args:\n",
        "    mu (float): absolute mean value of the symmetric observation distributions\n",
        "    sigma (float): Standard deviation of the observations.\n",
        "    stop_time_list (int): List of number of steps to run before stopping.\n",
        "    accuracy_analytical_list (int): List of analytical accuracies for each stop time\n",
        "    accuracy_list (int (optional)): List of simulated accuracies for each stop time\n",
        "  \"\"\"\n",
        "  T = stop_time_list[-1]\n",
        "  fig, ax = plt.subplots(figsize=(12,8))\n",
        "  ax.set_xlabel('Stop Time')\n",
        "  ax.set_ylabel('Average Accuracy')\n",
        "  ax.plot(stop_time_list, accuracy_analytical_list)\n",
        "  if accuracy_list is not None:\n",
        "    ax.plot(stop_time_list, accuracy_list)\n",
        "  ax.legend(['analytical','simulated'], loc='upper center')\n",
        "\n",
        "  # Show two gaussian\n",
        "  stop_time_list_plot = [max(1,T//10), T*2//3]\n",
        "  sigma_st_max = 2*mu*np.sqrt(stop_time_list_plot[-1])/sigma\n",
        "  domain = np.linspace(-3*sigma_st_max,3*sigma_st_max,50)\n",
        "  for stop_time in stop_time_list_plot:\n",
        "    ins = ax.inset_axes([stop_time/T,0.05,0.2,0.3])\n",
        "    for pos in ['right', 'top', 'bottom', 'left']:\n",
        "      ins.spines[pos].set_visible(False)\n",
        "    ins.axis('off')\n",
        "    ins.set_title(f\"stop_time={stop_time}\")\n",
        "\n",
        "    left = np.zeros_like(domain)\n",
        "    mu_st = 4*mu*mu*stop_time/2/sigma**2\n",
        "    sigma_st = 2*mu*np.sqrt(stop_time)/sigma\n",
        "    for i, mu1 in enumerate([-mu_st,mu_st]):\n",
        "      rv = stats.norm(mu1, sigma_st)\n",
        "      offset = rv.pdf(domain)\n",
        "      # lbl = \"measurement distribution\" if i==0 else \"\"\n",
        "      lbl = \"summed evidence\" if i==1 else \"\"\n",
        "      color = \"crimson\"\n",
        "      ls = \"solid\" if i==1 else \"dashed\"\n",
        "      ins.plot(domain, left+offset, label=lbl, color=color,ls=ls)\n",
        "\n",
        "    rv = stats.norm(mu_st, sigma_st)\n",
        "    domain0 = np.linspace(-3*sigma_st_max,0,50)\n",
        "    offset = rv.pdf(domain0)\n",
        "    ins.fill_between(domain0, np.zeros_like(domain0), offset, color=\"crimson\", label=\"error\")\n",
        "    ins.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\n",
        "\n",
        "\n",
        "    # ins.legend(loc=\"upper right\")\n",
        "\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def simulate_and_plot_SPRT_fixedthreshold(mu, sigma, num_sample, alpha,\n",
        "                                          verbose=True):\n",
        "  \"\"\"Simulate and plot a SPRT for a fixed amount of times given a std.\n",
        "\n",
        "  Args:\n",
        "    mu (float): absolute mean value of the symmetric observation distributions\n",
        "    sigma (float): Standard deviation of the observations.\n",
        "    num_sample (int): The number of samples to plot.\n",
        "    alpha (float): Threshold for making a decision.\n",
        "  \"\"\"\n",
        "  # calculate evidence threshold from error rate\n",
        "  threshold = threshold_from_errorrate(alpha)\n",
        "\n",
        "  # run simulation\n",
        "  evidence_history_list = []\n",
        "  if verbose:\n",
        "    print(\"#Trial\\tTime\\tAccumulated Evidence\\tDecision\")\n",
        "  for i in range(num_sample):\n",
        "    evidence_history, decision, Mvec = simulate_SPRT_threshold(mu, sigma, threshold)\n",
        "    if verbose:\n",
        "      print(\"{}\\t{}\\t{:f}\\t{}\".format(i, len(Mvec), evidence_history[-1],\n",
        "                                      decision))\n",
        "    evidence_history_list.append(evidence_history)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  maxlen_evidence = np.max(list(map(len,evidence_history_list)))\n",
        "  ax.plot(np.repeat(threshold,maxlen_evidence + 1), c=\"red\")\n",
        "  ax.plot(-np.repeat(threshold,maxlen_evidence + 1), c=\"red\")\n",
        "  ax.plot(np.zeros(maxlen_evidence + 1), '--', c='red', alpha=0.5)\n",
        "\n",
        "  for evidences in evidence_history_list:\n",
        "      ax.plot(np.arange(len(evidences) + 1), np.concatenate([[0], evidences]))\n",
        "\n",
        "  ax.set_xlabel(\"Time\")\n",
        "  ax.set_ylabel(\"Accumulated log likelihood ratio\")\n",
        "  ax.set_title(\"Log likelihood ratio trajectories under the threshold rule\")\n",
        "\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def simulate_and_plot_accuracy_vs_threshold(mu, sigma, threshold_list, num_sample):\n",
        "  \"\"\"Simulate and plot a SPRT for a set of thresholds given a std.\n",
        "\n",
        "  Args:\n",
        "    mu (float): absolute mean value of the symmetric observation distributions\n",
        "    sigma (float): Standard deviation of the observations.\n",
        "    alpha_list (float): List of thresholds for making a decision.\n",
        "    num_sample (int): The number of samples to plot.\n",
        "  \"\"\"\n",
        "  accuracies, decision_speeds = simulate_accuracy_vs_threshold(mu, sigma,\n",
        "                                                               threshold_list,\n",
        "                                                               num_sample)\n",
        "\n",
        "  # Plotting\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(decision_speeds, accuracies, linestyle=\"--\", marker=\"o\")\n",
        "  ax.plot([np.amin(decision_speeds), np.amax(decision_speeds)],\n",
        "          [0.5, 0.5], c='red')\n",
        "  ax.set_xlabel(\"Average Decision speed\")\n",
        "  ax.set_ylabel('Average Accuracy')\n",
        "  ax.set_title(\"Speed/Accuracy Tradeoff\")\n",
        "  ax.set_ylim(0.45, 1.05)\n",
        "\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def threshold_from_errorrate(alpha):\n",
        "  \"\"\"Calculate log likelihood ratio threshold from desired error rate `alpha`\n",
        "\n",
        "  Args:\n",
        "    alpha (float): in (0,1), the desired error rate\n",
        "\n",
        "  Return:\n",
        "    threshold: corresponding evidence threshold\n",
        "  \"\"\"\n",
        "  threshold = np.log((1. - alpha) / alpha)\n",
        "  return threshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68uQc483pPNg"
      },
      "source": [
        "#Part 1: Bayesian inference with one measurement\n",
        "\n",
        "The bayes theorem is the fundamental element of bayesian inference: \n",
        "\\begin{equation}\n",
        " P(s | m) = \\frac{P(m | s) P(s)}{P(m)}  \\tag{1} \\end{equation}\n",
        "\n",
        " where $s$ is the state and $m$ the measurement. \n",
        "\n",
        "$p(s|m)$ is called the posterior (updated knowledge). It represents your belief after gaining information with the measurement. For example it is the probability that the school of fish is on the right given that a fisherperson caught a fish on the left side of the dock.\n",
        "\n",
        "$p(m|s)$ is called the likelihood. It described of much information you gain from your measurement. For example, when fishing on the left it is the probability to catch a fish, given that the school is on the left. \n",
        "\n",
        "$p(s)$ is the prior knowledge. It describes what you know about the hidden state, before taking a measurement. For example, you know that there is 70% of chance that the fishes are on the left.\n",
        "\n",
        "$p(m)$ describes all possible measurements. It is a normalization term, also called the marginal likelihood or the evidence. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQMftYvJYZuk"
      },
      "source": [
        "##Likelihood\n",
        "\n",
        "Let's think a bit about the likelihood. Suppose that you observe a fisherperson **fishing at the left side of the dock**. Either he caught a fish or he didn't. Let's say that you have 50% chance of catching a fish if you are on the correct side of the dock, but only 10% if your are at the wrong side of the dock. \n",
        "\n",
        "1) Please figure out each of the following:\n",
        "- probability of catching a fish given that the school of fish is on the left side, $P(m = \\textrm{catch fish} | s = \\textrm{left} )$\n",
        "- probability of not catching a fish given that the school of fish is on the left side, $P(m = \\textrm{no fish} | s = \\textrm{left})$\n",
        "- probability of catching a fish given that the school of fish is on the right side, $P(m = \\textrm{catch  fish} | s = \\textrm{right})$\n",
        "- probability of not catching a fish given that the school of fish is on the right side, $P(m = \\textrm{no fish} | s = \\textrm{right})$\n",
        "\n",
        "Sum up these answers in a numpy array as described in the next cell. \n",
        "\n",
        "2) If the fisherperson catches a fish, which side would you guess the school is on?\n",
        "\n",
        "3) If the fisherperson does not catch a fish, which side would you guess the school is on? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyJxsNO-j-5s"
      },
      "source": [
        "###likelihood (ndarray): i x j array with likelihood probabilities where i is number of state options, j is number of measurement options\n",
        "\n",
        "likelihood = ... #first row is s = left, second row is s = right; first column is fish, second is no fish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQVd-xyF6FLu"
      },
      "source": [
        "##Prior\n",
        "\n",
        "In the prior exercise, you tried to guess where the school of fish was based on the measurement you took (watching someone fish). You did this by choosing the state that maximized the probability of the measurement. In other words, you estimated the state by maximizing the likelihood (recall the MLE you've encounter in the first tutorial).\n",
        "\n",
        "But, what if you had been going to this dock for years and you knew that the fish were almost always on the left side? This should probably affect how you make your estimate -- you would rely less on the single new measurement and more on your prior knowledge. This is the fundemental idea behind Bayesian inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYMGA1mJ_Aux"
      },
      "source": [
        "# p(s = left) = 0.7; p(s = right) = 0.3\n",
        "\n",
        "prior = np.array([0.3, 0.7]).reshape((2, 1)) # first row is s = left, second row is s = right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMjGPIMC6HPV"
      },
      "source": [
        "##Normalization (marginal likelihood)\n",
        "\n",
        "When we normalize to find the posterior, we need to determine the marginal likelihood - or evidence - for the measurement we observed. As we do not have direct access to the normalization term $p(m)$, we need to compute through the marginalization process. As stated in Equation 2-4, you can either use joint probabilities or conditional probabilities \n",
        "\n",
        "$$p(m) = \\sum_s p(m, s) \\tag{2} $$ \n",
        "\n",
        "\n",
        "$$ p(m,s) = p(m|s)p(s) \\tag{3} $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$p(m) = \\sum_s p(m | s) p(s) \\tag{4} $$\n",
        "\n",
        "\n",
        "Please complete the following math problem to further practice thinking through probabilities:\n",
        "\n",
        "1. Calculate the marginal likelihood of the fish being caught, $P(m = \\textrm{fish})$, considering the likelihoods and prior defined sooner.\n",
        "\n",
        "\n",
        "Normalizing by this $p(m)$ means that our posterior is a complete probability distribution that sums or integrates to 1 appropriately. \n",
        "For many complicated cases, like those we might be using to model behavioral or brain inferences, the normalization term can be intractable or extremely complex to calculate. We can be careful to choose probability distributions were we can analytically calculate the posterior probability or numerical approximation is reliable. Better yet, we sometimes don't need to bother with this normalization! The normalization term, $p(m)$, is the probability of the measurement. This does not depend on state so is essentially a constant we can often ignore. We can compare the unnormalized posterior distribution values for different states because how they relate to each other is unchanged when divided by the same constant. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZ7yyIy6I6G"
      },
      "source": [
        "##Posterior\n",
        "\n",
        "You are now ready to put everything together ! \n",
        "Our prior is $p(s = \\textrm{left}) = 0.3$ and $p(s = \\textrm{right}) = 0.7$. We also learned that the chance of catching a fish given they fish on the same side as the school was 50%. Otherwise, it was 10%. We observe a person fishing on the left side.\n",
        "\n",
        "First, calculate on paper the posterior probability that... (don't forget to normalize)\n",
        "\n",
        "1. The school is on the left if the fisherperson catches a fish: $p(s = \\textrm{left} | m = \\textrm{fish})$\n",
        "2. The school is on the right if the fisherperson does not catch a fish: $p(s = \\textrm{right} | m = \\textrm{no fish})$\n",
        "\n",
        "Let's implement our above math to be able to compute posteriors for different priors and likelihoods. You are asked to fill a function taking the prior and likelihoods as argument and return the posteriors.  We want our full posterior to take the same 2 by 2 form. Make sure the outputs match your math answers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "KyaGn0QV26yz"
      },
      "source": [
        "def compute_posterior(likelihood, prior):\n",
        "  \"\"\" Use Bayes' Rule to compute posterior from likelihood and prior\n",
        "\n",
        "  Args:\n",
        "    likelihood (ndarray): i x j array with likelihood probabilities where i is\n",
        "                    number of state options, j is number of measurement options\n",
        "    prior (ndarray): i x 1 array with prior probability of each state\n",
        "\n",
        "  Returns:\n",
        "    ndarray: i x j array with posterior probabilities where i is\n",
        "            number of state options, j is number of measurement options\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  ############################\n",
        "  ###### YOUR CODE HERE ######\n",
        "  ############################\n",
        "  \n",
        "  return posterior\n",
        "\n",
        "# prior and likelihood defined above\n",
        "\n",
        "# Compute posterior\n",
        "posterior = compute_posterior(likelihood, prior)\n",
        "\n",
        "# Visualize\n",
        "plot_prior_likelihood_posterior(prior, likelihood, posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CXA6qs9n26yz"
      },
      "source": [
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1665.0 height=674.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D1_BayesianDecisions/static/W3D1_Tutorial1_Solution_1a2cc907_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jc1G9VFp26yz"
      },
      "source": [
        "## Interactive Demo: What affects the posterior?\n",
        "\n",
        "Now that we can understand the implementation of *Bayes rule*, let's vary the parameters of the prior and likelihood to see how changing the prior and likelihood affect the posterior. \n",
        "\n",
        "In the demo below, you can change the prior by playing with the slider for $p( s = left)$. You can also change the likelihood by changing the probability of catching a fish given that the school is on the left and the probability of catching a fish given that the school is on the right. The fisherperson you are observing is fishing on the left.\n",
        " \n",
        "\n",
        "1.   Keeping the likelihood constant, when does the prior have the strongest influence over the posterior? Meaning, when does the posterior look most like the prior no matter whether a fish was caught or not?\n",
        "2.   What happens if the likelihoods for catching a fish are similar when you fish on the correct or incorrect side?\n",
        "3.  Set the prior probability of the state = left to 0.6 and play with the likelihood. When does the likelihood exert the most influence over the posterior?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "5J2x9xwh26y0"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "# style = {'description_width': 'initial'}\n",
        "ps_widget = widgets.FloatSlider(0.3, description='p(s = left)',\n",
        "                                min=0.01, max=0.99, step=0.01)\n",
        "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish on left | state = left)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "p_a_s0_widget = widgets.FloatSlider(0.1, description='p(fish on left | state = right)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "# observed_widget = widgets.Checkbox(value=False, description='Observed fish (m)',\n",
        "#                                  disabled=False, indent=False,\n",
        "#                                  layout=Layout(display=\"flex\", justify_content=\"center\"))\n",
        "\n",
        "observed_widget = ToggleButtons(options=['Fish', 'No Fish'],\n",
        "    description='Observation (m) on the left:', disabled=False, button_style='',\n",
        "    layout=Layout(width='auto', display=\"flex\"),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "widget_ui = VBox([ps_widget,\n",
        "                  HBox([p_a_s1_widget, p_a_s0_widget]),\n",
        "                  observed_widget])\n",
        "widget_out = interactive_output(plot_prior_likelihood,\n",
        "                                {'ps': ps_widget,\n",
        "                                'p_a_s1': p_a_s1_widget,\n",
        "                                'p_a_s0': p_a_s0_widget,\n",
        "                                'measurement': observed_widget})\n",
        "display(widget_ui, widget_out)\n",
        "\n",
        "# @widgets.interact(\n",
        "#     ps=ps_widget,\n",
        "#     p_a_s1=p_a_s1_widget,\n",
        "#     p_a_s0=p_a_s0_widget,\n",
        "#     m_right=observed_widget\n",
        "# )\n",
        "# def make_prior_likelihood_plot(ps,p_a_s1,p_a_s0,m_right):\n",
        "#     fig = plot_prior_likelihood(ps,p_a_s1,p_a_s0,m_right)\n",
        "#     plt.show(fig)\n",
        "#     plt.close(fig)\n",
        "#     return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhokZwlou1lh"
      },
      "source": [
        "We now can use this new, complete probability distribution for any future inference or decisions we like! In fact, as we will see in the second part of the tutorial, we can use it as a new prior! Finally, we often call this probability distribution our beliefs over the hidden states, to emphasize that it is our subjective knowlege about the hidden state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt9ezsX96KU1"
      },
      "source": [
        "##Utility and decision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "LIgENTQo26yj"
      },
      "source": [
        "Now that you have some belief about the location of the fishes, you need to decide where to fish. It may seem obvious - you could just fish on the side where the probability of the fish being is higher. Unfortunately, decisions and actions are always a little more complicated. Deciding where to fish may be influenced by more than just the probability of the school of fish being there. In our example, it is more difficult to fish on the right side because there is a motor boat at this side of the lake. Additionally, fishing on the wrong side, where there arenât many fish, is likely to lead to you spending your afternoon not catching fish and therefore getting a sunburn. The consequences of the action you take is based on the true (but hidden) state of the world **and** the action you choose ! \n",
        "\n",
        "We quantify gains and losses numerically using a **utility** function $U(s,a)$, which describes the consequences of your actions: how much value you gain (or if negative, lose) given the state of the world ($s$) and the action you take ($a$). In our example, our utility can be summarized as:\n",
        "\n",
        "|  U(s,a)   | a = left   | a = right  |\n",
        "| ----------------- |------------|------------|\n",
        "| s = Left          | +2         | -3         |\n",
        "| s = right         | -2         | +1         |\n",
        "\n",
        "\n",
        "If you want to know what to expect from taking the action of fishing on one side or the other, you need to calculate the expected utility by weighing the utilities with the probability of that state occuring. This allows us to choose actions by taking probabilities of events into account: we don't care if the outcome of an action-state pair is a loss if the probability of that state is very low. We can formalize this as:\n",
        "\n",
        "$$ \\text{Expected utility of action a} = \\sum_{s}U(s,a)P(s) $$\n",
        "\n",
        "Implement a function *expected_utility(utility, posterior)* to compute the expected utility associated with an action, given the probability of a state. Thanks to this function, for each of these scenarios, determine which action would have been correct.\n",
        "\n",
        "1.  You just arrived at the dock for the first time and have no sense of where the fish might be. Which side would you choose to fish on given our utility values?\n",
        "2.  You think that the probability of the school being on the left side is very low (0.1) and correspondingly high on the right side (0.9). Which side would you choose to fish on given our utility values?\n",
        "3. What probability distribution of the states would give you equal expected utility ? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyUAz9HbvWg5"
      },
      "source": [
        "def expected_utility(utility, state_proba):\n",
        "  \"\"\" Compute expected utility from utility and probability distribution of the state\n",
        "\n",
        "  Args:\n",
        "    utility (ndarray): i x j array with utility where i is\n",
        "                    number of state options, j is number of action options\n",
        "    state_proba (ndarray): i x 1 array with probability of each state\n",
        "\n",
        "  Returns:\n",
        "    ndarray: i array with the expected utility where i is\n",
        "            number of action options\n",
        "\n",
        "  \"\"\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BZxBj-Uh26y1"
      },
      "source": [
        "## Interactive Demo: What is more important, the probabilities or the utilities?\n",
        "\n",
        "Play a bit with the following widget to gain some intuitions on how each element that goes into a Bayesian decision comes together. Remember, the common assumption in neuroscience, psychology, economics, ecology, etc. is that we (humans and animals) are tying to maximize our expected utility. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "CWPZBH4u26y1",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "# style = {'description_width': 'initial'}\n",
        "\n",
        "ps_widget = widgets.FloatSlider(0.3, description='p(s = left)',\n",
        "                                min=0.01, max=0.99, step=0.01, layout=Layout(width='300px'))\n",
        "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish on left | state = left)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "p_a_s0_widget = widgets.FloatSlider(0.1, description='p(fish on left | state = right)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "\n",
        "observed_widget = ToggleButtons(options=['Fish', 'No Fish'],\n",
        "    description='Observation (m) on the left:', disabled=False, button_style='',\n",
        "    layout=Layout(width='auto', display=\"flex\"),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "widget_ui = VBox([ps_widget,\n",
        "                  HBox([p_a_s1_widget, p_a_s0_widget]),\n",
        "                  observed_widget])\n",
        "\n",
        "widget_out = interactive_output(plot_prior_likelihood_utility,\n",
        "                                {'ps': ps_widget,\n",
        "                                'p_a_s1': p_a_s1_widget,\n",
        "                                'p_a_s0': p_a_s0_widget,\n",
        "                                'measurement': observed_widget})\n",
        "display(widget_ui, widget_out)\n",
        "\n",
        "# @widgets.interact(\n",
        "#     ps=ps_widget,\n",
        "#     p_a_s1=p_a_s1_widget,\n",
        "#     p_a_s0=p_a_s0_widget,\n",
        "#     m_right=observed_widget\n",
        "# )\n",
        "# def make_prior_likelihood_utility_plot(ps, p_a_s1, p_a_s0,m_right):\n",
        "#     fig = plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0,m_right)\n",
        "#     plt.show(fig)\n",
        "#     plt.close(fig)\n",
        "#     return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSrJU5nqpj86"
      },
      "source": [
        "#Part 2: Integration of several measurements (Drift diffusion model)\n",
        "\n",
        "In this second part, we learn how to integrate evidence over time. We will still assume that the world state is _binary_ ($\\pm 1$) and _constant_ over time, but allow for multiple observations over time. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "vtFOwCIzSlfI"
      },
      "source": [
        "\n",
        "**Sequential Probability Ratio Test**\n",
        "\n",
        "The Sequential Probability Ratio Test is a likelihood ratio test for determining which of two hypotheses is more likely given some measurements, taken from time 1 up to time t  ($m_{1:t}$), \n",
        "\n",
        "In practice the likelihood ratio is the ratio of the likelihood of all these measurements given the state is +1 to the likelihood of the measurements given the state is -1. For some practical reasons, we are taking the log of the ratio. \n",
        "\n",
        "\\begin{align*}\n",
        "L_T &= log\\frac{p(m_{1:t}|s=+1)}{p(m_{1:t}|s=-1)}\n",
        "\\end{align*}\n",
        "\n",
        "The SPRT states that if $L_T$ is positive, then the state $s=+1$ is more likely than $s=-1$! \n",
        "\n",
        "\n",
        "Since our data (measurements) is independent and identically distributed, the probability of all measurements given the state equals the product of the separate probabilities of each measurement given the state:\n",
        "$$ p(m_{1:t}|s) = \\prod_{t=1}^T p(m_t | s) $$\n",
        "\n",
        " We can substitute this in and use log properties to convert to a sum.\n",
        "\n",
        "\\begin{align*}\n",
        "L_T &= log\\frac{p(m_{1:t}|s=+1)}{p(m_{1:t}|s=-1)}\\\\\n",
        "&= log\\frac{\\prod_{t=1}^Tp(m_{t}|s=+1)}{\\prod_{t=1}^Tp(m_{t}|s=-1)}\\\\\n",
        "&= \\sum_{t=1}^T log\\frac{p(m_{t}|s=+1)}{p(m_{t}|s=-1)}\\\\\n",
        "&= \\sum_{t=1}^T \\Delta_t\n",
        "\\end{align*}\n",
        "\n",
        "with $\\Delta_t = log\\frac{p(m_{t}|s=+1)}{p(m_{t}|s=-1)}$, the ratio at each time step.  \n",
        "\n",
        "To get the full log likelihood ratio, we are summing up the log likelihood ratios at each time step. The log likelihood ratio at a time step ($L_T$) will equal the ratio at the previous time step ($L_{T-1}$) plus the ratio for the measurement at that time step, given by $\\Delta_T$:\n",
        "\n",
        "\\begin{align*}\n",
        "L_T =  L_{T-1} + \\Delta_T\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "**Sequential Probability Ratio Test as a Drift Diffusion Model**\n",
        "\n",
        "Let's now assume that the probability of seeing a measurement given the state is a Gaussian distribution where the mean ($\\mu$) is different for the two states but the standard deviation ($\\sigma$) is the same:\n",
        "\n",
        "\\begin{align*}\n",
        "p(m_t | s = +1) &= \\mathcal{N}(\\mu, \\sigma^2)\\\\\n",
        "p(m_t | s = -1) &= \\mathcal{N}(-\\mu, \\sigma^2)\\\\\n",
        "\\end{align*}\n",
        "\n",
        "We can write the new evidence (the log likelihood ratio for the measurement at time $t$) as\n",
        "\n",
        "$$\\Delta_t=b+c\\epsilon_t$$\n",
        "\n",
        "The first term, $b$, is a consistant value and equals $b=2\\mu^2/\\sigma^2$. This term favors the actual hidden state. The second term, $c\\epsilon_t$ where $\\epsilon_t\\sim\\mathcal{N}(0,1)$, is a standard random variable which is scaled by the diffusion $c=2\\mu/\\sigma$. \n",
        "\n",
        "DEMONSTRATION DE CA ? ETRE A L'AISE POUR L'EXPLIQUER \n",
        "\n",
        "The accumulation of evidence will thus \"drift\" toward one outcome, while \"diffusing\" in random directions, hence the term \"drift-diffusion model\" (DDM). The process is most likely (but not guaranteed) to reach the correct outcome eventually.\n",
        "\n",
        "This tutorial is simply to provide an intuition about the DMM, more details will be introduced in LGBIO2072\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "-hItrZjtSlfJ"
      },
      "source": [
        "##  Coding Exercise: Simulating an SPRT model\n",
        "\n",
        "Let's now generate simulated data with $s=+1$ and see if the SPRT can infer the state correctly.\n",
        "\n",
        "You will implement a function `simulate_SPRT_fixedtime`, which will generate data according to the true state and the probability distributions (this part is already done for you) and accumulate evidence over the time steps and output a decision on the state. The decision will be the state that is more likely according to the accumulated evidence. It might be useful to implement first a function `log_likelihood_ratio`, which computes the ratio as defined above. (*Hint: see the logpdf method in scipy.stats.norm*) \n",
        "\n",
        "We will then visualize 10 simulations of the DDM. In the next exercise you'll see how the parameters affect performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "1b7tL3MKY2Fz"
      },
      "source": [
        "def simulate_SPRT_fixedtime(mu, sigma, stop_time, true_dist = 1):\n",
        "  \"\"\"Simulate a Sequential Probability Ratio Test with fixed time stopping\n",
        "  rule. Two observation models are 1D Gaussian distributions N(mu,sigma^2) and\n",
        "  N(-mu,sigma^2).\n",
        "\n",
        "  Args:\n",
        "    mu (float): absolute mean value of the symmetric observation distributions\n",
        "    sigma (float): Standard deviation of observation models\n",
        "    stop_time (int): Number of samples to take before stopping\n",
        "    true_dist (1 or -1): Which state is the true state.\n",
        "\n",
        "  Returns:\n",
        "    evidence_history (numpy vector): the history of cumulated evidence given\n",
        "                                      generated data\n",
        "    decision (int): 1 for s = 1, -1 for s = -1\n",
        "    Mvec (numpy vector): the generated sequences of measurement data in this trial\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Set means of observation distributions\n",
        "  assert mu > 0, \"Mu should be > 0\"\n",
        "  mu_pos = mu\n",
        "  mu_neg = -mu\n",
        "\n",
        "  # Make observation distributions\n",
        "  p_pos = stats.norm(loc = mu_pos, scale = sigma)\n",
        "  p_neg = stats.norm(loc = mu_neg, scale = sigma)\n",
        "\n",
        "  # Generate a random sequence of measurements\n",
        "  if true_dist == 1:\n",
        "    Mvec = p_pos.rvs(size = stop_time)\n",
        "  else:\n",
        "    Mvec = p_neg.rvs(size = stop_time)\n",
        "\n",
        "\n",
        "  # Step 1 : Calculate log likelihood ratio for each measurement (delta_t)\n",
        "  ll_ratio_vec = ...\n",
        "\n",
        "  # STEP 1: Calculate accumulated evidence given a time series of evidence\n",
        "  evidence_history = ...\n",
        "\n",
        "  # STEP 2: Make decision based on the sign of the evidence at the final time.\n",
        "  decision = ...\n",
        "\n",
        "  return evidence_history, decision, Mvec\n",
        "\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(100)\n",
        "\n",
        "# Set model parameters\n",
        "mu = .2\n",
        "sigma = 3.5  # standard deviation for p+ and p-\n",
        "num_sample = 10  # number of simulations to run\n",
        "stop_time = 150 # number of steps before stopping\n",
        "\n",
        "# Simulate and visualize\n",
        "simulate_and_plot_SPRT_fixedtime(mu, sigma, stop_time, num_sample)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "aZMVXN9USlfK"
      },
      "source": [
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1145.0 height=832.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D2_HiddenDynamics/static/W3D2_Tutorial1_Solution_985833af_1.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jUav6NWRSlfL"
      },
      "source": [
        "## Interactive Demo: Trajectories under the fixed-time stopping rule\n",
        "\n",
        "\n",
        "In the following demo, you can change the drift level (mu), noise level (sigma) in the observation model and the number of time steps before stopping (stop_time) using the sliders. You will then observe 10 simulations with those parameters. As in the previous exercise, the true state is +1.\n",
        "\n",
        "1.   Are you more likely to make the wrong decision (choose the incorrect state) with high or low noise?\n",
        "2. What happens when sigma is very small? Why?\n",
        "3.   Are you more likely to make the wrong decision (choose the incorrect state) with fewer or more time steps before stopping?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "execution": {},
        "id": "QoTyFegISlfL"
      },
      "source": [
        "# @markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "def simulate_SPRT_fixedtime(mu, sigma, stop_time, true_dist = 1):\n",
        "  \"\"\"Simulate a Sequential Probability Ratio Test with fixed time stopping\n",
        "  rule. Two observation models are 1D Gaussian distributions N(1,sigma^2) and\n",
        "  N(-1,sigma^2).\n",
        "\n",
        "  Args:\n",
        "    mu (float): absolute mean value of the symmetric observation distributions\n",
        "    sigma (float): Standard deviation of observation models\n",
        "    stop_time (int): Number of samples to take before stopping\n",
        "    true_dist (1 or -1): Which state is the true state.\n",
        "\n",
        "  Returns:\n",
        "    evidence_history (numpy vector): the history of cumulated evidence given\n",
        "                                      generated data\n",
        "    decision (int): 1 for s = 1, -1 for s = -1\n",
        "    Mvec (numpy vector): the generated sequences of measurement data in this trial\n",
        "  \"\"\"\n",
        "\n",
        "  # Set means of observation distributions\n",
        "  assert mu > 0, \"Mu should be >0\"\n",
        "  mu_pos = mu\n",
        "  mu_neg = -mu\n",
        "\n",
        "  # Make observation distributions\n",
        "  p_pos = stats.norm(loc = mu_pos, scale = sigma)\n",
        "  p_neg = stats.norm(loc = mu_neg, scale = sigma)\n",
        "\n",
        "  # Generate a random sequence of measurements\n",
        "  if true_dist == 1:\n",
        "    Mvec = p_pos.rvs(size = stop_time)\n",
        "  else:\n",
        "    Mvec = p_neg.rvs(size = stop_time)\n",
        "\n",
        "  # Calculate log likelihood ratio for each measurement (delta_t)\n",
        "  ll_ratio_vec = log_likelihood_ratio(Mvec, p_neg, p_pos)\n",
        "\n",
        "  # STEP 1: Calculate accumulated evidence (S) given a time series of evidence (hint: np.cumsum)\n",
        "  evidence_history = np.cumsum(ll_ratio_vec)\n",
        "\n",
        "  # STEP 2: Make decision based on the sign of the evidence at the final time.\n",
        "  decision = np.sign(evidence_history[-1])\n",
        "\n",
        "  return evidence_history, decision, Mvec\n",
        "\n",
        "np.random.seed(100)\n",
        "num_sample = 10\n",
        "\n",
        "@widgets.interact(mu=widgets.FloatSlider(min=0.1, max=5.0, step=0.1, value=0.5), sigma=(0.05, 10.0, 0.05), stop_time=(5, 500, 1))\n",
        "def plot(mu, sigma, stop_time):\n",
        "  simulate_and_plot_SPRT_fixedtime(mu, sigma, stop_time, num_sample, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "AsgMM3haSlfP"
      },
      "source": [
        "\n",
        "## Summary\n",
        "\n",
        "Good job! By simulating Drift Diffusion Models, you have learnt how to:\n",
        "\n",
        "* Calculate individual sample evidence as the log likelihood ratio of two candidate models\n",
        "* Accumulate evidence from new data points, and compute posterior using recursive formula\n",
        "\n",
        "## Further insights... \n",
        "\n",
        "As you probably conclude from the interactive demo, the accuracy of your decision depends on how quickly you have to make that decision. It is then also interesting to analyze the speed-accuracy trade-off of your model. The speed-accuracy trade-off is typically the kind of experimental data that are useful to set your model parameters. **Mettre en ref un article pour ceux qui veulent**\n",
        "\n",
        "Other stopping rules exist. For example, instead of making your decision after a fixed number of sample, you could set some thresholds (one for decision A, one for decision B). Once your evidence goes beyond one or the other threshold, your decision is made.  \n",
        "\n"
      ]
    }
  ]
}