{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "cfc5db31",
        "f04d4f4e",
        "0769a1c8",
        "37db783c",
        "NUOfjg2mComQ",
        "EQMftYvJYZuk",
        "UQVd-xyF6FLu",
        "LIgENTQo26yj"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53044446"
      },
      "source": [
        "# LGBIO2060 Exercice session 1\n",
        "__Authors:__ Simon Vandergooten and Cl√©mence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n",
        "\n",
        "In this first exercise session, you will first review the basics of statistics needed for the upcoming exercise sessions and projects. In a second part, you will use the bayesian inference to produce an estimate of a hidden binary state and take decisions based on this estimate and the associated uncertainty.\n",
        "\n",
        "After this session you should be able to:\n",
        "* Sample data from a specific probability distribution.\n",
        "* Use basic probability rules to infer parameters.\n",
        "* Understand and use the maximum likelihood principle.\n",
        "* Understand the Bayes' rule and the impact of a prior in the inference.\n",
        "* Use probability distribution to represent hidden states\n",
        "* Combine new information with your prior knowledge\n",
        "* Combine the possible loss (or gain) for making a decision with your probabilistic knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647e2160"
      },
      "source": [
        "## Imports and helper functions\n",
        "**Please execute the cell(s) below to initialize the notebook environment.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69d47f08"
      },
      "source": [
        "#Import the libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "from scipy.stats import norm\n",
        "from numpy.random import default_rng\n",
        "from matplotlib import patches, transforms, gridspec\n",
        "from scipy.optimize import fsolve\n",
        "from collections import namedtuple\n",
        "from scipy import stats\n",
        "from scipy.special import erf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c399337b",
        "cellView": "form"
      },
      "source": [
        "#@title Figure Settings\n",
        "import ipywidgets as widgets  # interactive display\n",
        "from ipywidgets import interact, fixed, HBox, Layout, VBox, interactive, Label, interact_manual\n",
        "from ipywidgets import GridspecLayout, HBox, VBox, FloatSlider, Layout, ToggleButtons\n",
        "from ipywidgets import interactive, interactive_output, Checkbox, Select\n",
        "from IPython.display import clear_output\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")\n",
        "my_layout = widgets.Layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35ba945b",
        "cellView": "form"
      },
      "source": [
        "#@title Plotting Functions\n",
        "\n",
        "def plot_random_sample(x, y, figtitle = None):\n",
        "    \"\"\" Plot the random sample between 0 and 1 for both the x and y axes.\n",
        "\n",
        "    Args:\n",
        "        x (ndarray): array of x coordinate values across the random sample\n",
        "        y (ndarray): array of y coordinate values across the random sample\n",
        "        figtitle (str): title of histogram plot (default is no title)\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel('x')\n",
        "    ax.set_ylabel('y')\n",
        "    plt.xlim([-0.25, 1.25]) # set x and y axis range to be a bit less than 0 and greater than 1\n",
        "    plt.ylim([-0.25, 1.25])\n",
        "    plt.scatter(dataX, dataY)\n",
        "    if figtitle is not None:\n",
        "        fig.suptitle(figtitle, size=16)\n",
        "    plt.show()\n",
        "\n",
        "def plot_random_walk(x, y, figtitle = None):\n",
        "    \"\"\" Plots the random walk within the range 0 to 1 for both the x and y axes.\n",
        "\n",
        "    Args:\n",
        "        x (ndarray): array of steps in x direction\n",
        "        y (ndarray): array of steps in y direction\n",
        "        figtitle (str): title of histogram plot (default is no title)\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    plt.plot(x,y,'b-o', alpha = 0.5)\n",
        "    plt.xlim(-0.1,1.1)\n",
        "    plt.ylim(-0.1,1.1)\n",
        "    ax.set_xlabel('x location')\n",
        "    ax.set_ylabel('y location')\n",
        "    plt.plot(x[0], y[0], 'go')\n",
        "    plt.plot(x[-1], y[-1], 'ro')\n",
        "\n",
        "    if figtitle is not None:\n",
        "        fig.suptitle(figtitle, size=16)\n",
        "    plt.show()\n",
        "\n",
        "def plot_hist(data, xlabel, figtitle = None, num_bins = None):\n",
        "    \"\"\" Plot the given data as a histogram.\n",
        "\n",
        "    Args:\n",
        "        data (ndarray): array with data to plot as histogram\n",
        "        xlabel (str): label of x-axis\n",
        "        figtitle (str): title of histogram plot (default is no title)\n",
        "        num_bins (int): number of bins for histogram (default is 10)\n",
        "\n",
        "    Returns:\n",
        "        count (ndarray): number of samples in each histogram bin\n",
        "        bins (ndarray): center of each histogram bin\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Count')\n",
        "    if num_bins is not None:\n",
        "        count, bins, _ = plt.hist(data, bins = num_bins)\n",
        "    else:\n",
        "        count, bins, _ = plt.hist(data, bins = np.arange(np.min(data)-.5, np.max(data)+.6)) # 10 bins default\n",
        "    if figtitle is not None:\n",
        "        fig.suptitle(figtitle, size=16)\n",
        "    plt.show()\n",
        "    return count, bins\n",
        "\n",
        "def my_plot_single(x, px):\n",
        "    \"\"\"\n",
        "    Plots normalized Gaussian distribution\n",
        "\n",
        "    Args:\n",
        "        x (numpy array of floats):     points at which the likelihood has been evaluated\n",
        "        px (numpy array of floats):    normalized probabilities for prior evaluated at each `x`\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "  \"\"\"\n",
        "    if px is None:\n",
        "        px = np.zeros_like(x)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, px, '-', color='C2', linewidth=2, label='Prior')\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Orientation (Degrees)')\n",
        "\n",
        "def plot_gaussian_samples_true(samples, xspace, mu, sigma, xlabel, ylabel):\n",
        "    \"\"\" Plot a histogram of the data samples on the same plot as the gaussian\n",
        "    distribution specified by the give mu and sigma values.\n",
        "\n",
        "    Args:\n",
        "        samples (ndarray): data samples for gaussian distribution\n",
        "        xspace (ndarray): x values to sample from normal distribution\n",
        "        mu (scalar): mean parameter of normal distribution\n",
        "        sigma (scalar): variance parameter of normal distribution\n",
        "        xlabel (str): the label of the x-axis of the histogram\n",
        "        ylabel (str): the label of the y-axis of the histogram\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    # num_samples = samples.shape[0]\n",
        "\n",
        "    count, bins, _ = plt.hist(samples, density=True)\n",
        "    plt.plot(xspace, norm.pdf(xspace, mu, sigma),'r-')\n",
        "    plt.show()\n",
        "\n",
        "def plot_likelihoods(likelihoods, mean_vals, variance_vals):\n",
        "    \"\"\" Plot the likelihood values on a heatmap plot where the x and y axes match\n",
        "    the mean and variance parameter values the likelihoods were computed for.\n",
        "\n",
        "    Args:\n",
        "        likelihoods (ndarray): array of computed likelihood values\n",
        "        mean_vals (ndarray): array of mean parameter values for which the\n",
        "                            likelihood was computed\n",
        "        variance_vals (ndarray): array of variance parameter values for which the\n",
        "                            likelihood was computed\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(likelihoods)\n",
        "\n",
        "    cbar = ax.figure.colorbar(im, ax=ax)\n",
        "    cbar.ax.set_ylabel('log likelihood', rotation=-90, va=\"bottom\")\n",
        "\n",
        "    ax.set_xticks(np.arange(len(mean_vals)))\n",
        "    ax.set_yticks(np.arange(len(variance_vals)))\n",
        "    ax.set_xticklabels(mean_vals)\n",
        "    ax.set_yticklabels(variance_vals)\n",
        "    ax.set_xlabel('Mean')\n",
        "    ax.set_ylabel('Variance')\n",
        "\n",
        "def posterior_plot(x, likelihood=None, prior=None, posterior_pointwise=None, ax=None):\n",
        "    \"\"\"\n",
        "    Plots normalized Gaussian distributions and posterior.\n",
        "\n",
        "    Args:\n",
        "        x (numpy array of floats):         points at which the likelihood has been evaluated\n",
        "        auditory (numpy array of floats):  normalized probabilities for auditory likelihood evaluated at each `x`\n",
        "        visual (numpy array of floats):    normalized probabilities for visual likelihood evaluated at each `x`\n",
        "        posterior (numpy array of floats): normalized probabilities for the posterior evaluated at each `x`\n",
        "        ax: Axis in which to plot. If None, create new axis.\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    if likelihood is None:\n",
        "        likelihood = np.zeros_like(x)\n",
        "\n",
        "    if prior is None:\n",
        "        prior = np.zeros_like(x)\n",
        "\n",
        "    if posterior_pointwise is None:\n",
        "        posterior_pointwise = np.zeros_like(x)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "    ax.plot(x, likelihood, '-C1', LineWidth=2, label='Auditory')\n",
        "    ax.plot(x, prior, '-C0', LineWidth=2, label='Visual')\n",
        "    ax.plot(x, posterior_pointwise, '-C2', LineWidth=2, label='Posterior')\n",
        "    ax.legend()\n",
        "    ax.set_ylabel('Probability')\n",
        "    ax.set_xlabel('Orientation (Degrees)')\n",
        "    plt.show()\n",
        "\n",
        "    return ax\n",
        "\n",
        "def plot_classical_vs_bayesian_normal(num_points, mu_classic, var_classic,\n",
        "                                      mu_bayes, var_bayes):\n",
        "    \"\"\" Helper function to plot optimal normal distribution parameters for varying\n",
        "    observed sample sizes using both classic and Bayesian inference methods.\n",
        "\n",
        "    Args:\n",
        "        num_points (int): max observed sample size to perform inference with\n",
        "        mu_classic (ndarray): estimated mean parameter for each observed sample size\n",
        "                                using classic inference method\n",
        "        var_classic (ndarray): estimated variance parameter for each observed sample size\n",
        "                                using classic inference method\n",
        "        mu_bayes (ndarray): estimated mean parameter for each observed sample size\n",
        "                                using Bayesian inference method\n",
        "        var_bayes (ndarray): estimated variance parameter for each observed sample size\n",
        "                                using Bayesian inference method\n",
        "\n",
        "    Returns:\n",
        "        Nothing.\n",
        "    \"\"\"\n",
        "    xspace = np.linspace(0, num_points, num_points)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel('n data points')\n",
        "    ax.set_ylabel('mu')\n",
        "    plt.plot(xspace, mu_classic,'r-', label = \"Classical\")\n",
        "    plt.plot(xspace, mu_bayes,'b-', label = \"Bayes\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.set_xlabel('n data points')\n",
        "    ax.set_ylabel('sigma^2')\n",
        "    plt.plot(xspace, var_classic,'r-', label = \"Classical\")\n",
        "    plt.plot(xspace, var_bayes,'b-', label = \"Bayes\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# @title Plotting Functions\n",
        "\n",
        "def plot_prior_likelihood_posterior(prior, likelihood, posterior):\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.12\n",
        "    small_width = 0.1\n",
        "    left_space = left + small_width + padding\n",
        "    added_space = padding + width\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom , width, height]\n",
        "    rect_posterior = [left_space +  added_space, bottom , width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood, sharey=ax_prior)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey = ax_prior)\n",
        "\n",
        "    rect_colormap = plt.cm.Blues\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = rect_colormap(prior[0, 0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = rect_colormap(prior[1, 0]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='Greens')\n",
        "\n",
        "\n",
        "    # Probabilities plot details\n",
        "    # ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "    #              ylabel = 'state (s)', title = \"Prior p(s)\")\n",
        "    ax_prior.set(xlim = [1, 0], xticks = [], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\")\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m (left) | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "\n",
        "    ax_posterior.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Posterior p(s | m)')\n",
        "    ax_posterior.xaxis.set_ticks_position('bottom')\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{posterior[i,j]:.2f}\"\n",
        "        ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i, 0]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "\n",
        "\n",
        "def plot_prior_likelihood(ps, p_a_s1, p_a_s0, measurement):\n",
        "    likelihood = np.asarray([[p_a_s1, 1-p_a_s1],[p_a_s0, 1-p_a_s0]])\n",
        "    assert 0.0 <= ps <= 1.0\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "    if measurement == \"Fish\":\n",
        "        posterior = likelihood[:, 0] * prior\n",
        "    else:\n",
        "        posterior = (likelihood[:, 1] * prior).reshape(-1)\n",
        "    posterior /= np.sum(posterior)\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.12\n",
        "    small_width = 0.2\n",
        "    left_space = left + small_width + padding\n",
        "    small_padding = 0.05\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 4))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom , width, height]\n",
        "    rect_posterior = [left_space + width + small_padding, bottom , small_width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood, sharey=ax_prior)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey=ax_prior)\n",
        "\n",
        "    prior_colormap = plt.cm.Blues\n",
        "    posterior_colormap = plt.cm.Greens\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = prior_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = prior_colormap(prior[1]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    # ax_posterior.matshow(posterior, vmin=0., vmax=1., cmap='')\n",
        "    ax_posterior.barh(0, posterior[0], facecolor = posterior_colormap(posterior[0]))\n",
        "    ax_posterior.barh(1, posterior[1], facecolor = posterior_colormap(posterior[1]))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\", xticks = [])\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "    ax_posterior.set(xlim = [0, 1], xticks = [], yticks = [0, 1],\n",
        "                     yticklabels = ['left', 'right'], title = \"Posterior p(s | m)\")\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    # for i,j in zip(x.flatten(), y.flatten()):\n",
        "    #     c = f\"{posterior[i,j]:.2f}\"\n",
        "    #     ax_posterior.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = posterior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_posterior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "from matplotlib import colors\n",
        "def plot_utility(ps):\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "\n",
        "    utility = np.array([[2, -3], [-2, 1]])\n",
        "\n",
        "    expected = prior @ utility\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.16\n",
        "    bottom, height = 0.05, 0.9\n",
        "    padding = 0.02\n",
        "    small_width = 0.1\n",
        "    left_space = left + small_width + padding\n",
        "    added_space = padding + width\n",
        "\n",
        "    fig = plt.figure(figsize=(17, 3))\n",
        "\n",
        "    rect_prior = [left, bottom, small_width, height]\n",
        "    rect_utility = [left + added_space , bottom , width, height]\n",
        "    rect_expected = [left + 2* added_space, bottom , width, height]\n",
        "\n",
        "    ax_prior = fig.add_axes(rect_prior)\n",
        "    ax_utility = fig.add_axes(rect_utility, sharey=ax_prior)\n",
        "    ax_expected = fig.add_axes(rect_expected)\n",
        "\n",
        "    rect_colormap = plt.cm.Blues\n",
        "\n",
        "    # Data of plots\n",
        "    ax_prior.barh(0, prior[0], facecolor = rect_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = rect_colormap(prior[1]))\n",
        "    ax_utility.matshow(utility, cmap='cool')\n",
        "    norm = colors.Normalize(vmin=-3, vmax=3)\n",
        "    ax_expected.bar(0, expected[0], facecolor = rect_colormap(norm(expected[0])))\n",
        "    ax_expected.bar(1, expected[1], facecolor = rect_colormap(norm(expected[1])))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], xticks = [], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Probability of state\")\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Utility plot details\n",
        "    ax_utility.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'action (a)',\n",
        "                   title = 'Utility')\n",
        "    ax_utility.xaxis.set_ticks_position('bottom')\n",
        "    ax_utility.spines['left'].set_visible(False)\n",
        "    ax_utility.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Expected utility plot details\n",
        "    ax_expected.set(title = 'Expected utility', ylim = [-3, 3],\n",
        "                    xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                    xlabel = 'action (a)',\n",
        "                    yticks = [])\n",
        "    ax_expected.xaxis.set_ticks_position('bottom')\n",
        "    ax_expected.spines['left'].set_visible(False)\n",
        "    ax_expected.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{utility[i,j]:.2f}\"\n",
        "        ax_utility.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = expected[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_expected.text(i, 2.5, c, va='center', ha='center', color='black')\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0, measurement):\n",
        "    assert 0.0 <= ps <= 1.0\n",
        "    assert 0.0 <= p_a_s1 <= 1.0\n",
        "    assert 0.0 <= p_a_s0 <= 1.0\n",
        "    prior = np.asarray([ps, 1 - ps])\n",
        "    likelihood = np.asarray([[p_a_s1, 1-p_a_s1],[p_a_s0, 1-p_a_s0]])\n",
        "    utility = np.array([[2.0, -3.0], [-2.0, 1.0]])\n",
        "    # expected = np.zeros_like(utility)\n",
        "\n",
        "    if measurement == \"Fish\":\n",
        "        posterior = likelihood[:, 0] * prior\n",
        "    else:\n",
        "        posterior = (likelihood[:, 1] * prior).reshape(-1)\n",
        "    posterior /= np.sum(posterior)\n",
        "    # expected[:, 0] = utility[:, 0] * posterior\n",
        "    # expected[:, 1] = utility[:, 1] * posterior\n",
        "    expected = posterior @ utility\n",
        "\n",
        "    # definitions for the axes\n",
        "    left, width = 0.05, 0.3\n",
        "    bottom, height = 0.05, 0.3\n",
        "    padding = 0.12\n",
        "    small_width = 0.2\n",
        "    left_space = left + small_width + padding\n",
        "    small_padding = 0.05\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 9))\n",
        "\n",
        "    rect_prior = [left, bottom + height + padding, small_width, height]\n",
        "    rect_likelihood = [left_space , bottom + height + padding , width, height]\n",
        "    rect_posterior = [left_space + width + small_padding, bottom + height + padding , small_width, height]\n",
        "\n",
        "    rect_utility = [padding, bottom, width, height]\n",
        "    rect_expected = [padding + width + padding + left, bottom, width, height]\n",
        "\n",
        "    ax_likelihood = fig.add_axes(rect_likelihood)\n",
        "    ax_prior = fig.add_axes(rect_prior, sharey=ax_likelihood)\n",
        "    ax_posterior = fig.add_axes(rect_posterior, sharey=ax_likelihood)\n",
        "    ax_utility = fig.add_axes(rect_utility)\n",
        "    ax_expected = fig.add_axes(rect_expected)\n",
        "\n",
        "    prior_colormap = plt.cm.Blues\n",
        "    posterior_colormap = plt.cm.Greens\n",
        "    expected_colormap = plt.cm.Wistia\n",
        "\n",
        "    # Show posterior probs and marginals\n",
        "    ax_prior.barh(0, prior[0], facecolor = prior_colormap(prior[0]))\n",
        "    ax_prior.barh(1, prior[1], facecolor = prior_colormap(prior[1]))\n",
        "    ax_likelihood.matshow(likelihood, vmin=0., vmax=1., cmap='Reds')\n",
        "    ax_posterior.barh(0, posterior[0], facecolor = posterior_colormap(posterior[0]))\n",
        "    ax_posterior.barh(1, posterior[1], facecolor = posterior_colormap(posterior[1]))\n",
        "    ax_utility.matshow(utility, vmin=0., vmax=1., cmap='cool')\n",
        "    # ax_expected.matshow(expected, vmin=0., vmax=1., cmap='Wistia')\n",
        "    ax_expected.bar(0, expected[0], facecolor = expected_colormap(expected[0]))\n",
        "    ax_expected.bar(1, expected[1], facecolor = expected_colormap(expected[1]))\n",
        "\n",
        "    # Probabilities plot details\n",
        "    ax_prior.set(xlim = [1, 0], yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                 title = \"Prior p(s)\", xticks = [])\n",
        "    ax_prior.yaxis.tick_right()\n",
        "    ax_prior.spines['left'].set_visible(False)\n",
        "    ax_prior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Likelihood plot details\n",
        "    ax_likelihood.set(xticks = [0, 1], xticklabels = ['fish', 'no fish'],\n",
        "                  yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   ylabel = 'state (s)', xlabel = 'measurement (m)',\n",
        "                   title = 'Likelihood p(m | s)')\n",
        "    ax_likelihood.xaxis.set_ticks_position('bottom')\n",
        "    ax_likelihood.spines['left'].set_visible(False)\n",
        "    ax_likelihood.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Posterior plot details\n",
        "    ax_posterior.set(xlim = [0, 1], xticks = [], yticks = [0, 1],\n",
        "                     yticklabels = ['left', 'right'], title = \"Posterior p(s | m)\")\n",
        "    ax_posterior.spines['left'].set_visible(False)\n",
        "    ax_posterior.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Utility plot details\n",
        "    ax_utility.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                   xlabel = 'action (a)', yticks = [0, 1], yticklabels = ['left', 'right'],\n",
        "                   title = 'Utility', ylabel = 'state (s)')\n",
        "    ax_utility.xaxis.set_ticks_position('bottom')\n",
        "    ax_utility.spines['left'].set_visible(False)\n",
        "    ax_utility.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # Expected Utility plot details\n",
        "    ax_expected.set(ylim = [-2, 2], xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "                 xlabel = 'action (a)', title = 'Expected utility', yticks=[])\n",
        "    # ax_expected.axis('off')\n",
        "    ax_expected.spines['left'].set_visible(False)\n",
        "    # ax_expected.set(xticks = [0, 1], xticklabels = ['left', 'right'],\n",
        "    #                 xlabel = 'action (a)',\n",
        "    #                title = 'Expected utility')\n",
        "    # ax_expected.xaxis.set_ticks_position('bottom')\n",
        "    # ax_expected.spines['left'].set_visible(False)\n",
        "    # ax_expected.spines['bottom'].set_visible(False)\n",
        "\n",
        "    # show values\n",
        "    ind = np.arange(2)\n",
        "    x,y = np.meshgrid(ind,ind)\n",
        "    for i in ind:\n",
        "        v = posterior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_posterior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{likelihood[i,j]:.2f}\"\n",
        "        ax_likelihood.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i,j in zip(x.flatten(), y.flatten()):\n",
        "        c = f\"{utility[i,j]:.2f}\"\n",
        "        ax_utility.text(j,i, c, va='center', ha='center', color='black')\n",
        "    # for i,j in zip(x.flatten(), y.flatten()):\n",
        "    #     c = f\"{expected[i,j]:.2f}\"\n",
        "    #     ax_expected.text(j,i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = prior[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_prior.text(v+0.2, i, c, va='center', ha='center', color='black')\n",
        "    for i in ind:\n",
        "        v = expected[i]\n",
        "        c = f\"{v:.2f}\"\n",
        "        ax_expected.text(i, v, c, va='center', ha='center', color='black')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc5db31"
      },
      "source": [
        "## Section 1: Probability distribution\n",
        "\n",
        "This section will be devoted to the exploration of some probability distribution and discover some useful functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1761d1f9"
      },
      "source": [
        "### 1.1: Create randomness\n",
        "\n",
        "Numpy has many functions and capabilities related to randomness.  We can draw random numbers from various probability distributions. For example, to draw 5 uniform numbers between 0 and 100, you would use `np.random.uniform(0, 100, size = (5,))`.\n",
        "\n",
        " We will use `np.random.seed` to set a specific seed for the random number generator. For example, `np.random.seed(0)` sets the seed as 0. By including this, we are actually making the random numbers reproducible, which may seem odd at first. Basically if we do the below code without that 0, we would get different random numbers every time we run it. By setting the seed to 0, we ensure we will get the same random numbers. There are lots of reasons we may want randomness to be reproducible.\n",
        "\n",
        "```\n",
        "np.random.seed(0)\n",
        "random_nums = np.random.uniform(0, 100, size = (5,))\n",
        "```\n",
        "\n",
        "Below, you will complete a function `generate_random_sample` that randomly generates `num_points` $x$ and $y$ coordinate values drawn from a uniform distribution, all within the range 0 to 1. You will then generate 10 points and visualize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0b3b718"
      },
      "source": [
        "def generate_random_sample(num_points):\n",
        "    \"\"\" Generate a random sample containing a desired number of points (num_points)\n",
        "    in the range [0, 1] using a random number generator object.\n",
        "\n",
        "     Args:\n",
        "       num_points (int): number of points desired in random sample\n",
        "\n",
        "     Returns:\n",
        "       dataX, dataY (ndarray, ndarray): arrays of size (num_points,) containing x\n",
        "       and y coordinates of sampled points\n",
        "\n",
        "     \"\"\"\n",
        "    ###################################################################\n",
        "    ## TODO for students: Draw the uniform numbers\n",
        "    ###################################################################\n",
        "    dataX = ...\n",
        "    dataY = ...\n",
        "\n",
        "    return dataX, dataY\n",
        "    ###################################################################\n",
        "\n",
        "# Set a seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Set number of points to draw\n",
        "num_points = 10\n",
        "\n",
        "# Draw random points\n",
        "dataX, dataY = generate_random_sample(num_points)\n",
        "\n",
        "# Visualize\n",
        "plot_random_sample(dataX, dataY, \"Random sample of 10 points\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc027602"
      },
      "source": [
        "### 1.2: Gaussian Distribution\n",
        "\n",
        "The most widely used continuous distribution is probably the Gaussian distribution. It is extremely common across all kinds of statistical analyses. Because of the central limit theorem, many quantities are Gaussian distributed. Gaussians also have some nice mathematical properties that permit simple closed-form solutions to several important problems.\n",
        "\n",
        "As a working example, imagine that a human participant is asked to point in the direction where they perceived a sound coming from. As an approximation, we can assume that the direction they point towards is Gaussian distributed, with a mean on target and variance depending on the error range.\n",
        "\n",
        "In this exercise, you will implement a Gaussian by filling in the missing portions of code for the function `my_gaussian` below. Gaussians have two parameters. The **mean** $\\mu$, which sets the location of its center, and its \"scale\" or spread is controlled by its **standard deviation** $\\sigma$, or **variance** $\\sigma^2$ (i.e. the square of standard deviation). **Be careful not to use one when the other is required.**\n",
        "\n",
        "The equation for a Gaussian probability density function is:\n",
        "$$\n",
        "f(x;\\mu,\\sigma^2)=\\mathcal{N}(\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right)\n",
        "$$\n",
        "In Python $\\pi$ and $e$ can be written as `np.pi` and `np.exp` respectively.\n",
        "\n",
        "As a probability distribution this has an integral of one when integrated from $-\\infty$ to $\\infty$. However, in the following, your numerical Gaussian will only be computed over a finite number of points. You therefore need to explicitly normalize it to sum to one yourself based on the `step_size` used.\n",
        "\n",
        "You can test your function with $\\mu= 1$ and $\\sigma = 1.5$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0e05afc"
      },
      "source": [
        "def my_gaussian(x_points, mu, sigma):\n",
        "    \"\"\" Returns normalized Gaussian estimated at points `x_points`, with\n",
        "    parameters: mean `mu` and standard deviation `sigma`\n",
        "\n",
        "    Args:\n",
        "      x_points (ndarray of floats): points at which the gaussian is evaluated\n",
        "      mu (scalar): mean of the Gaussian\n",
        "      sigma (scalar): standard deviation of the gaussian\n",
        "\n",
        "    Returns:\n",
        "      (numpy array of floats) : normalized Gaussian evaluated at `x`\n",
        "    \"\"\"\n",
        "    ###################################################################\n",
        "    ## TODO for students: Implement the gaussian equation\n",
        "    ###################################################################\n",
        "    px = ...\n",
        "    #Do not forget to normalize\n",
        "\n",
        "    return px\n",
        "#Choose some parameters\n",
        "mu = ...\n",
        "sigma = ...\n",
        "\n",
        "###################################################################\n",
        "#Create data\n",
        "step_size = 0.1\n",
        "x_points = np.arange(-5, 5, step_size)\n",
        "\n",
        "#Generate the Gaussian\n",
        "Gaussian = my_gaussian(x_points, mu, sigma)\n",
        "\n",
        "\n",
        "#Visualise\n",
        "my_plot_single(x_points, Gaussian)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coCXopw26tnj"
      },
      "source": [
        "*Example output:*\n",
        "\n",
        "\n",
        "<img alt='Solution hint' align='left' width=413 height=300 src=https://raw.githubusercontent.com/svandergoote/LGBIO2060-2021/master/Solutions/TP1_ex1.PNG>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1743733d"
      },
      "source": [
        "## Section 2: Statistical inference\n",
        "The goal of this section is to explain how it is possible to do inference by inverting the generative process.\n",
        "\n",
        "By completing the exercises in this tutorial, you should:\n",
        "* understand what the likelihood function is, and have some intuition of why it is important\n",
        "* know how to summarise the Gaussian distribution using mean and variance\n",
        "* know how to maximise a likelihood function\n",
        "* be able to do simple inference in both classical and Bayesian ways"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1b54b8f"
      },
      "source": [
        "### 2.1 Likelihoods\n",
        "Most of the times when you are trying to model something you have two things:\n",
        "\n",
        "* Observations/data **x**.\n",
        "* A probabilistic model $P(x|\\theta)$\n",
        "\n",
        "And your goal is to estimate the hidden properties $\\theta$ that gave rise to the data **x**.\n",
        "\n",
        "For example, if your probabilistic model is a Gaussian distribution $\\mathcal{N}(x_i,\\mu,\\sigma)$ your goal is to find the parameters $\\theta=\\{\\mu,\\sigma\\}$ that maximise the probability that your data **x** were obtained given those parameters.\n",
        "\n",
        "A classical method to achieve such result is the **maximum likelihood**, which consists to maximise the probability of model with regard to $\\theta$.\n",
        "\n",
        "$$\\hat\\theta = argmax \\, P(x|\\theta)$$\n",
        "\n",
        "This equation translates the fact the that you want to find the parameters $\\theta$ that maximize the probability that your data **x** were indeed obtained by your probabilistic model.\n",
        "\n",
        "In other words, if we do not know the parameters $\\mu$, $\\sigma$ that generated the data, we can try to **infer** which parameter values (given our model) gives the best (highest) likelihood. This is what we call statistical inference: trying to infer what parameters make our observed data the most likely or probable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f04d4f4e"
      },
      "source": [
        "### Coding Exercise 2.1: Computing likelihood\n",
        "\n",
        "Let's start with computing the likelihood of some set of data points being drawn from a Gaussian distribution with a mean and variance we choose.\n",
        "\n",
        "\n",
        "\n",
        "As multiplying small probabilities together can lead to very small numbers, it is often convenient to report the *logarithm* of the likelihood. This is just a convenient transformation and as logarithm is a monotonically increasing function this does not change what parameters maximise the function.\n",
        "\n",
        "Here you have to implement the log-likelihood in `compute_likelihood_normal`:\n",
        "$$Log \\, Likelihood = \\sum_{i=1}^n log\\left(P(x_i)\\right)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de42220a"
      },
      "source": [
        "def compute_likelihood_normal(x, mean_val, standard_dev_val):\n",
        "  \"\"\" Computes the log-likelihood values given a observed data sample x, and\n",
        "  potential mean and standard deviation values for a normal distribution\n",
        "\n",
        "    Args:\n",
        "      x (ndarray): 1-D array with all the observed data\n",
        "      mean_val (scalar): value of mean for which to compute likelihood\n",
        "      standard_dev_val (scalar): value of std for which to compute likelihood\n",
        "\n",
        "    Returns:\n",
        "      likelihood (scalar): value of likelihood for this combination of means/variances\n",
        "  \"\"\"\n",
        "\n",
        "  ###################################################################\n",
        "  ## TODO for student\n",
        "  ###################################################################\n",
        "\n",
        "  # Get probability of each data point\n",
        "  #HINT: check the stats library\n",
        "  p_data = ...\n",
        "\n",
        "  # Compute likelihood\n",
        "  likelihood = ...\n",
        "\n",
        "  return likelihood\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "# Compute likelihood for a guessed mean/standard dev\n",
        "guess_mean = 4\n",
        "guess_standard_dev = .1\n",
        "likelihood = compute_likelihood_normal(x, guess_mean, guess_standard_dev)\n",
        "print(likelihood)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78e3d1e5"
      },
      "source": [
        "You should get a likelihood of -92904.81. This is somewhat meaningless to us! For it to be useful, we need to compare it to the likelihoods computed using other guesses of the mean or standard deviation. The visualization below shows us the likelihood for various values of the mean and the standard deviation. Essentially, we are performing a rough grid-search over means and standard deviations.  What would you guess as the true mean and standard deviation based on this visualization?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "a1e3e75d"
      },
      "source": [
        "# @title Execute to visualize likelihoods\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "\n",
        "# Compute likelihood for different mean/variance values\n",
        "mean_vals = np.linspace(1, 10, 10) # potential mean values to ry\n",
        "standard_dev_vals = np.array([0.7, 0.8, 0.9, 1, 1.2, 1.5, 2, 3, 4, 5]) # potential variance values to try\n",
        "\n",
        "# Initialise likelihood collection array\n",
        "likelihood = np.zeros((mean_vals.shape[0], standard_dev_vals.shape[0]))\n",
        "\n",
        "# Compute the likelihood for observing the gvien data x assuming\n",
        "# each combination of mean and variance values\n",
        "for idxMean in range(mean_vals.shape[0]):\n",
        "  for idxVar in range(standard_dev_vals .shape[0]):\n",
        "    likelihood[idxVar,idxMean]= sum(np.log(norm.pdf(x, mean_vals[idxMean],\n",
        "                                              standard_dev_vals[idxVar])))\n",
        "\n",
        "# Uncomment once you've generated the samples and compute likelihoods\n",
        "xspace = np.linspace(0, 10, 100)\n",
        "plot_likelihoods(likelihood, mean_vals, standard_dev_vals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0769a1c8"
      },
      "source": [
        "### 2.2: Searching for best parameters\n",
        "\n",
        "We want to do inference on this data set, i.e. we want to infer the parameters that most likely gave rise to the data given our model. Intuitively that means that we want as good as possible a fit between the observed data and the probability distribution function with the best inferred parameters. We can search for the best parameters manually by trying out a bunch of possible values of the parameters, computing the likelihoods, and picking the parameters that resulted in the highest likelihood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37db783c"
      },
      "source": [
        "#### Interactive Demo 2.2: Maximum likelihood inference\n",
        "\n",
        "Try to see how well you can fit the probability distribution to the data by using the demo sliders to control the mean and standard deviation parameters of the distribution. We will visualize the histogram of data points (in blue) and the Gaussian density curve with that mean and standard deviation (in red). Below, we print the log-likelihood.\n",
        "\n",
        "- What (approximate) values of mu and sigma result in the best fit?\n",
        "- How does the value below the plot (the log-likelihood) change with the quality of fit?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "41b51288"
      },
      "source": [
        "# @title Make sure you execute this cell to enable the widget and fit by hand!\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "vals = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "def plotFnc(mu,sigma):\n",
        "  loglikelihood= sum(np.log(norm.pdf(vals,mu,sigma)))\n",
        "  #calculate histogram\n",
        "\n",
        "  #prepare to plot\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_xlabel('x')\n",
        "  ax.set_ylabel('probability')\n",
        "\n",
        "  #plot histogram\n",
        "  count, bins, ignored = plt.hist(vals,density=True)\n",
        "  x = np.linspace(0,10,100)\n",
        "\n",
        "  #plot pdf\n",
        "  plt.plot(x, norm.pdf(x,mu,sigma),'r-')\n",
        "  plt.show()\n",
        "  print(\"The log-likelihood for the selected parameters is: \" + str(loglikelihood))\n",
        "\n",
        "#interact(plotFnc, mu=5.0, sigma=2.1);\n",
        "#interact(plotFnc, mu=widgets.IntSlider(min=0.0, max=10.0, step=1, value=4.0),sigma=widgets.IntSlider(min=0.1, max=10.0, step=1, value=4.0));\n",
        "interact(plotFnc, mu=(0.0, 15.0, 0.1),sigma=(0.1, 5.0, 0.1));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUOfjg2mComQ"
      },
      "source": [
        "### 2.3: Analytical solution\n",
        "\n",
        "Sometimes, things work out well and we can come up with formulas for the maximum likelihood estimates of parameters. We won't get into this further but basically we could set the derivative of the likelihood to 0 (to find a maximum) and solve for the parameters. This won't always work but for the Gaussian distribution, it does.\n",
        "\n",
        "Specifically , the special thing about the Gaussian is that mean and standard deviation of the random sample can effectively approximate the two parameters of a Gaussian, $\\mu, \\sigma$.\n",
        "\n",
        "\n",
        "Hence using the  mean, $\\bar{x}=\\frac{1}{n}\\sum_i x_i$, and variance, $\\bar{\\sigma}^2=\\frac{1}{n} \\sum_i (x_i-\\bar{x})^2 $ of the sample should give us the best/maximum likelihood.\n",
        "\n",
        "Complete the missing parts in the following piece of code to observe what are the estimate you find with the analytical method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpsQMdjkC0bE"
      },
      "source": [
        "# Set random seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate data\n",
        "true_mean = 5\n",
        "true_standard_dev = 1\n",
        "n_samples = 1000\n",
        "x = np.random.normal(true_mean, true_standard_dev, size = (n_samples,))\n",
        "\n",
        "###################################################################\n",
        "  ## TODO for student\n",
        "###################################################################\n",
        "# Compute and print sample means and standard deviations\n",
        "sample_mean = ...\n",
        "sample_std = ...\n",
        "\n",
        "print(\"This is the sample mean as estimated by analytical method: \" + str(sample_mean))\n",
        "print(\"This is the sample standard deviation as estimated by analytical method: \" + str(sample_std))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "132241fd"
      },
      "source": [
        "### 2.4: Numerical optimization to find parameters\n",
        "Sometimes, it is not possible to find the optimal parameters analitycally and numerical methods must be used.\n",
        "\n",
        "\n",
        "Let's again assume that we have a data set, $\\mathbf{x}$, assumed to be generated by a normal distribution.\n",
        "We want to maximise the likelihood of the parameters $\\mu$ and $\\sigma^2$. We can do so using a couple of tricks:\n",
        "\n",
        "*   Using a log transform will not change the maximum of the function, but will allow us to work with very small numbers that could lead to problems with machine precision.\n",
        "*   Maximising a function is the same as minimising the negative of a function, allowing us to use the minimize optimisation provided by scipy.\n",
        "\n",
        "The optimisation could be done using `sp.optimize.minimize`, which does a version of gradient descent (there are hundreds of ways to do numerical optimisation, we will not cover these here!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c08a4a7"
      },
      "source": [
        "## Section 3: Bayesian Inference\n",
        "\n",
        "In this section, you will apply the Bayes theorem, seen last week during the course, to a simple exemple with binary variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ7G2IGx64V_"
      },
      "source": [
        "### 3.1 Putting things into practice\n",
        "\n",
        "The world is full of latent variables that you cannot observe directly. Then you need to estimate these variables from the prior knowledge you have about it and some noisy measurements.\n",
        "\n",
        "Bayesian inference is a method based on the Bayes theorem used to  combine prior knowledge and noisy measurements to infer an unknown state. The estimate obtained with the Bayes theorem is associated to some uncertainty (how confident we are that our estimate is true), and helps making a decision.\n",
        "\n",
        "We will first handle binary states. It means that X can be either A or B. Next week, we will extend this theorem to continuous states, i.e. X can take any value between $-\\infty \\to \\infty$.\n",
        "\n",
        "Today, you're going to the lake, hoping to catch some fishes. At the dock, you have to decide if you are going to fish on the left side or on the right side. To make this decision, you would want to know which side the school of fish is on to maximise your chances to catch a fish. As you cannot see directly where they are (the fishes's state is the latent variable that you're trying to estimate), you will have to base your decision on your previous knowledge (for example, you know that most of the time, fishes are on the left) and on a measurement (a fisherperson is on the right side of the deck and has caught a fish).  \n",
        "<Figure>\n",
        "  <img alt='Gone fishing' align='centered' width=413 height=300 src=https://raw.githubusercontent.com/svandergoote/LGBIO2060-2021/master/Solutions/Gone%20fishing.PNG>\n",
        "\n",
        "</Figure>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68uQc483pPNg"
      },
      "source": [
        "### 3.2: Theoretical reminder\n",
        "\n",
        "The bayes theorem is the fundamental element of bayesian inference. It states that:  \n",
        "\\begin{equation}\n",
        " P(s | m) = \\frac{P(m | s) P(s)}{P(m)}  \\tag{1} \\end{equation}\n",
        "\n",
        " where $s$ is the state and $m$ the measurement.\n",
        "\n",
        "$p(s|m)$ is called the **posterior** (updated knowledge). It represents your belief after gaining information with the measurement. For example it is the probability that the school of fish is on the right given that a fisherperson caught a fish on the left side of the dock.\n",
        "\n",
        "$p(m|s)$ is called the **likelihood**. It described of much information you gain from your measurement. For example, when fishing on the left it is the probability to catch a fish, given that the school is on the left.\n",
        "\n",
        "$p(s)$ is the **prior knowledge**. It describes what you know about the hidden state, before taking a measurement. For example, you know that there is 70% of chance that the fishes are on the left.\n",
        "\n",
        "$p(m)$ describes all possible measurements. It is a **normalization term**, also called the marginal likelihood or the evidence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQMftYvJYZuk"
      },
      "source": [
        "### 3.3 Likelihood\n",
        "\n",
        "Let's think a bit about the likelihood. Suppose that you observe a fisherperson **fishing at the left side of the dock**. Either he caught a fish or he didn't. Let's say that you have 50% chance of catching a fish if you are on the correct side of the dock, but only 10% if your are at the wrong side of the dock.\n",
        "\n",
        "1) Please figure out each of the following:\n",
        "- probability of catching a fish given that the school of fish is on the left side, $P(m = \\textrm{catch fish} | s = \\textrm{left} )$\n",
        "- probability of not catching a fish given that the school of fish is on the left side, $P(m = \\textrm{no fish} | s = \\textrm{left})$\n",
        "- probability of catching a fish given that the school of fish is on the right side, $P(m = \\textrm{catch  fish} | s = \\textrm{right})$\n",
        "- probability of not catching a fish given that the school of fish is on the right side, $P(m = \\textrm{no fish} | s = \\textrm{right})$\n",
        "\n",
        "Sum up these answers in a numpy array as described in the next cell.\n",
        "\n",
        "2) If the fisherperson catches a fish, which side would you guess the school is on?\n",
        "\n",
        "3) If the fisherperson does not catch a fish, which side would you guess the school is on?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uyJxsNO-j-5s"
      },
      "source": [
        "###likelihood (ndarray): i x j array with likelihood probabilities where i is number of state options, j is number of measurement options\n",
        "\n",
        "likelihood = ... #first row is s = left, second row is s = right; first column is fish, second is no fish"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQVd-xyF6FLu"
      },
      "source": [
        "### 3.4 Prior\n",
        "\n",
        "In the previous exercise, you tried to guess where the school of fish was based on the measurement you took (watching someone fish). You did this by choosing the state that maximized the probability of the measurement. In other words, you estimated the state by maximizing the likelihood (recall the MLE you've encounter in the first tutorial).\n",
        "\n",
        "But, what if you had been going to this dock for years and you knew that the fish were almost always on the right side? This should probably affect how you make your estimate -- you would rely less on the single new measurement and more on your prior knowledge. This is the fundamental idea behind Bayesian inference.\n",
        "\n",
        "By running the next cell, we decide that $P(s = \\textrm{left} )$ and $P(s = \\textrm{right} )$ are respectively 0.3 and 0.7.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYMGA1mJ_Aux"
      },
      "source": [
        "#Run this cell\n",
        "# p(s = left) = 0.3; p(s = right) = 0.7\n",
        "\n",
        "prior = np.array([0.3, 0.7]).reshape((2, 1)) # first row is s = left, second row is s = right"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMjGPIMC6HPV"
      },
      "source": [
        "### 3.5 Normalization (marginal likelihood)\n",
        "\n",
        "When we normalize to find the posterior, we need to determine the marginal likelihood - or evidence - for the measurement we observed. As we do not have direct access to the normalization term $p(m)$, we need to compute it through the marginalization process. As stated in Equation 2-4, you can either use joint probabilities or conditional probabilities\n",
        "\n",
        "$$p(m) = \\sum_s p(m, s) \\tag{2} $$\n",
        "\n",
        "\n",
        "$$ p(m,s) = p(m|s)p(s) \\tag{3} $$\n",
        "\n",
        "\\\\\n",
        "\n",
        "$$p(m) = \\sum_s p(m | s) p(s) \\tag{4} $$\n",
        "\n",
        "\n",
        "Please complete the following math problem to further practice thinking through probabilities:\n",
        "\n",
        "1. Calculate the marginal likelihood of the fish being caught, $P(m = \\textrm{catch fish})$, considering the likelihoods and prior defined sooner (you are still observing a person fishing on the **left** side of the dock).\n",
        "\n",
        "\n",
        "Normalizing by this $p(m)$ means that our posterior is a complete probability distribution that sums or integrates to 1 appropriately.\n",
        "For many complicated cases, like those we might be using to model behavioral or brain inferences, the normalization term can be intractable or extremely complex to calculate. We can be careful to choose probability distributions were we can analytically calculate the posterior probability or numerical approximation is reliable. Better yet, we sometimes don't need to bother with this normalization! The normalization term, $p(m)$, is the probability of the measurement. This does not depend on state so is essentially a constant we can often ignore. We can compare the unnormalized posterior distribution values for different states because how they relate to each other is unchanged when divided by the same constant."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcZ7yyIy6I6G"
      },
      "source": [
        "### 3.6 Posterior\n",
        "\n",
        "You are now ready to put everything together !\n",
        "Our prior is $p(s = \\textrm{left}) = 0.3$ and $p(s = \\textrm{right}) = 0.7$. We also learned that the chance of catching a fish given they fish on the same side as the school was 50%. Otherwise, it was 10%. We observe a person fishing on the left side.\n",
        "\n",
        "First, calculate on paper the posterior probability that... (don't forget to normalize)\n",
        "\n",
        "1. The school is on the left if the fisherperson catches a fish: $p(s = \\textrm{left} | m = \\textrm{fish})$\n",
        "2. The school is on the right if the fisherperson does not catch a fish: $p(s = \\textrm{right} | m = \\textrm{no fish})$\n",
        "\n",
        "Let's implement our above math to be able to compute posteriors for different priors and likelihoods. You are asked to fill a function taking the prior and likelihoods as argument and return the posteriors.  We want our full posterior to take the same 2 by 2 form. Make sure the outputs match your math answers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "KyaGn0QV26yz"
      },
      "source": [
        "def compute_posterior(likelihood, prior):\n",
        "  \"\"\" Use Bayes' Rule to compute posterior from likelihood and prior\n",
        "\n",
        "  Args:\n",
        "    likelihood (ndarray): i x j array with likelihood probabilities where i is\n",
        "                    number of state options, j is number of measurement options\n",
        "    prior (ndarray): i x 1 array with prior probability of each state\n",
        "\n",
        "  Returns:\n",
        "    ndarray: i x j array with posterior probabilities where i is\n",
        "            number of state options, j is number of measurement options\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  ############################\n",
        "  ###### YOUR CODE HERE ######\n",
        "  ############################\n",
        "\n",
        "  return posterior\n",
        "\n",
        "# prior and likelihood defined above\n",
        "\n",
        "# Compute posterior\n",
        "posterior = compute_posterior(likelihood, prior)\n",
        "\n",
        "# Visualize\n",
        "plot_prior_likelihood_posterior(prior, likelihood, posterior)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "CXA6qs9n26yz"
      },
      "source": [
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=1665.0 height=674.0 src=https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/tutorials/W3D1_BayesianDecisions/static/W3D1_Tutorial1_Solution_1a2cc907_0.png>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "jc1G9VFp26yz"
      },
      "source": [
        "#### Interactive Demo: What affects the posterior?\n",
        "\n",
        "Now that we can understand the implementation of *Bayes rule*, let's vary the parameters of the prior and likelihood to see how changing the prior and likelihood affect the posterior.\n",
        "\n",
        "In the demo below, you can change the prior by playing with the slider for $p( s = left)$. You can also change the likelihood by changing the probability of catching a fish given that the school is on the left and the probability of catching a fish given that the school is on the right. The fisherperson you are observing is fishing on the left.\n",
        "\n",
        "\n",
        "1.   Keeping the likelihood constant, when does the prior have the strongest influence over the posterior? Meaning, when does the posterior look most like the prior no matter whether a fish was caught or not?\n",
        "2.   What happens if the likelihoods for catching a fish are similar when you fish on the correct or incorrect side?\n",
        "3.  Set the prior probability of the state = left to 0.6 and play with the likelihood. When does the likelihood exert the most influence over the posterior?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "5J2x9xwh26y0",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "ps_widget = widgets.FloatSlider(0.3, description='p(s = left)',\n",
        "                                min=0.01, max=0.99, step=0.01)\n",
        "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish on left | state = left)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style = style, layout=Layout(width='370px'))\n",
        "p_a_s0_widget = widgets.FloatSlider(0.1, description='p(fish on left | state = right)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style = style, layout=Layout(width='370px'))\n",
        "# observed_widget = widgets.Checkbox(value=False, description='Observed fish (m)',\n",
        "#                                  disabled=False, indent=False,\n",
        "#                                  layout=Layout(display=\"flex\", justify_content=\"center\"))\n",
        "\n",
        "observed_widget = ToggleButtons(options=['Fish', 'No Fish'],\n",
        "    description='Observation (m) on the left:', disabled=False, button_style='',\n",
        "    layout=Layout(width='auto', display=\"flex\"),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "widget_ui = VBox([ps_widget,\n",
        "                  HBox([p_a_s1_widget, p_a_s0_widget]),\n",
        "                  observed_widget])\n",
        "widget_out = interactive_output(plot_prior_likelihood,\n",
        "                                {'ps': ps_widget,\n",
        "                                'p_a_s1': p_a_s1_widget,\n",
        "                                'p_a_s0': p_a_s0_widget,\n",
        "                                'measurement': observed_widget})\n",
        "display(widget_ui, widget_out)\n",
        "\n",
        "# @widgets.interact(\n",
        "#     ps=ps_widget,\n",
        "#     p_a_s1=p_a_s1_widget,\n",
        "#     p_a_s0=p_a_s0_widget,\n",
        "#     m_right=observed_widget\n",
        "# )\n",
        "# def make_prior_likelihood_plot(ps,p_a_s1,p_a_s0,m_right):\n",
        "#     fig = plot_prior_likelihood(ps,p_a_s1,p_a_s0,m_right)\n",
        "#     plt.show(fig)\n",
        "#     plt.close(fig)\n",
        "#     return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhokZwlou1lh"
      },
      "source": [
        "We now can use this new, complete probability distribution for any future inference or decisions we like! In fact, as we will see in the second part of the tutorial, we can use it as a new prior! Finally, we often call this probability distribution our beliefs over the hidden states, to emphasize that it is our subjective knowlege about the hidden state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "LIgENTQo26yj"
      },
      "source": [
        "### 3.7 Utility and decision\n",
        "Now that you have some belief about the location of the fishes, you need to decide where to fish. It may seem obvious - you could just fish on the side where the probability of the fish being is higher. Unfortunately, decisions and actions are always a little more complicated. Deciding where to fish may be influenced by more than just the probability of the school of fish being there. In our example, it is more difficult to fish on the right side because there is a motor boat at this side of the lake. Additionally, fishing on the wrong side, where there aren‚Äôt many fish, is likely to lead to you spending your afternoon not catching fish and therefore getting a sunburn. The consequences of the action you take is based on the true (but hidden) state of the world **and** the action you choose !\n",
        "\n",
        "We quantify gains and losses numerically using a **utility** function $U(s,a)$, which describes the consequences of your actions: how much value you gain (or if negative, lose) given the state of the world ($s$) and the action you take ($a$). In our example, our utility can be summarized as:\n",
        "\n",
        "|  U(s,a)   | a = left   | a = right  |\n",
        "| ----------------- |------------|------------|\n",
        "| s = Left          | +2         | -3         |\n",
        "| s = right         | -2         | +1         |\n",
        "\n",
        "\n",
        "If you want to know what to expect from taking the action of fishing on one side or the other, you need to calculate the expected utility by weighing the utilities with the probability of that state occuring. This allows us to choose actions by taking probabilities of events into account: we don't care if the outcome of an action-state pair is a loss if the probability of that state is very low. We can formalize this as:\n",
        "\n",
        "$$ \\text{Expected utility of action a} = \\sum_{s}U(s,a)P(s) $$\n",
        "\n",
        "Implement a function `expected_utility(utility, posterior)` to compute the expected utility associated with an action, given the probability of a state. Then, for each of these scenarios, determine which action would have been correct.\n",
        "\n",
        "1.  You just arrived at the dock for the first time and have no sense of where the fish might be. Which side would you choose to fish on given our utility values?\n",
        "2.  You think that the probability of the school being on the left side is very low (0.1) and correspondingly high on the right side (0.9). Which side would you choose to fish on given our utility values?\n",
        "3. What probability distribution of the states would give you equal expected utility ?\n",
        "\n",
        "Note that we can use our posterior probability as the probability of the state $P(s)$.\n",
        "\n",
        "As you have seen in the previous interactive demo, once you determined the measurement $m$, your posterior probabilities is no longer a 2x2 array. It becomes a 2x1 vector that can be called $P(s)$ as it does no longer depends on a variable $m$. Keep in mind that $P(s)$ is the probability of the state $s$ and can be therefore a prior probability (before taking any measurement) or a posterior probability (probability that integrate the knowledge gained with 1 or several measurement(s))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyUAz9HbvWg5"
      },
      "source": [
        "def expected_utility(utility, state_proba):\n",
        "  \"\"\" Compute expected utility from utility and probability distribution of the state\n",
        "\n",
        "  Args:\n",
        "    utility (ndarray): i x j array with utility where i is\n",
        "                    number of state options, j is number of action options\n",
        "    state_proba (ndarray): i x 1 array with probability of each state\n",
        "\n",
        "  Returns:\n",
        "    expected_utility (ndarray): 1 x i array with the expected utility where i is\n",
        "            number of action options\n",
        "\n",
        "  \"\"\"\n",
        "  ############################\n",
        "  ###### YOUR CODE HERE ######\n",
        "  ############################\n",
        "  ...\n",
        "  return ...\n",
        "\n",
        "utility = ...\n",
        "state_proba = ...\n",
        "\n",
        "print(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "execution": {},
        "id": "BZxBj-Uh26y1"
      },
      "source": [
        "#### Interactive Demo: What is more important, the probabilities or the utilities?\n",
        "\n",
        "Play a bit with the following widget to gain some intuitions on how each element that goes into a Bayesian decision comes together. Remember, the common assumption in neuroscience, psychology, economics, ecology, etc. is that we (humans and animals) are tying to maximize our expected utility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "CWPZBH4u26y1",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to enable the widget\n",
        "# style = {'description_width': 'initial'}\n",
        "\n",
        "ps_widget = widgets.FloatSlider(0.3, description='p(s = left)',\n",
        "                                min=0.01, max=0.99, step=0.01, layout=Layout(width='300px'))\n",
        "p_a_s1_widget = widgets.FloatSlider(0.5, description='p(fish on left | state = left)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "p_a_s0_widget = widgets.FloatSlider(0.1, description='p(fish on left | state = right)',\n",
        "                                    min=0.01, max=0.99, step=0.01, style=style, layout=Layout(width='370px'))\n",
        "\n",
        "observed_widget = ToggleButtons(options=['Fish', 'No Fish'],\n",
        "    description='Observation (m) on the left:', disabled=False, button_style='',\n",
        "    layout=Layout(width='auto', display=\"flex\"),\n",
        "    style={'description_width': 'initial'}\n",
        ")\n",
        "\n",
        "widget_ui = VBox([ps_widget,\n",
        "                  HBox([p_a_s1_widget, p_a_s0_widget]),\n",
        "                  observed_widget])\n",
        "\n",
        "widget_out = interactive_output(plot_prior_likelihood_utility,\n",
        "                                {'ps': ps_widget,\n",
        "                                'p_a_s1': p_a_s1_widget,\n",
        "                                'p_a_s0': p_a_s0_widget,\n",
        "                                'measurement': observed_widget})\n",
        "display(widget_ui, widget_out)\n",
        "\n",
        "# @widgets.interact(\n",
        "#     ps=ps_widget,\n",
        "#     p_a_s1=p_a_s1_widget,\n",
        "#     p_a_s0=p_a_s0_widget,\n",
        "#     m_right=observed_widget\n",
        "# )\n",
        "# def make_prior_likelihood_utility_plot(ps, p_a_s1, p_a_s0,m_right):\n",
        "#     fig = plot_prior_likelihood_utility(ps, p_a_s1, p_a_s0,m_right)\n",
        "#     plt.show(fig)\n",
        "#     plt.close(fig)\n",
        "#     return None"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}