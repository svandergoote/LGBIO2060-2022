{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBIO2060_TP3.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2021/blob/Tp3/LGBIO2060_TP3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p50qGxfb42PA"
      },
      "source": [
        "# LGBIO2060 Exercice session 3\n",
        "\n",
        "#Bayesian inference of a continuous hidden state \n",
        "\n",
        "__Authors:__ Simon Vandergooten, Clémence Vandamme, Florence Blondiaux & Antoine De Comité\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzTP399FOxlc"
      },
      "source": [
        "##Introduction and context\n",
        "\n",
        "In this tutorial, we will extend the Bayes' theorem introduced last week to continuous hidden state. \n",
        "\n",
        "In the first part, we will describe how to compute a posterior from continuous distributions. Then, we will apply this to a real life example. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PtqBzPnDwvix"
      },
      "source": [
        "# @title Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TG3O_IP2vp_"
      },
      "source": [
        "##Mathematical formalism\n",
        "\n",
        "Along this tutorial, we will again be using Bayes' theorem. As a reminder, this theorem combines two sources of information into an optimal one. It combines the *likelihood*, which is a distribution that quantifies how likely the measurement is, given the real latent state; and the *prior* which quantifies the knowledge we have about the system, thanks to our prior experience. Bayes' rule defines the *posterior* distribution in the following manner: \n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\text{Posterior} = \\dfrac{ \\text{Likelihood} \\times \\text{Prior}}{ \\text{Normalization constant}} \\tag{1}\n",
        "\\end{eqnarray}\n",
        "\n",
        "We consider now continuous distributions. When both the prior and likelihood are gaussian distributions, the posterior will be a gaussian distribution as well.\n",
        "\n",
        "In fact, the mean of the posterior distribution is simply a weighted average of the means of the prior and of the likelihood. The weight is determined by the confidence associated with each source of information. \n",
        "If one source of information is very noisy (of high variance), we will not have high confidence in it and grant it less weight. \n",
        "\n",
        "$$\n",
        "\\mu_{3} = a\\mu_{1} + (1-a)\\mu_{2} \\tag{2}\n",
        "$$\n",
        "$$\n",
        "a = \\frac{\\sigma_{1}^{-2}}{\\sigma_{1}^{-2} + \\sigma_{2}^{-2}} \\tag{3}\n",
        "$$\n",
        "\n",
        "Since we are combining two sources of information, we are **gaining** information. The amount of information carried by a gaussian is the inverse of the variance. Therefore, the information of the posterior is equal to the sum of the information of prior and the likelihood. Then, we have: \n",
        "\n",
        "$$\\sigma_{3}^{-2} = \\sigma_{1}^{-2} + \\sigma_{2}^{-2} \\tag{4}$$\n",
        "\n",
        "Therefore: \n",
        "\n",
        "$$\n",
        "\\begin{array}{rcl}\n",
        "\\text{Posterior} &\\propto& \\mathcal{N}(\\mu_{\\text{likelihood}},\\sigma_{\\text{likelihood}}^2) \\times \\mathcal{N}(\\mu_{\\text{prior}},\\sigma_{\\text{prior}}^2) \\\\\n",
        "&= & \\mathcal{N}\\left( \\dfrac{\\sigma^2_{\\text{likelihood}}\\mu_{\\text{prior}}+\\sigma^2_{\\text{prior}}\\mu_{\\text{likelihood}}}{\\sigma^2_{\\text{likelihood}}+\\sigma^2_{\\text{prior}}}, \\dfrac{\\sigma^2_{\\text{likelihood}}\\sigma^2_{\\text{prior}}}{\\sigma^2_{\\text{likelihood}}+\\sigma^2_{\\text{prior}}} \\right) \n",
        "\\end{array} \\tag{5}\n",
        "$$\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK-Y21Q12vqC"
      },
      "source": [
        "### Exercise 1A: Finding the posterior computationally\n",
        "\n",
        "Implement the Bayes' theorem to compute the posterior probability distribution as a function of the prior and the likelihood\n",
        "\n",
        "Hints: \n",
        "\n",
        "* You can use the function `my_gaussian(x, mean, scale)` that you implementend in tutorial 1 to generate likelihood ($\\mu$=3, $\\sigma$=1.5) and prior ($\\mu=-1$ and $\\sigma$ =1.5). This function is provided for you in this notebook.\n",
        "* Compute the posterior **using pointwise multiplication** of the likelihood and prior. Don't forget to normalize so the posterior adds up to 1.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {},
        "id": "iy6zLMkyMNgv",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to enable the function `my_gaussian`\n",
        "def my_gaussian(x, μ, σ):\n",
        "    return np.exp(-((x - μ) / σ)**2 / 2) / np.sqrt(2 * np.pi * σ**2)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEihJYRS2vqT"
      },
      "source": [
        "x_vector = np.arange(-10,10,0.1)\n",
        "\n",
        "def compute_posterior_pointwise(prior, likelihood):\n",
        "    '''\n",
        "    Author: Florence Blondiaux\n",
        "    Returns the normalized posterior probability based on the prior and the likelihood\n",
        "    Prior: The prior probabilities\n",
        "    Likelihood: The likelihood probabilities\n",
        "    '''\n",
        "    ##########################\n",
        "    ##### Your code here #####\n",
        "    ##########################\n",
        "\n",
        "\n",
        "# Run the lines below to test your code\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x_vector,likelihood,'r',Linewidth=2.5,label='likelihood')\n",
        "ax.plot(x_vector,prior,'g',Linewidth=2.5,label='prior')\n",
        "ax.plot(x_vector,posterior,'b',Linewidth=2.5,label='posterior')\n",
        "ax.set_xlabel('x-position')\n",
        "ax.set_ylabel('probability')\n",
        "ax.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLyG1j6f_KU1"
      },
      "source": [
        "\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=413 height=300 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-Ex1.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHXZhfgFoDDN"
      },
      "source": [
        "###Relation between prior, likelihood and posterior\n",
        "**Execute the widget below to investigate the impact of prior and likelihood on the posterior distribution**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslhpIQu2vqb",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "x = np.arange(-15, 15, 0.1)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def refresh(mu_likelihood=3, sigma_likelihood=1.5, mu_prior=-1, sigma_prior=1.5):\n",
        "    likelihood = my_gaussian(x, mu_likelihood, sigma_likelihood)\n",
        "    prior = my_gaussian(x, mu_prior, sigma_prior)\n",
        "    posterior = compute_posterior_pointwise(prior,likelihood)\n",
        "    plt.plot(x, likelihood, 'r')\n",
        "    plt.plot(x,prior,'g')\n",
        "    plt.plot(x, posterior, 'b')\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "_ = widgets.interact(refresh,\n",
        "    mu_posterior=widgets.FloatSlider(value=3, min=-10, max=10, step=0.5, description=\"mu_likelihood:\", style=style),\n",
        "    sigma_posterior=widgets.FloatSlider(value=1.5, min=0.5, max=10, step=0.5, description=\"sigma_likelihood:\", style=style),\n",
        "    mu_prior=widgets.FloatSlider(value=-1, min=-10, max=10, step=0.5, description=\"mu_prior:\", style=style),\n",
        "    sigma_prior=widgets.FloatSlider(value=1.5, min=0.5, max=10, step=0.5, description=\"sigma_prior:\", style=style)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89wCtKoA2vqo"
      },
      "source": [
        "### Exercise 1B: Finding the posterior analytically\n",
        "In the previous exercise, you computed the posterior distribution regardless its parameters, using multiplication of the distribution. Instead, we can directly compute the  parameters of that Gaussian from the means and variances of the prior and likelihood with Equation 5.  \n",
        "\n",
        "In this exercise, we ask you to verify the expression of $\\mu_{\\text{posterior}}$.  To do so, we will keep our likelihood constant as an $\\mathcal{N}(3, 1.5)$ distribution, while considering priors with different means ranging from $\\mu=-10$ to $\\mu=10$.\n",
        "\n",
        "For each prior,\n",
        "\n",
        "* Compute the posterior distribution using the function you wrote in Exercise 1A. Next, find its mean. The mean of a probability distribution is $\\int_x p(x) dx$ or $\\sum_x x\\cdot p(x)$. \n",
        "* Compute the analytical posterior mean from likelihood and prior using the equation above.\n",
        "* Plot both estimates of the mean. \n",
        "\n",
        "Are the estimates of the posterior mean the same in both cases? \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzn6FYAC2vqr"
      },
      "source": [
        "def compare_computational_analytical_means():\n",
        "\n",
        "    ##########################\n",
        "    ##### Your code here #####\n",
        "    ##########################\n",
        "    \n",
        "    return mu_priors, mus_analytical, mus_by_integration\n",
        "\n",
        "mu_visuals, mu_analytical, mu_computational = compare_computational_analytical_means()\n",
        "\n",
        "\n",
        "#Run the lines below to test your code \n",
        "plt.plot(mu_visuals,mu_analytical,'r', label= 'Analytical')\n",
        "plt.plot(mu_visuals,mu_computational,'b--', label= 'Computational')\n",
        "plt.legend()\n",
        "plt.xlabel('Mean of the prior distribution')\n",
        "plt.ylabel('Mean of the posterior distribution')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RrN1DJQkkra"
      },
      "source": [
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=416 height=300 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-Ex2.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOy4d5RW2vq3"
      },
      "source": [
        "## Application of Bayes' theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZaeMwX2vq4"
      },
      "source": [
        "Consider you are playing the final of a very important tennis tournament against a very famous tennis player, Yannick Noah. You want to perfectly estimate where the ball he just hit will land. To do this you have two sources of information: \n",
        "\n",
        "  1. The visual information you gathered as he hit the ball (the angle of his wirst, shoulder, the angle of attack,...)\n",
        "  2. The information you gathered before the match about his game (his game stats, his current season,...)\n",
        "\n",
        "In this exercise, we will: \n",
        "  1. Generate the likelihood for multiple possible stimulus inputs\n",
        "  2. Generate a Gaussian prior for multiple possible stimulus inputs\n",
        "  3. Estimate our posterior as a function of the stimulus input\n",
        "  4. Estimate a participant response given the posterior\n",
        "  \n",
        "Typically, in an experimental setup, we know the participant response and the stimulus. We use then these information to recover the model used by the brain. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUVeMzBW2vq4"
      },
      "source": [
        "\n",
        "\n",
        "![Generative model](https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-GenerativeModel.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgb5TmZ82vq5"
      },
      "source": [
        "\n",
        "### 1 -  Likelihood array\n",
        "  \n",
        "The first step is to create the likelihood distribution from the encoded stimulus. For the sake of visualization, we will create a likelihood $f(x) = p(\\tilde x|x)$ for each potential encoded stimulus $\\tilde x$. We will then be able to visualize the likelihood as a function of hypothesized true stimulus positions: $x$ on the x-axis and encoded position $\\tilde x$ on the y-axis.\n",
        "\n",
        "**Exercise 2.A**\n",
        "We assume that the encoding of the position is on average equal to the true position, with some gaussian noise. Then, using the equation for the `my_gaussian` and the values in `hypothetical_stim`:\n",
        "  \n",
        "* Create a Gaussian likelihood with mean varying from `hypothetical_stim`, keeping $\\sigma_{\\text{likelihood}}$ constant at 1.\n",
        "* Each likelihood will have a different mean and thus a different row-likelihood of your 2D array, such that you end up with a likelihood array made up of 1,000 row-Gaussians with different means. \n",
        "\n",
        "In other words, each entry of the output array is the probability  the probability that the brain encodes the position $\\tilde x_i$ given the true position $x_j$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sdk60vTN2vq6"
      },
      "source": [
        "x = np.arange(-10, 10, 0.1)\n",
        "hypothetical_stim = np.linspace(-8, 8, 1000)\n",
        "\n",
        "def compute_likelihood_array(x_points, stim_array, sigma=1.):\n",
        "    \"\"\"\n",
        "    This function computes the likelihood array of a given stimulus\n",
        "    \n",
        "    Inputs : x_points (numpy array) is the set of points on which one wants to evaluate the likelihood\n",
        "             stim_array (numpy array) contains the mean position of every possible stimulus\n",
        "             sigma (float) is the standard deviation of the gaussian distribution\n",
        "             \n",
        "    Outputs : likelihood array (numpy array) is a len(stim_array) x len(x_points) that contains the evaluation of \n",
        "              the likelihood for every possible stimulus on the the set x_points. Each line of this array corresponds\n",
        "              to a single simulus (stim_array[i]).\n",
        "    \"\"\"\n",
        "    #########################\n",
        "    ## your code goes here ##\n",
        "    #########################\n",
        "    return likelihood_array \n",
        "\n",
        "\n",
        "\n",
        "# Run the lines below to test your code\n",
        "likelihood_array = compute_likelihood_array(x, hypothetical_stim)\n",
        "\n",
        "# Plot the results\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "colormap = ax.imshow(likelihood_array, extent=[-10, 10, 8, -8])\n",
        "cbar = plt.colorbar(colormap, ax=ax)\n",
        "cbar.set_label('probability')\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel(\"$x$ : Potential true stimulus $x$\")\n",
        "ax.set_title(\"Likelihood as a function of $\\~x$ : $p(\\~x | x)$\")\n",
        "ax.set_ylabel(\"Possible brain encoding $\\~x$\")\n",
        "ax.set_aspect('auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZnk0i9PktQZ"
      },
      "source": [
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-Likelihood.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRtb0rIu2vrB"
      },
      "source": [
        "**Question : How does the likelihood distribution for a given stimulus encoding looks like?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Pct7652vrD"
      },
      "source": [
        "\n",
        "### 2 - Mixture-of-Gaussians Prior\n",
        "\n",
        "The prior distribution can sometimes be more complex than a single gaussian distribution. In this exercise, we will implement a prior distribution which is a mixture of gaussian distributions. To go back to our tennis example, let us assume that your opponent sends the ball in the middle of the court 50% of the time and in the corner of the court 50% of the time. A mixture of Gaussian will combine these two information to obtain a prior distribution that integrate them both. \n",
        "We will control how the Gaussians are mixed by summing them together with a *mixing* or *weight* parameter $p_{centre}$, set to a value between 0 and 1, such that:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\text{Mixture} = p_{\\text{centre}} \\times \\mathcal{N}\\left(\\mu_{\\text{centre}},\\sigma_{\\text{centre}}\\right) + \\left(1-p_{\\text{centre}}\\right) \\times \\mathcal{N}\\left(\\mu_{\\text{corner}},\\sigma_{\\text{corner}}\\right)\n",
        "\\end{eqnarray}\n",
        "\n",
        "where $p_{centre}$ denotes the probability that the ball lands at the centre of the court.\n",
        "\n",
        "\n",
        "For visualization reasons, we will create a prior array that has the same dimension as the likelihood array we created in the previous exercise. Since the prior does not change as a function of $\\tilde x$ it will be identical for each row of the prior 2D array. \n",
        "\n",
        "Using the equation for the Gaussian `my_gaussian`:\n",
        "* Generate a Gaussian *center* with mean 0 and standard deviation 0.5. \n",
        "* Generate another Gaussian *corner* with mean 3 and standard deviation 1\n",
        "* Combine the two Gaussians (*center* + *corner*) to make a new prior by mixing the two Gaussians with mixing parameter $p_{corner}$ = 0.50. \n",
        "* This will be the first row of your prior 2D array\n",
        "* Now repeat this for varying brain encodings $\\tilde x$. Since the prior does not depend on $\\tilde x$ you can just repeat the prior for each $\\tilde x$ (hint: use np.tile) that row prior to make an array of 1,000 (i.e. `hypothetical_stim.shape[0]`)  row-priors.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7BdXql52vrE"
      },
      "source": [
        "x = np.arange(-10, 10, 0.1)\n",
        "\n",
        "def calculate_prior_array(x_points, stim_array, p_center,\n",
        "                          prior_mean_center=.0, prior_sigma_center=.5,\n",
        "                          prior_mean_corner=3, prior_sigma_corner=1):\n",
        "    \"\"\"\n",
        "    This function computes the prior distribution based on the mixture of gaussians\n",
        "    \n",
        "    Inputs : x_points (numpy array) is the set of points on which we want to evaluate the prior\n",
        "             stim_array (numpy array) is the array of stimulus input\n",
        "             p_center (float) is the probability that the ball lands in center of the court\n",
        "             mean_center (float) mean value of the center ball landing site\n",
        "             mean_corner (float) mean value of the corner ball landing site \n",
        "             sigma_center (float) std deviation of the center ball landing site\n",
        "             sigma_corner (float) std deviation of the corner ball landing site \n",
        "             \n",
        "    Outputs : prior array (numpy array) contains the prior in a len(stim_array) x len(x_points) array\n",
        "        'indep' stands for independent\n",
        "    \"\"\"\n",
        "\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n",
        "   \n",
        "    return prior_array\n",
        "\n",
        "\n",
        "# Run the lines below to test your code\n",
        "\n",
        "\n",
        "\n",
        "p_center = .5\n",
        "hypothetical_stim = np.linspace(-8, 8, 1000)\n",
        "prior_array = calculate_prior_array(x, hypothetical_stim, p_center)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "colormap = ax.imshow(prior_array, extent=[-10, 10, 8, -8])\n",
        "cbar = plt.colorbar(colormap, ax=ax)\n",
        "cbar.set_label('probability')\n",
        "ax.invert_yaxis()\n",
        "ax.set_xlabel(\"$x$ : Potential true stimulus $x$\")\n",
        "ax.set_title(\"Prior as a function of $\\~x$ : $p(\\~x|x)$\")\n",
        "ax.set_ylabel(\"Possible brain encoding $\\~x$\")\n",
        "ax.set_aspect('auto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3h4Me3aKUIE"
      },
      "source": [
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-Prior.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsnJSNVa2vrO"
      },
      "source": [
        "---\n",
        "### 3 - Bayes Theorem with Complex Posteriors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5CYsWyMUFzF"
      },
      "source": [
        "We now want to compute the posterior distribution using Bayes' theorem. Since we have already created a likelihood and a prior distributions for each brain encoded position $\\tilde x$, all we need to do is to multiply them row-wise. That is, each row of the posterior array will be the posterior distribution resulting from the multiplication of the prior and likelihood distributions of the same equivalent row.\n",
        "\n",
        "Mathematically:\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "    Posterior\\left[i, :\\right] \\propto Likelihood\\left[i, :\\right] \\odot Prior\\left[i, :\\right]\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "where $\\odot$ represents the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e., elementwise multiplication) of the corresponding prior and likelihood row vectors `i` from each matrix.\n",
        "\n",
        "Follow these steps to build the posterior as a function of the brain encoded stimulus $\\tilde x$:\n",
        "* For each row of the prior and likelihood (i.e. each possible brain encoding $\\tilde x$), fill in the posterior matrix so that every row of the posterior array represents the posterior density for a different brain encode  $\\tilde x$.\n",
        "\n",
        "**Try to implement a vectorial operation to compute the posterior**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRneAULr2vrS"
      },
      "source": [
        "def calculate_posterior_array(prior_array, likelihood_array):\n",
        "    \"\"\"\n",
        "    This function computes the posterior distribution from the prior & the likelihood\n",
        "    \n",
        "    Inputs : prior_array (numpy array) is the prior distribution\n",
        "             likelihood_array (numpy array) is the likelihood distribution\n",
        "             For both these arrays, each line correspond to a different input stimulus\n",
        "    Outputs : posterior_array (numpy array) that contains the posterior distribution for the different\n",
        "              input stimulus (each line corresponds to a different input)\n",
        "    \"\"\"\n",
        "\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n",
        "\n",
        "    return posterior_array\n",
        "    \n",
        "#Run the lines below to test your code \n",
        "posterior_array = calculate_posterior_array(prior_array, likelihood_array)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "colormap = ax.imshow(posterior_array, extent=[-10, 10, 8, -8])\n",
        "cbar = plt.colorbar(colormap, ax=ax)\n",
        "cbar.set_label('probability')\n",
        "ax.invert_yaxis()\n",
        "ax.set_ylabel('Brain encoded Stimulus $\\~x$')\n",
        "ax.set_title('Posterior as a fcn of $\\~x$ : $p(x | \\~x)$')\n",
        "ax.set_xlabel('Hypothesized Position $x$')\n",
        "ax.set_aspect('auto')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8IHJ-8XKYtB"
      },
      "source": [
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-Posterior.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8PirivP2vrY"
      },
      "source": [
        "---\n",
        "### 4 - Estimating the position $\\hat x$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzw-1nq02vrZ"
      },
      "source": [
        "Now that we have a posterior distribution (for each possible brain encoding $\\tilde x$) that represents the brain's estimated ball position: $p(x|\\tilde x)$, we want to make an estimate (response) of the ball location $\\hat x$ using the posterior distribution. This would represent the subject's estimate if their (for us as experimentalist unobservable) brain encoding took on each possible value. \n",
        "\n",
        "This effectively encodes the *decision* that a participant would make for a given brain encoding $\\tilde x$. In this exercise, we make the assumption that participants take the mean of the posterior (decision rule) as a response estimate for the ball location (use the function `moments_myfunc()` provided to calculate the mean of the posterior).\n",
        "\n",
        "Using this knowledge, we will now represent $\\hat x$ as a function of the encoded stimulus $\\tilde x$. This will result in a 2D binary decision array. To do so, we will scan the posterior matrix (i.e. row-wise), and set the array cell value to 1 at the mean of the row-wise posterior.\n",
        "\n",
        "**Suggestions**\n",
        "* For each brain encoding $\\tilde x$ (row of the posterior array), calculate the mean of the posterior, and set the corresponding cell of the binary decision array to 1. (e.g., if the mean of the posterior is at position 0, then set the cell with x_column == 0 to 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-RE1MNs2vra"
      },
      "source": [
        "def calculate_binary_decision_array(x_points, posterior_array):\n",
        "    \"\"\"\n",
        "    This function computes the decision taken by the participants for every potential decision input\n",
        "    \n",
        "    Inputs : x_points (numpy array) is the set of points on which we evaluated the posterior\n",
        "             posterior_array (numpy array) is the posterior distribution\n",
        "             \n",
        "    Outputs : binary_decision_array (numpy array) that contains the decision taken for every potential input stimulus\n",
        "    \"\"\"\n",
        "\n",
        "    binary_decision_array = np.zeros_like(posterior_array)\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n",
        "\n",
        "\n",
        "    return binary_decision_array\n",
        "\n",
        "    \n",
        "#Run the lines below to test your code\n",
        "binary_decision_array = calculate_binary_decision_array(x, posterior_array)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "colormap = ax.imshow(binary_decision_array, extent=[-10, 10, 8, -8])\n",
        "cbar = plt.colorbar(colormap, ax=ax)\n",
        "cbar.set_label('probability')\n",
        "ax.invert_yaxis()\n",
        "ax.set_ylabel('Brain encoded Stimulus $\\~x$')\n",
        "ax.set_title('Sample Binary Decision Array')\n",
        "ax.set_xlabel('Chosen position $\\hat x$')\n",
        "ax.set_aspect('auto')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxdP4th3LR6R"
      },
      "source": [
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-Decision.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1suFysH15Dh"
      },
      "source": [
        "In real life experiment, we typically have the response and the stimulus. With the schema above, we can therefore recover the brain encoding given the subject response. "
      ]
    }
  ]
}