{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LGBIO2060_TP4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/svandergoote/LGBIO2060-2021/blob/main/LGBIO2060_TP4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q9FHZbAXUjK"
      },
      "source": [
        "# LGBIO2060 Exercice session 4\n",
        "\n",
        "#Hidden markov model\n",
        "\n",
        "__Authors:__ Simon Vandergooten, Cl√©mence Vandamme\n",
        "\n",
        "__Content inspired from__: Neuromatch Academy github.com/NeuromatchAcademy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A4jkLCkXr0B"
      },
      "source": [
        "##Introduction and context\n",
        "In this tutorial the models developed in the last sessions will get a bit more complex. Until now, we have used binary or continuous variables whose state was fixed in time. It is easy to see that this greatly limits our ability to represent real systems. One way to get closer to reality would be to allow our hidden states to change their values. \n",
        "\n",
        "Let's take the example of the fish. Previously they were either on the right or on the left. But now, we will add to the model the fact that fishes can switch side. \n",
        "\n",
        "It is exactly what **Hidden markov models** permits. The Markov property specifies that you can fully encapsulate the important properties of a system based on its current state at the current time. Any previous history does not matter, it is memoryless. Back to the fishes, it means that the school of fish is only at one position at a time and the probability of them being on the left or on the right at time *t* depends only on the state at time *t-1*.  The probabilities of transition from one state to another can be represented with a matrix called the **transition matrix**.\n",
        "\n",
        "<img alt='Solution hint' align='left' width=650 height=300 src=https://raw.githubusercontent.com/svandergoote/LGBIO2060-2021/master/Solutions/HMM.png>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neP8J8DKm2k6"
      },
      "source": [
        "We can use linear algebra to express the probabilities of the current state.\n",
        "\n",
        "$$P_i = [P(state_i = right), P(state_i = left) ] $$\n",
        "\n",
        "To compute the vector of probabilities of the state at the time i+1, we can use linear algebra and multiply our vector of the probabilities of the current state with the transition matrix.\n",
        "\n",
        "$$P_{i+1} = P_{i} T$$\n",
        "where $T$ is our transition matrix.\n",
        "\n",
        "This is the same formula for every step, which allows us to get the probabilities for a time more than 1 step in advance easily. If we started at $i=0$ and wanted to look at the probabilities at step $i=2$, we could do:\n",
        "\n",
        "\\begin{align*}\n",
        "P_{1} &= P_{0}T\\\\\n",
        "P_{2} &= P_{1}T = P_{0}TT = P_{0}T^2\\\\\n",
        "\\end{align*}\n",
        "\n",
        "So, every time we take a further step we can just multiply with the transition matrix again. So, the probability vector of states at j timepoints after the current state at timepoint i is equal to the probability vector at timepoint i times the transition matrix raised to the jth power.\n",
        "$$P_{i + j} = P_{i}T^j $$\n",
        "\n",
        "By the end of this tutorial, you should be able to:\n",
        "- Describe how the hidden states in a Hidden Markov model evolve over time, both in words, mathematically, and in code\n",
        "- Estimate hidden states from data using forward inference in a Hidden Markov model\n",
        "- Describe how measurement noise and state transition probabilities affect uncertainty in predictions in the future and the ability to estimate hidden states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9haFetXebje"
      },
      "source": [
        "# Section 1: Binary HMM with Gaussian measurements\n",
        "We will represent the hidden state *s* of the fish with the values +1 and -1 for the right and left position respectively. \n",
        "\n",
        "The probability of switching to state $s_t=j$ from the previous state $s_{t-1}=i$ is the conditional probability distribution $p(s_t = j| s_{t-1} = i)$.\n",
        "\n",
        "In our case, we can summarize those transition probabilities into the **Transition matrix T**.\n",
        "\n",
        "\\begin{align*}\n",
        "T = \\begin{bmatrix}p(s_t = +1 | s_{t-1} = +1) & p(s_t = -1 | s_{t-1} = +1)\\\\p(s_t = +1 | s_{t-1} = -1)& p(s_t = -1 | s_{t-1} = -1)\\end{bmatrix}\n",
        "\\end{align*}\n",
        "\n",
        "###Measurements\n",
        "In a Hidden Markov model, we cannot directly observe the latent states $s_t$. Instead we get noisy measurements $m_t \\sim p(m|s_t)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "89xniMe31KWO"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy import stats\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from collections import namedtuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import patches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "xlKicbtl1RPO"
      },
      "source": [
        "#@title Figure Settings\n",
        "# import ipywidgets as widgets       # interactive display\n",
        "from IPython.html import widgets\n",
        "from ipywidgets import interactive, interact, HBox, Layout,VBox\n",
        "from IPython.display import HTML\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/NMA2020/nma.mplstyle\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFxuEpkr1WJw",
        "cellView": "form"
      },
      "source": [
        "# @title Plotting Functions\n",
        "\n",
        "def plot_hmm1(noise_level, states, measurements, flag_m=True):\n",
        "  \"\"\"Plots HMM states and measurements for 1d states and measurements.\n",
        "\n",
        "  Args:\n",
        "    model (hmmlearn model):               hmmlearn model used to get state means.\n",
        "    states (numpy array of floats):       Samples of the states.\n",
        "    measurements (numpy array of floats): Samples of the states.\n",
        "  \"\"\"\n",
        "  #Correction to avoid model\n",
        "  means_vec = np.array([1., -1.])\n",
        "  n_components = 2\n",
        "  vars_vec = np.ones(2) * noise_level * noise_level\n",
        "\n",
        "  T = states.shape[0]\n",
        "  nsteps = states.size\n",
        "  aspect_ratio = 2\n",
        "  fig, ax1 = plt.subplots(figsize=(8,4))\n",
        "  #states_forplot = list(map(lambda s: means_vec[s], states))\n",
        "  ax1.step(np.arange(nstep), states, \"-\", where=\"mid\", alpha=1.0, c=\"green\")\n",
        "  ax1.set_xlabel(\"Time\")\n",
        "  ax1.set_ylabel(\"Latent State\", c=\"green\")\n",
        "  ax1.set_yticks([-1, 1])\n",
        "  ax1.set_yticklabels([\"-1\", \"+1\"])\n",
        "  ax1.set_xticks(np.arange(0,T,10))\n",
        "  ymin = min(measurements)\n",
        "  ymax = max(measurements)\n",
        "\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.set_ylabel(\"Measurements\", c=\"crimson\")\n",
        "\n",
        "  # show measurement gaussian\n",
        "  if flag_m:\n",
        "    ax2.plot([T,T],ax2.get_ylim(), color=\"maroon\", alpha=0.6)\n",
        "    for i in range(n_components):\n",
        "      mu = means_vec[i]\n",
        "      scale = np.sqrt(vars_vec[i])\n",
        "      rv = stats.norm(mu, scale)\n",
        "      num_points = 50\n",
        "      domain = np.linspace(mu-3*scale, mu+3*scale, num_points)\n",
        "\n",
        "      left = np.repeat(float(T), num_points)\n",
        "      # left = np.repeat(0.0, num_points)\n",
        "      offset = rv.pdf(domain)\n",
        "      offset *= T / 15\n",
        "      lbl = \"measurement\" if i == 0 else \"\"\n",
        "      # ax2.fill_betweenx(domain, left, left-offset, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "      ax2.scatter(np.arange(nstep), measurements, c=\"crimson\", s=4)\n",
        "      ax2.legend(loc=\"upper left\")\n",
        "    ax1.set_ylim(ax2.get_ylim())\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def plot_marginal_seq(predictive_probs, switch_prob):\n",
        "  \"\"\"Plots the sequence of marginal predictive distributions.\n",
        "\n",
        "    Args:\n",
        "      predictive_probs (list of numpy vectors): sequence of predictive probability vectors\n",
        "      switch_prob (float):                      Probability of switching states.\n",
        "  \"\"\"\n",
        "  T = len(predictive_probs)\n",
        "  prob_neg = [p_vec[1] for p_vec in predictive_probs]\n",
        "  prob_pos = [p_vec[0] for p_vec in predictive_probs]\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.plot(np.arange(T), prob_neg, color=\"blue\")\n",
        "  ax.plot(np.arange(T), prob_pos, color=\"orange\")\n",
        "  ax.legend([\n",
        "    \"prob in state -1\", \"prob in state 1\"\n",
        "  ])\n",
        "  ax.text(T/2, 0.05, \"switching probability from right to left={}\".format(switch_prob), fontsize=12,\n",
        "          bbox=dict(boxstyle=\"round\", facecolor=\"wheat\", alpha=0.6))\n",
        "  ax.set_xlabel(\"Time\")\n",
        "  ax.set_ylabel(\"Probability\")\n",
        "  ax.set_title(\"Forgetting curve in a changing world\")\n",
        "  #ax.set_aspect(aspect_ratio)\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def plot_evidence_vs_noevidence(posterior_matrix, predictive_probs):\n",
        "  \"\"\"Plots the average posterior probabilities with evidence v.s. no evidence\n",
        "\n",
        "  Args:\n",
        "    posterior_matrix: (2d numpy array of floats): The posterior probabilities in state 1 from evidence (samples, time)\n",
        "    predictive_probs (numpy array of floats):  Predictive probabilities in state 1 without evidence\n",
        "  \"\"\"\n",
        "  nsample, T = posterior_matrix.shape\n",
        "  posterior_mean = posterior_matrix.mean(axis=0)\n",
        "  fig, ax = plt.subplots(1)\n",
        "  # ax.plot([0.0, T],[0.5, 0.5], color=\"red\", linestyle=\"dashed\")\n",
        "  ax.plot([0.0, T],[0., 0.], color=\"red\", linestyle=\"dashed\")\n",
        "  ax.plot(np.arange(T), predictive_probs, c=\"orange\", linewidth=2, label=\"No evidence\")\n",
        "  ax.scatter(np.tile(np.arange(T), (nsample, 1)), posterior_matrix, s=0.8, c=\"green\", alpha=0.3, label=\"With evidence(Sample)\")\n",
        "  ax.plot(np.arange(T), posterior_mean, c='green', linewidth=2, label=\"With evidence(Average)\")\n",
        "  ax.legend()\n",
        "  ax.set_yticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
        "  ax.set_xlabel(\"Time\")\n",
        "  ax.set_ylabel(\"Probability in State +1\")\n",
        "  ax.set_title(\"Gain confidence with evidence\")\n",
        "  plt.show(fig)\n",
        "\n",
        "\n",
        "def plot_forward_inference(noise_level, states, measurements, states_inferred,\n",
        "                           predictive_probs, likelihoods, posterior_probs,\n",
        "                           t=None,\n",
        "                           flag_m=True, flag_d=True, flag_pre=True, flag_like=True, flag_post=True,\n",
        "                           ):\n",
        "  \"\"\"Plot ground truth state sequence with noisy measurements, and ground truth states v.s. inferred ones\n",
        "\n",
        "      Args:\n",
        "          model (instance of hmmlearn.GaussianHMM): an instance of HMM\n",
        "          states (numpy vector): vector of 0 or 1(int or Bool), the sequences of true latent states\n",
        "          measurements (numpy vector of numpy vector): the un-flattened Gaussian measurements at each time point, element has size (1,)\n",
        "          states_inferred (numpy vector): vector of 0 or 1(int or Bool), the sequences of inferred latent states\n",
        "  \"\"\"\n",
        "  #Correction to avoid model\n",
        "  means_vec = np.array([1., -1.])\n",
        "  n_components = 2\n",
        "  vars_vec = np.ones(2) * noise_level * noise_level\n",
        "\n",
        "  T = states.shape[0]\n",
        "  if t is None:\n",
        "    t = T-1\n",
        "  nsteps = states.size\n",
        "  fig, ax1 = plt.subplots(figsize=(11,6))\n",
        "  # inferred states\n",
        "  #ax1.step(np.arange(nstep)[:t+1], states_forplot[:t+1], \"-\", where=\"mid\", alpha=1.0, c=\"orange\", label=\"inferred\")\n",
        "  # true states\n",
        "  #states_forplot = list(map(lambda s: model.means[s], states))\n",
        "  ax1.step(np.arange(nstep)[:t+1], states[:t+1], \"-\", where=\"mid\", alpha=1.0, c=\"green\", label=\"true\")\n",
        "  ax1.step(np.arange(nstep)[t+1:], states[t+1:], \"-\", where=\"mid\", alpha=0.3, c=\"green\", label=\"\")\n",
        "  # Posterior curve\n",
        "  delta = means_vec[1] - means_vec[0]\n",
        "  states_interpolation = means_vec[0] + delta * posterior_probs[:,1]\n",
        "  if flag_post:\n",
        "    ax1.step(np.arange(nstep)[:t+1], states_inferred[:t+1], \"-\", where=\"mid\", c=\"grey\", label=\"posterior\")\n",
        "\n",
        "  ax1.set_xlabel(\"Time\")\n",
        "  ax1.set_ylabel(\"Latent State\", c=\"green\")\n",
        "  ax1.set_yticks([-1, 1])\n",
        "  ax1.set_yticklabels([\"-1\", \"+1\"])\n",
        "  ax1.legend(bbox_to_anchor=(0,1.02,0.2,0.1), borderaxespad=0, ncol=2)\n",
        "  \n",
        "\n",
        "\n",
        "  ax2 = ax1.twinx()\n",
        "  ax2.set_ylim(\n",
        "      min(-1.2, np.min(measurements)),\n",
        "      max(1.2, np.max(measurements))\n",
        "      )\n",
        "  if flag_d:\n",
        "    ax2.scatter(np.arange(nstep)[:t+1], measurements[:t+1], c=\"crimson\", s=4, label=\"measurement\")\n",
        "    ax2.set_ylabel(\"Measurements\", c=\"crimson\")\n",
        "  \n",
        "  # show measurement distributions\n",
        "  if flag_m:\n",
        "    for i in range(n_components):\n",
        "      mu = means_vec[i]\n",
        "      scale = np.sqrt(vars_vec[i])\n",
        "      rv = stats.norm(mu, scale)\n",
        "      num_points = 50\n",
        "      domain = np.linspace(mu-3*scale, mu+3*scale, num_points)\n",
        "\n",
        "      left = np.repeat(float(T), num_points)\n",
        "      offset = rv.pdf(domain)\n",
        "      offset *= T /15\n",
        "      # lbl = \"measurement\" if i == 0 else \"\"\n",
        "      lbl = \"\"\n",
        "      # ax2.fill_betweenx(domain, left, left-offset, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"maroon\", label=lbl)\n",
        "  ymin, ymax = ax2.get_ylim()\n",
        "  width = 0.1 * (ymax-ymin) / 2.0\n",
        "  centers = [-1.0, 1.0]\n",
        "  bar_scale = 15\n",
        "  \n",
        "  # Predictions\n",
        "  data = predictive_probs\n",
        "  if flag_pre:\n",
        "    for i in range(n_components):\n",
        "      domain = np.array([centers[i]-1.5*width, centers[i]-0.5*width])\n",
        "      left = np.array([t,t])\n",
        "      offset = np.array([data[t,i]]*2)\n",
        "      offset *= bar_scale\n",
        "      lbl = \"todays prior\" if i == 0 else \"\"\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"dodgerblue\", label=lbl)\n",
        "\n",
        "  # Likelihoods\n",
        "  # data = np.stack([likelihoods, 1.0-likelihoods],axis=-1)\n",
        "  data = likelihoods\n",
        "  data /= np.sum(data,axis=-1, keepdims=True)\n",
        "  if flag_like:\n",
        "    for i in range(n_components):\n",
        "      domain = np.array([centers[i]+0.5*width, centers[i]+1.5*width])\n",
        "      left = np.array([t,t])\n",
        "      offset = np.array([data[t,i]]*2)\n",
        "      offset *= bar_scale\n",
        "      lbl = \"likelihood\" if i == 0 else \"\"\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"crimson\", label=lbl)\n",
        "  # Posteriors\n",
        "  data = posterior_probs\n",
        "  if flag_post:\n",
        "    for i in range(n_components):\n",
        "      domain = np.array([centers[i]-0.5*width, centers[i]+0.5*width])\n",
        "      left = np.array([t,t])\n",
        "      offset = np.array([data[t,i]]*2)\n",
        "      offset *= bar_scale\n",
        "      lbl = \"posterior\" if i == 0 else \"\"\n",
        "      ax2.fill_betweenx(domain, left+offset, left, alpha=0.3, lw=2, color=\"grey\", label=lbl)\n",
        "  if t<T-1:\n",
        "    ax2.plot([t,t],ax2.get_ylim(), color='black',alpha=0.6)\n",
        "  if flag_pre or flag_like or flag_post:\n",
        "    ax2.plot([t,t],ax2.get_ylim(), color='black',alpha=0.6)\n",
        "\n",
        "    ax2.legend(bbox_to_anchor=(0.4,1.02,0.6, 0.1), borderaxespad=0, ncol=4)\n",
        "  ax1.set_ylim(ax2.get_ylim())\n",
        "  return fig\n",
        "  # plt.show(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWqpbUheiMJ5"
      },
      "source": [
        "##Coding exercice 1.1 : Hidden states\n",
        "You will first implement the function `generate_state`. This function will create the vector of states *S* based on the model parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC2tO6qgXQXr"
      },
      "source": [
        "def generate_state(switch_proba, start_proba, n_states):\n",
        "  '''\n",
        "  Create an HMM binary state variable.\n",
        "  Args:\n",
        "    switch_proba (array): the probabilities to switch from states. [p(rightToLeft), p(LeftToRight)]\n",
        "    start_proba (array): the initial probabilities of being on each side. P0 = [p_right, p_left]\n",
        "    n_states (int): the number of time steps\n",
        "\n",
        "  Returns:\n",
        "    S (array): the vector of state for each time step.\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "  #Initialize S\n",
        "  S = ...\n",
        "\n",
        "  #Step 1: Initial state (Hint: np.random.choice)\n",
        "  S[0] = ...\n",
        "\n",
        "  #Step 2: Transition matrix\n",
        "  T = ...\n",
        "  \n",
        "  #Step 3: Iterate on each time step to find the new state s[t] based on S[t-1]\n",
        "\n",
        "\n",
        "\n",
        "  return S\n",
        "\n",
        "#Set random seed\n",
        "np.random.seed(54)\n",
        "\n",
        "#Set parameters of HMM\n",
        "switch_proba = np.array([0.4, 0.7])\n",
        "start_proba = np.array([0, 1]) #The initial state is left (-1)\n",
        "n_states = 50\n",
        "\n",
        "#Generate the hidden states vector\n",
        "S = generate_state(switch_proba, start_proba, n_states)\n",
        "\n",
        "#Print values\n",
        "print(S[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTcRu5NxCNmD"
      },
      "source": [
        "You should see that the first five states are:\n",
        " \n",
        " `[-1.  1.  1.  1.  1.]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlMpx1F1rxRJ"
      },
      "source": [
        "##Coding exercice 1.2 : Noisy measurements\n",
        "Now that you have created the hidden states, you will create the noisy Gaussian measurements vector *M* from these states.\n",
        "\n",
        "Recall that in reality we don't have access to the hidden states but only to noisy measurements that give us information about those hidden states we want to infer.\n",
        "\n",
        "You will implement the function `sample` that generates samples $m_t$ based on the hidden states $s_t$. \n",
        "- if hidden state $s_t = +1 $ : $m_t \\sim \\mathcal{N}(+1,\\sigma^2)$\n",
        "- if hidden state $s_t = -1 $ : $m_t \\sim \\mathcal{N}(-1,\\sigma^2)$\n",
        "\n",
        "Back to the fish example, the measurement could be for example the position of a fish you managed to see. It is noisy because of water refraction as well as the imperfection of the visual system. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-P-EAX-rvUz"
      },
      "source": [
        "def sample(means, var, S):\n",
        "  '''\n",
        "  Create a Gaussian measurement from HMM states\n",
        "\n",
        "    Args: \n",
        "      means (array): Mean measurement for each state [right, left].\n",
        "      var (float): Variance of measurement models. Same for each state.\n",
        "      S (array): The series of hidden states.\n",
        "    \n",
        "    Returns:\n",
        "      M (array): The series of measurements.\n",
        "  '''\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "  \n",
        "  #Initialize measurements vector M\n",
        "  M = ...\n",
        "\n",
        "  #Calculate measurements conditioned on the latent states (Hint: np.random.normal)\n",
        "  \n",
        "\n",
        "  return M\n",
        "\n",
        "#Set parameters of Gaussian \n",
        "np.random.seed(54)\n",
        "means = np.array([1, -1])\n",
        "variance = 4\n",
        "\n",
        "#Generate the measurements vector M\n",
        "M = sample(means, variance, S)\n",
        "\n",
        "#Print values\n",
        "print(M[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea6m7OV5CC0Q"
      },
      "source": [
        "You should see that the first five measurements are:\n",
        " \n",
        " `[-4.70442149 -1.16199798  1.15712765 -1.70827118  2.25229692]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXb6ild9Cbda"
      },
      "source": [
        "# Interactive Demo 1.2: Binary HMM\n",
        "\n",
        "In the demo below, we simulate and plot a similar HMM. You can change the probability of switching states and the noise level (the standard deviation of the Gaussian distributions for measurements). You can click the empty box to also visualize the measurements.\n",
        "\n",
        "**First**, think about and discuss these questions:\n",
        "\n",
        "1.   What will the states do if the switching probability from Left to Right is zero? One?\n",
        "2.   What will measurements look like with high noise? Low?\n",
        "\n",
        "\n",
        "\n",
        "**Then**, play with the demo to see if you were correct or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SDKt9XE7qiq",
        "cellView": "form"
      },
      "source": [
        "#@title\n",
        "\n",
        "#@markdown Execute this cell to enable the widget!\n",
        "nstep = 100\n",
        "\n",
        "@widgets.interact\n",
        "def plot_samples_widget(\n",
        "    switch_RtoL=widgets.FloatSlider(min=0.0, max=1.0, step=0.02, value=0.1),\n",
        "    switch_LtoR=widgets.FloatSlider(min=0.0, max=1.0, step=0.02, value=0.1),\n",
        "    log10_noise_level=widgets.FloatSlider(min=-1., max=1., step=.01, value=-0.3),\n",
        "    flag_m=widgets.Checkbox(value=False, description='measurements', disabled=False, indent=False)\n",
        "    ):\n",
        "  np.random.seed(54)\n",
        "  #Parameters\n",
        "  switch_proba = np.array([switch_RtoL, switch_LtoR])\n",
        "  start_proba = np.array([0, 1])\n",
        "  means = np.array([1., -1.])\n",
        "  var = np.ones(2) * 10.**log10_noise_level * 10.**log10_noise_level\n",
        "\n",
        "  states = generate_state(switch_proba, start_proba, nstep)\n",
        "  observations = sample(means, var[0], states)\n",
        "\n",
        "  plot_hmm1(10.**log10_noise_level, states, observations, flag_m=flag_m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpR66jW-17RT"
      },
      "source": [
        "#Section 2: Predicting the future in an HMM\n",
        "Even if we know the world state for sure, the world changes. We will become less and less confident on the states as time goes if we do not add any measurement. In this exercise, we'll see how a Hidden Markov Model gradually \"forgets\" the current state when predicting the future without measurements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vGprtDG6m4k"
      },
      "source": [
        "##Coding exercice 2.1: future without measurements\n",
        "Assume we know the initial state $s_0 = -1$, so $p(s_0)=[0, 1]$. We will plot $p(s_t)$ versus time.\n",
        "\n",
        "You will implement the function `simulate_prediction_only` which simulates the diffusion of a HMM with no measurement.\n",
        "In practice, you will iteratively implement a **one step prediction** using the dynamic of the HMM and the previous probability vector to compute the next one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plK-86wV6zPu"
      },
      "source": [
        "def simulate_prediction_only(nstep, T):\n",
        "  '''\n",
        "  Simulate the diffusion of HMM with no measurements.\n",
        "\n",
        "    Args:\n",
        "      nstep (int): total number of time steps to simulate\n",
        "      T (array): transition matrix of the HMM\n",
        "\n",
        "    Returns:\n",
        "      predictive_probs (list of numpy vector): the list of maginal probabilities\n",
        "  '''\n",
        "  #Initialization\n",
        "  predictive_probs = []\n",
        "\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "  p0 = ...\n",
        "  \n",
        "  #Compute the probability vector for each time step\n",
        "\n",
        "  return predictive_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVleJVPjIDdk"
      },
      "source": [
        "### Interactive Demo 2.1: Forgetting in a changing world\n",
        "Using the following widget manipulate the process dynamics via the slider controlling the switching probabilities.\n",
        "\n",
        "- What happens when both probabilities are the same ? And when they are not ?\n",
        "\n",
        "- How does the curves look when both `switch_RtoL` and `switch_LtoR` are 0.05 ? 0.95 ?\n",
        "\n",
        "- Do you forget more quickly with high or low switching probabilities ? Why ?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeMVXFCJIH2z",
        "cellView": "form"
      },
      "source": [
        "#@markdown Execute this cell to enable the widget\n",
        "noise_level = 0.5\n",
        "nsteps = 100\n",
        "np.random.seed(54)\n",
        "@widgets.interact\n",
        "def plot_samples_widget(\n",
        "    switch_RtoL=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.1),\n",
        "    switch_LtoR=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.1)):\n",
        "  np.random.seed(54)\n",
        "  transition_matrix = np.array([[1. - switch_RtoL, switch_RtoL],\n",
        "                                [switch_LtoR, 1. - switch_LtoR]])\n",
        "  predictive_probs = simulate_prediction_only(nsteps, transition_matrix)\n",
        "  plot_marginal_seq(predictive_probs, switch_RtoL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPh-H3CuwIsB"
      },
      "source": [
        "#Section 3: Forward inference in an HMM\n",
        "Now that we have seen that it is not possible to do predictions without using  measurements, we will see how it is possible using Bayesian theory.\n",
        "\n",
        "It will be a recursive algorithm. Let's assume we already have yesterday's posterior $p(s_{t-1}|m_{1:t-1})$ which is our belief of the state of yesterday based on all the previous measurements. When the new data $m_t$ comes in, the algorithm performs the following steps:\n",
        "\n",
        "- **Predict**: Tranform yesterday's posterior over $s_{t-1}$ into today's prior over $s_t$ using the transition $T$. \n",
        "- **Update**: Incorporate measurement $m_t$ to calculate the posterior $p(s_t|m_{1:t})$\n",
        "\n",
        "Today's prior and the posterior are illustrated in the following picture.\n",
        "\n",
        "\n",
        "<img alt='Solution hint' align='left' width=800 height=300 src=https://raw.githubusercontent.com/svandergoote/LGBIO2060-2021/master/Solutions/TP4_posterior.PNG>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9qJVuTZAyFO"
      },
      "source": [
        "## Coding Exercise 3.1: Forward inference of HMM\n",
        "In this exercise, you will:\n",
        "\n",
        "* STEP 1: Complete the code in function `compute_likelihood` to calculate the likelihood of seeing data 'M' for all measurement models.\n",
        "\n",
        "* STEP 2: Complete the code in function `one_step_update` to combine predictive probabilities and data likelihood into a new posterior.\n",
        "\n",
        "* STEP 3: Complete the code in the function `forward inference` which calls `one_step_update` recursively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y6yBjk0AxFF"
      },
      "source": [
        "def compute_likelihood(means, var, M):\n",
        "  \"\"\"\n",
        "  Calculate likelihood of seeing data `M` for all measurement models\n",
        "\n",
        "  Args:\n",
        "    means (array): Mean mesurement for each state [right, left].\n",
        "    var (float): Variance of measurement models. Same for each state.\n",
        "    M (float or numpy vector)\n",
        "\n",
        "  Returns:\n",
        "    L (numpy vector or matrix): the likelihood. if M is a float -> L = [p(M|s=1), p(M|s=-1)] \n",
        "                                                   M is a vector-> L = [[p(M|s=1)],\n",
        "                                                                        [p(M|s=-1)]]\n",
        "  \"\"\"\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "  #Initialization of distribution (hint: scipy.norm)\n",
        "  p_pos = ...\n",
        "  p_neg = ...\n",
        "\n",
        "  #Extract the likelihood from pdf (hint: method pdf of a normal continuous variable)\n",
        "  L = ...\n",
        "  \n",
        "  \n",
        "  return L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz4FDYQUt0M4"
      },
      "source": [
        "def one_step_update(T, posterior_tm1, M_t, means, var):\n",
        "  \"\"\"Given a HMM model, calculate the one-time-step updates to the posterior.\n",
        "  Args:\n",
        "    T (numpy array): transition matrix of the HMM\n",
        "    posterior_tm1 (numpy vector): Posterior at `t-1`\n",
        "    M_t (numpy array): measurement at `t`\n",
        "    means (array): Mean mesurement for each state [right, left].\n",
        "    var (float): Variance of measurement models. Same for each state.\n",
        "\n",
        "  Returns:\n",
        "    prediction (numpy array): prediction at `t` (Today's prior)\n",
        "    likelihood (numpy array): likelihood of seeing data M_t for all measurements models\n",
        "    posterior_t (numpy array): Posterior at `t`\n",
        "  \"\"\"\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "\n",
        "  # Calculate predictive probabilities (prior)\n",
        "  prediction = ...\n",
        "\n",
        "  #Get the likelihood (Hint: Use compute_likelihood)\n",
        "  likelihood = ...\n",
        "\n",
        "  #Calculate posterior\n",
        "  posterior_t = ...\n",
        "\n",
        "  #Normalize\n",
        "  ...\n",
        "\n",
        "  return prediction, likelihood, posterior_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Z0cb-33twz4"
      },
      "source": [
        "def simulate_forward_inference(means, var, T, N, data):\n",
        "  \"\"\"\n",
        "  Given the HMM model, calculate posterior marginal predictions of x_t for N-1 time steps ahead based on\n",
        "  evidence `data`. \n",
        "\n",
        "  Args:\n",
        "    means (array): Mean mesurement for each state [right, left].\n",
        "    var (float): Variance of measurement models. Same for each state.\n",
        "    T (numpy array): transition matrix of the HMM\n",
        "    N (int): length of returned array\n",
        "    data (numpy array): measurements vector\n",
        "\n",
        "  Returns:\n",
        "    predictive_probs (numpy array): predictive probabilities \n",
        "    likelihoods (numpy array): likelihood of seeing data M_t for all measurements models\n",
        "    posterior_probs (numpy array): posterior probabilities \n",
        "  \"\"\"\n",
        "  \n",
        "  #Initialize arrays\n",
        "  predictive_probs = np.zeros((N,2))\n",
        "  likelihoods = np.zeros((N,2))\n",
        "  posterior_probs = np.zeros((N,2))\n",
        "\n",
        "  ##########################\n",
        "  ##### Your code here #####\n",
        "  ##########################\n",
        "\n",
        "  #Calculate marginal for each latent state x_t\n",
        "\n",
        "  #Start with the first element\n",
        "  predictive_probs[0,:] = ...\n",
        "  likelihoods[0,:] = ...\n",
        "  posterior_probs[0,:] = ...\n",
        "\n",
        "  #Then iterate for the rest of the N elements\n",
        "  for t in range(1, N):\n",
        "    ...\n",
        "\n",
        "  return predictive_probs, likelihoods, posterior_probs\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(54)\n",
        "\n",
        "# Set parameters\n",
        "switch_proba = np.array([0.4, 0.7])\n",
        "T = np.array([[1-switch_proba[0], switch_proba[0]],\n",
        "                [switch_proba[1], 1-switch_proba[1]]])\n",
        "start_proba = np.array([0, 1]) #The initial state is left (-1)\n",
        "noise_level = .4\n",
        "means = np.array([1, -1])\n",
        "n_steps = 100\n",
        "N = 75\n",
        "\n",
        "# Create and sample from model\n",
        "states = generate_state(switch_proba, start_proba, n_steps)\n",
        "measurements = sample(means, noise_level**2, states)\n",
        "\n",
        "# Infer state sequence\n",
        "predictive_probs, likelihoods, posterior_probs = simulate_forward_inference(means, noise_level**2, T, start_proba, n_steps, measurements)\n",
        "states_inferred = np.ones(n_steps)\n",
        "states_inferred[posterior_probs[:,0] < 0.5] =-1\n",
        "\n",
        "# Visualize\n",
        "plot_forward_inference(\n",
        "      noise_level, states, measurements, states_inferred,\n",
        "      predictive_probs, likelihoods, posterior_probs,t=N, flag_m = 0\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7y6kBJz2Bf6S"
      },
      "source": [
        "##Interactive Demo 3.2: Forward inference in binary HMM\n",
        "Now visualize your inference algorithm. Play with the sliders and checkboxes to help you gain intuition.\n",
        "\n",
        "Use the sliders switch_RtoL, switch_LtoR and log10_noise_level to change the switching probabilities and measurement noise level.\n",
        "\n",
        "Use the slider t to view prediction (prior) probabilities, likelihood, and posteriors at different times.\n",
        "\n",
        "When does the inference make a mistake? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Zz8KhrRzBbtt"
      },
      "source": [
        "# @markdown Execute this cell to enable the demo\n",
        "\n",
        "nstep = 100\n",
        "\n",
        "@widgets.interact\n",
        "def plot_forward_inference_widget(\n",
        "    switch_RtoL=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.05),\n",
        "    switch_LtoR=widgets.FloatSlider(min=0.0, max=1.0, step=0.01, value=0.05),\n",
        "    log10_noise_level=widgets.FloatSlider(min=-1., max=1., step=.01, value=0.1),\n",
        "    t=widgets.IntSlider(min=0, max=nstep-1, step=1, value=nstep//2),\n",
        "    #flag_m=widgets.Checkbox(value=True, description='measurement distribution', disabled=False, indent=False),\n",
        "    flag_d=widgets.Checkbox(value=True, description='measurements', disabled=False, indent=False),\n",
        "    flag_pre=widgets.Checkbox(value=True, description='todays prior', disabled=False, indent=False),\n",
        "    flag_like=widgets.Checkbox(value=True, description='likelihood', disabled=False, indent=False),\n",
        "    flag_post=widgets.Checkbox(value=True, description='posterior', disabled=False, indent=False),\n",
        "    ):\n",
        "\n",
        "  np.random.seed(102)\n",
        "\n",
        "  # global model, measurements, states, states_inferred, predictive_probs, likelihoods, posterior_probs\n",
        "  switch_proba = np.array([switch_RtoL, switch_LtoR])\n",
        "  start_proba = np.array([0, 1])\n",
        "  means = np.array([1., -1.])\n",
        "  var = np.ones(2) * 10.**log10_noise_level * 10.**log10_noise_level\n",
        "\n",
        "  states = generate_state(switch_proba, start_proba, nstep)\n",
        "  observations = sample(means, var[0], states)\n",
        "\n",
        "  # Infer state sequence\n",
        "  predictive_probs, likelihoods, posterior_probs = simulate_forward_inference(means, var[0], T, start_proba, nstep, observations)\n",
        "\n",
        "  states_inferred = np.ones(n_steps)\n",
        "  states_inferred[posterior_probs[:,0] < 0.5] =-1\n",
        "\n",
        "  fig = plot_forward_inference(\n",
        "      noise_level, states, measurements, states_inferred,\n",
        "      predictive_probs, likelihoods, posterior_probs,t=t, flag_m = 0,\n",
        "        flag_d=flag_d,flag_pre=flag_pre,flag_like=flag_like,flag_post=flag_post\n",
        "      )\n",
        "  plt.show(fig)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}